[
  {
    "objectID": "data/combine_data.html",
    "href": "data/combine_data.html",
    "title": "Combine data",
    "section": "",
    "text": "After having cleaned all individual studies and put the data into a shared format, we combine the studies into a single data frame.\n\n\nCode\n# combine data\n\n# List all matching files recursively\nfiles &lt;- list.files(\n  path = \"papers\", \n  pattern = \"^cleaned.*\\\\.rds$\", \n  full.names = TRUE, \n  recursive = TRUE\n)\n\n# Load each file into a named list\nstudies_data &lt;- lapply(files, function(file) {\n  readRDS(file) |&gt; \n    # remove old labels from stata files \n    haven::zap_labels() |&gt; \n    # ensure all identifier variables are of same variable type\n    mutate(subject_id = as.character(subject_id), \n           scale = as.character(scale))\n})\n\n# Name list elements based on filenames (without extension or path)\nnames(studies_data) &lt;- tools::file_path_sans_ext(basename(files))\n\ndata &lt;- bind_rows(studies_data)",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#a-single-data-frame",
    "href": "data/combine_data.html#a-single-data-frame",
    "title": "Combine data",
    "section": "",
    "text": "After having cleaned all individual studies and put the data into a shared format, we combine the studies into a single data frame.\n\n\nCode\n# combine data\n\n# List all matching files recursively\nfiles &lt;- list.files(\n  path = \"papers\", \n  pattern = \"^cleaned.*\\\\.rds$\", \n  full.names = TRUE, \n  recursive = TRUE\n)\n\n# Load each file into a named list\nstudies_data &lt;- lapply(files, function(file) {\n  readRDS(file) |&gt; \n    # remove old labels from stata files \n    haven::zap_labels() |&gt; \n    # ensure all identifier variables are of same variable type\n    mutate(subject_id = as.character(subject_id), \n           scale = as.character(scale))\n})\n\n# Name list elements based on filenames (without extension or path)\nnames(studies_data) &lt;- tools::file_path_sans_ext(basename(files))\n\ndata &lt;- bind_rows(studies_data)",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#control-and-intervention-group-selection",
    "href": "data/combine_data.html#control-and-intervention-group-selection",
    "title": "Combine data",
    "section": "Control and intervention group selection",
    "text": "Control and intervention group selection\nSeveral experiments have multiple control and/or intervention groups. For our meta-analysis, for each experiment, we only pair one control and one treatment group. In a case where we needed to make a selection, we have coded our selection choice in the variables intervention_selection and control_selection.\n\n\nCode\ndata &lt;- data |&gt;\n  # for cases where there was an intervention selection, make sure to only pick the selected intervention\n  filter(is.na(intervention_selection) | intervention_label == intervention_selection | condition == \"control\") |&gt; \n  # same but for the control\n  filter(is.na(control_selection) | control_label == control_selection | condition == \"treatment\")\n\n# check\n# data |&gt; \n#   group_by(paper_id, condition) |&gt; \n#   summarise(sum(!is.na(accuracy_raw)))\n\n# check transformation with dias paper\n# data |&gt; \n#   filter(paper_id == \"dias_2020\") |&gt; \n#   filter(experiment_id == 1) |&gt; \n#   distinct(condition, control_label, control_selection, intervention_label, intervention_selection)",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#collapse-accuracy-measures",
    "href": "data/combine_data.html#collapse-accuracy-measures",
    "title": "Combine data",
    "section": "Collapse accuracy measures",
    "text": "Collapse accuracy measures\n\n\nCode\ndata &lt;- data |&gt; \n  # collapes all raw accuracy scores on different scales into a binary scale\n  mutate(\n    # make a numeric version of scale, for all scales that are not binary\n    scale_numeric = as.numeric(scale),\n    # add helper variable that indicates whether a scale has a midpoint\n    midpoint_scale = ifelse(scale_numeric %% 2 != 0, TRUE, FALSE),\n    accuracy = case_when(\n      # for scales with midpoints, code midpoints as NA\n      midpoint_scale == TRUE & \n        accuracy_raw == (scale_numeric/2)+0.5 ~ NA,\n      # transform continuous scores\n      accuracy_raw &lt;= scale_numeric/2 ~ 0, \n      accuracy_raw &gt; scale_numeric/2 ~ 1, \n      TRUE ~ accuracy_raw)\n    )\n\n# check\n# data |&gt;\n#   distinct(scale, accuracy, accuracy_raw)",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#make-unique-paper-experiment-and-subject-identifiers",
    "href": "data/combine_data.html#make-unique-paper-experiment-and-subject-identifiers",
    "title": "Combine data",
    "section": "Make unique paper, experiment, and subject identifiers",
    "text": "Make unique paper, experiment, and subject identifiers\n\n\nCode\ndata &lt;- data |&gt; \n    mutate(    \n    # unique experiment identifier\n    unique_experiment_id = paste(paper_id, experiment_id, sep = \"_\"), \n    # unique participant identifier\n    unique_subject_id = paste0(paper_id, \"_\", experiment_id, \"_\", subject_id)\n    )",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#use-deviation-coding",
    "href": "data/combine_data.html#use-deviation-coding",
    "title": "Combine data",
    "section": "Use deviation coding",
    "text": "Use deviation coding\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(    \n    # make numeric helper variables using deviation coding\n    veracity_numeric = ifelse(veracity == \"true\", 0.5, -0.5),\n    condition_numeric = ifelse(condition == \"treatment\",  0.5, -0.5)\n  )",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#exclude-long_term-effects",
    "href": "data/combine_data.html#exclude-long_term-effects",
    "title": "Combine data",
    "section": "Exclude long_term effects",
    "text": "Exclude long_term effects\nSome studies measured long_term effects.\n\n\nCode\ndata |&gt; \n  filter(long_term == TRUE) |&gt; \n  distinct(paper_id)\n\n\n# A tibble: 1 × 1\n  paper_id  \n  &lt;chr&gt;     \n1 guess_2020\n\n\nCode\n# detailed check\n# data |&gt; \n#   filter(paper_id == \"guess_2020\") |&gt; \n#   group_by(long_term) |&gt; \n#   count()\n\n\nWe want to exclude these from the main analysis (including them would likely reduce the detected average treatment effect, as effects tend to regress over time).\n\n\nCode\ndata &lt;- data |&gt;\n  filter(\n    # remove long term effects\n    long_term == FALSE | is.na(long_term)\n  )\n\n# check\n# table(data$long_term, useNA = \"always\")",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/combine_data.html#save-data",
    "href": "data/combine_data.html#save-data",
    "title": "Combine data",
    "section": "Save data",
    "text": "Save data\n\n\nCode\n# build additional variables\n# Save as CSV\nwrite_csv(data, \"data.csv\")\n\n# Save as RDS\nsaveRDS(data, \"data.rds\")",
    "crumbs": [
      "Data",
      "Combine data"
    ]
  },
  {
    "objectID": "data/papers/bago_2022/bago_2022.html",
    "href": "data/papers/bago_2022/bago_2022.html",
    "title": "Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.",
    "section": "",
    "text": "Bago, Bence, Leah R. Rosenzweig, Adam J. Berinsky, and David G. Rand. 2022. “Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.” Cognition and Emotion, June, 1–15. https://doi.org/10.1080/02699931.2022.2090318."
  },
  {
    "objectID": "data/papers/bago_2022/bago_2022.html#reference",
    "href": "data/papers/bago_2022/bago_2022.html#reference",
    "title": "Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.",
    "section": "",
    "text": "Bago, Bence, Leah R. Rosenzweig, Adam J. Berinsky, and David G. Rand. 2022. “Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.” Cognition and Emotion, June, 1–15. https://doi.org/10.1080/02699931.2022.2090318."
  },
  {
    "objectID": "data/papers/bago_2022/bago_2022.html#intervention",
    "href": "data/papers/bago_2022/bago_2022.html#intervention",
    "title": "Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'Only studies 2, 3 and 4 tested an intervention. The intervention aimed at emotion regulation. In study 2, participants were asked to apply either emotional suppression (`intervention_label` = `emotion_suppression`) or emotion reappraisal techniques (`intervention_label` = `emotion_reappraisal`) when considering the veracity of several headlines. In the emotion reappraisal condition, participants read: \"As you view and read the headlines, please try to adopt a detached and unemotional attitude. Please try to think about what you are reading objectively. Read all of the headlines carefully, but please try to think about what you are seeing in such a way that you feel less emotion.\" In the emotion suppression condition, they read: \"As you view and read the headlines, if you have any feelings, please try your best not to let those feelings show. Read all of the headlines carefully, but try to behave so that someone watching you would not know that you are feeling anything at all.\" In studies 3 and 4 of the paper, the authors only tested the emotion suppression treatment.',\n    intervention_selection = \"suppression\",\n    intervention_selection_description = 'Since we can only include one treatment per control condition in the meta-analysis, we remove participants in the emotion reappraisal condition in study 2. This allows us to compare across their studies.\n',\n    control_format = \"picture, lede, source\",\n    originally_identified_treatment_effect = FALSE)\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\nintervention_selection_description\n\n\n\n\nOnly studies 2, 3 and 4 tested an intervention. The intervention aimed at emotion regulation. In study 2, participants were asked to apply either emotional suppression (`intervention_label` = `emotion_suppression`) or emotion reappraisal techniques (`intervention_label` = `emotion_reappraisal`) when considering the veracity of several headlines. In the emotion reappraisal condition, participants read: \"As you view and read the headlines, please try to adopt a detached and unemotional attitude. Please try to think about what you are reading objectively. Read all of the headlines carefully, but please try to think about what you are seeing in such a way that you feel less emotion.\" In the emotion suppression condition, they read: \"As you view and read the headlines, if you have any feelings, please try your best not to let those feelings show. Read all of the headlines carefully, but try to behave so that someone watching you would not know that you are feeling anything at all.\" In studies 3 and 4 of the paper, the authors only tested the emotion suppression treatment.\nSince we can only include one treatment per control condition in the meta-analysis, we remove participants in the emotion reappraisal condition in study 2. This allows us to compare across their studies.\n\n\n\n\n\n\n\n\nNotes\nThe authors did not find evidence that emotion regulation helped people distinguish false from true news headlines, in any of the studies.\nThe authors sampled from a pool of 24 news items–a single participant only rated 16 items from this pool\n\n“We used a pool of 24 items, taken from Pennycook and Rand (2019), half of which were real (true) and the other fake (false). Moreover, half of the items were Republican-consistent and the other half were Democrat-consistent items, based on a pre-test. Participants were presented with 16 randomly selected headlines altogether; 4 from each category (i.e. Republican-consistent fake, Republican-consistent real, Democrat-consistent fake, Democrat-consistent real).”\n\nReference:\n\nPennycook, G., & Rand, D. G. (2019). Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning. Cognition, 188, 39–50. https://doi.org/10.1016/j.cognition.2018.06.011\n\nFor study 4, they used different headlines, but again a pool of 24 with a random sample of 16 per participant.\n\n“We used newer headlines taken from Pennycook et al., (2021a).”\n\nReference:\n\nPennycook, G., Binnendyk, J., Newton, C., & Rand, D. (2021a). A practical guide to doing behavioural research on fake news and misinformation Collabra: Psychology, 7(1), 25293. https://doi.org/10.1525/collabra.25293"
  },
  {
    "objectID": "data/papers/bago_2022/bago_2022.html#data-cleaning",
    "href": "data/papers/bago_2022/bago_2022.html#data-cleaning",
    "title": "Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nStudy 2\nRead data.\n\n\nCode\nd2 &lt;- read_csv(\"bago_2022-study_2.csv\")\n\n\nRows: 16112 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): reality, condition, concordancy\ndbl (11): ID, accuracy, share, click, seen, Gender, libcons, DemRep_C, age, ...\nlgl  (1): Gender_TEXT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(d2)\n\n\n# A tibble: 6 × 15\n     ID reality accuracy share click  seen Gender Gender_TEXT libcons DemRep_C\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1     1 real           0     0     0     0      1 NA                1        3\n2     1 fake           1     0     0     0      1 NA                1        3\n3     1 real           1     0     0     0      1 NA                1        3\n4     1 fake           1     0     0     0      1 NA                1        3\n5     1 real           1     0     0     0      1 NA                1        3\n6     1 real           1     0     0     1      1 NA                1        3\n# ℹ 5 more variables: condition &lt;chr&gt;, age &lt;dbl&gt;, concordancy &lt;chr&gt;,\n#   perceived_accu &lt;dbl&gt;, item &lt;dbl&gt;\n\n\n\nveracity\n\n\nCode\n# check levels of accuracy \ntable(d2$reality, useNA = \"always\")\n\n\n\nfake real &lt;NA&gt; \n8056 8056    0 \n\n\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(veracity = ifelse(reality == \"fake\", \"false\", \"true\"))\n\n\n\n\naccuracy_raw and scale\n\n\nCode\n# check levels of accuracy \ntable(d2$perceived_accu, useNA = \"always\")\n\n\n\n   0    1 &lt;NA&gt; \n8374 7738    0 \n\n\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(accuracy_raw = perceived_accu, \n         scale = \"binary\")\n\n\n\n\nnews_id, recycled_news, recycled_news_reference\n\n\nCode\nd2 |&gt; \n  group_by(item) |&gt; \n  count()\n\n\n# A tibble: 24 × 2\n# Groups:   item [24]\n    item     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1   674\n 2     2   673\n 3     3   671\n 4     4   671\n 5     5   668\n 6     6   671\n 7     7   666\n 8     8   673\n 9     9   670\n10    10   673\n# ℹ 14 more rows\n\n\nWe rename the variable\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(news_id = item, \n         recycled_news = TRUE, \n         recycled_news_reference = \"Pennycook, G., & Rand, D. G. (2019). Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning. Cognition, 188, 39–50. https://doi.org/10.1016/j.cognition.2018.06.011\")\n\n\n\n\nConcordance (concordance, partisan_identity, news_slant)\nA concordance variable is already present.\n\n\nCode\n# check levels of concordance \ntable(d2$concordancy, useNA = \"always\")\n\n\n\nConcordant Discordant       &lt;NA&gt; \n      8008       8008         96 \n\n\nFrom the legend that the authors provide, we know that DemRep_C describes the partisan identity (“Which of the following best describes your political preference? 1: Strong Democrat 2: Democrat 3: Lean Democrat 4: Lean Republican 5: Republican 6: Strong Republican”). We will collapse this variable into a binary one.\nWe alse add a news slant variable.\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(concordance = tolower(concordancy),\n         partisan_identity = ifelse(DemRep_C &lt;= 3, \"democrat\", \"republican\"),\n         news_slant = case_when(partisan_identity == \"democrat\" & \n                                  concordance == \"concordant\" ~ \"democrat\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"concordant\" ~ \"republican\", \n                                partisan_identity == \"democrat\" & \n                                  concordance == \"discordant\" ~ \"republican\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"discordant\" ~ \"democrat\", \n                                TRUE ~ NA_character_\n                                )\n         )\n\n\n\n\nConditions (intervention_label, condition)\n\n\nCode\n# check levels of condition \nlevels(as.factor(d2$condition))\n\n\n[1] \"Control\"     \"Reappraisal\" \"Suppression\"\n\n\n\n\nCode\nd2 &lt;- d2 |&gt; \n  # only assign a label to interventions conditions, not control conditions\n  mutate(intervention_label = ifelse(condition == \"Control\", NA, tolower(condition)),\n         condition = ifelse(condition == \"Control\", \"control\", \"treatment\")\n         )\n\n\n\n\nage\n\n\nCode\n# check levels of age\ntable(d2$age, useNA = \"always\") # 98 is likely code for NA\n\n\n\n  19   20   22   23   24   25   26   27   28   29   30   31   32   33   34   35 \n  80   80  128  304  240  544  416  480  544  640  704  784 1040  832  816  608 \n  36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51 \n 816  640  528  592  608  368  368  208  240  192  304  160  240  192  144  160 \n  52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67 \n 176  160  128  176  128   80  144  112  144  128  144   80   64   96  112   80 \n  68   69   70   71   74   98 &lt;NA&gt; \n  16   32   32   48   16   16    0 \n\n\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(age = ifelse(age == 98, NA, age))\n\n\n\n\nyear\nThere is no date variable, and in the paper, data collection date is not recorded for Study 2. However, it says that Study 3 has been conducted in September 2020. We therefor use 2020\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(year = 2020)\n\n\n\n\nIdentifiers (subject_id, experiment_id)\n\n\nCode\nd2 &lt;- d2 |&gt; \n  mutate(subject_id = ID, \n         experiment_id = 2)\n\n\n\n\n\nStudy 3\nWe proceed as for the previous study.\n\n\nCode\nd3 &lt;- read_csv(\"bago_2022-study_3.csv\")\n\n\nRows: 48016 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): reality, Gender_TEXT, condition, consistent, concordancy\ndbl (7): ID, Gender, libcons, DemRep_C, Age, perceived_accu, item\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nnames(d3)\n\n\n [1] \"ID\"             \"reality\"        \"Gender\"         \"Gender_TEXT\"   \n [5] \"libcons\"        \"DemRep_C\"       \"condition\"      \"Age\"           \n [9] \"consistent\"     \"concordancy\"    \"perceived_accu\" \"item\"          \n\n\nCode\n# check levels of condition variable\nlevels(as.factor(d3$condition))\n\n\n[1] \"Control\"     \"Suppression\"\n\n\nCode\n# check levels of age\ntable(d3$Age, useNA = \"always\") # 90 seems old, but NA's are coded explictly as such\n\n\n\n  19   20   22   23   24   25   26   27   28   29   30   31   32   33   34   35 \n 368  336  464  672  624  608 1456  992 1184 1584 1744 2576 1872 2160 1632 1456 \n  36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51 \n1936 1536 1248 1488 1424 1456  944 1056 1040  976  832  800  736  704  880  784 \n  52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67 \n 624  640  704  560  768  560  496  528  480  448  640  336  480  384  304  496 \n  68   69   70   71   72   73   74   75   76   77   78   79   80   82   90 &lt;NA&gt; \n 352  368  336  160  176   80  112   96   64   16   96   48   48   16   16   16 \n\n\nCode\nd3 &lt;- d3 |&gt; \n  mutate(subject_id = ID, \n         news_id = item, \n         recycled_news = TRUE, \n         recycled_news_reference = \"Pennycook, G., & Rand, D. G. (2019). Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning. Cognition, 188, 39–50. https://doi.org/10.1016/j.cognition.2018.06.011\",\n         veracity = ifelse(reality == \"fake\", \"false\", \"true\"),\n         intervention_label = ifelse(condition == \"Control\", NA, tolower(condition)),\n         condition = ifelse(condition == \"Control\", \"control\", \"treatment\"), \n         accuracy_raw = perceived_accu,\n         scale = \"binary\",\n         concordance = tolower(concordancy),\n         partisan_identity = ifelse(DemRep_C &lt;= 3, \"democrat\", \"republican\"),\n         news_slant = case_when(partisan_identity == \"democrat\" & \n                                  concordance == \"concordant\" ~ \"democrat\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"concordant\" ~ \"republican\", \n                                partisan_identity == \"democrat\" & \n                                  concordance == \"discordant\" ~ \"republican\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"discordant\" ~ \"democrat\", \n                                TRUE ~ NA_character_\n                                ),\n         experiment_id = 3,\n         age = Age, \n         year = 2020) \n\n\n\n\nStudy 4\nWe proceed as for the previous studies. Note, however, that we add different news source\n\n\nCode\nd4 &lt;- read_csv(\"bago_2022-study_4.csv\")\n\n\nRows: 48336 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): reality, screener1, Gender_TEXT, condition, consistent, concordancy\ndbl (8): ID, sceener2, Gender, libcons, DemRep_C, Age, perceived_accu, item\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(d4)\n\n\n# A tibble: 6 × 14\n     ID reality screener1 sceener2 Gender Gender_TEXT libcons DemRep_C condition\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    \n1     1 fake    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n2     1 real    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n3     1 fake    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n4     1 real    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n5     1 real    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n6     1 fake    cat              1      1 &lt;NA&gt;              7        4 Suppress…\n# ℹ 5 more variables: Age &lt;dbl&gt;, consistent &lt;chr&gt;, concordancy &lt;chr&gt;,\n#   perceived_accu &lt;dbl&gt;, item &lt;dbl&gt;\n\n\nCode\n# check levels of condition variable\nlevels(as.factor(d4$condition))\n\n\n[1] \"Control\"     \"Suppression\"\n\n\nCode\n# check levels of age\ntable(d4$Age, useNA = \"always\") # again, 90 and 98 seem old, but NA's are coded explictly as such\n\n\n\n  19   20   22   23   24   25   26   27   28   29   30   31   32   33   34   35 \n 288  416  336  528  640  768 1136 1232 1152 1440 1584 2336 1984 1872 1920 1760 \n  36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51 \n1712 1712 1520 1744 1280 1472  848  960  944  960  880  960  720  928  832  704 \n  52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67 \n 656  720  432  624  720  496  512  560  656  384  560  480  432  464  400  304 \n  68   69   70   71   72   73   74   75   76   77   78   79   80   81   84   85 \n 352  368  320  304  160  144  128   96   48   48   64   64   16   32   16   16 \n  90   98 &lt;NA&gt; \n  16   16  160 \n\n\nCode\nd4 &lt;- d4 |&gt; \n  mutate(subject_id = ID, \n          news_id = item, \n         recycled_news = TRUE, \n         recycled_news_reference = \"Pennycook, G., & Rand, D. G. (2019). Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning. Cognition, 188, 39–50. https://doi.org/10.1016/j.cognition.2018.06.011\",\n         veracity = ifelse(reality == \"fake\", \"false\", \"true\"),\n         intervention_label = ifelse(condition == \"Control\", NA, tolower(condition)),\n         condition = ifelse(condition == \"Control\", \"control\", \"treatment\"), \n         accuracy_raw = perceived_accu,\n         concordance = tolower(concordancy),\n         partisan_identity = ifelse(DemRep_C &lt;= 3, \"democrat\", \"republican\"),\n         news_slant = case_when(partisan_identity == \"democrat\" & \n                                  concordance == \"concordant\" ~ \"democrat\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"concordant\" ~ \"republican\", \n                                partisan_identity == \"democrat\" & \n                                  concordance == \"discordant\" ~ \"republican\", \n                                partisan_identity == \"republican\" & \n                                  concordance == \"discordant\" ~ \"democrat\", \n                                TRUE ~ NA_character_\n                                ),\n         experiment_id = 4,\n         age = Age, \n         # again, no direct info on year, we impute year of data collection reported for Study 3\n         year = 2020) \n\n\n\n\nCombine and add identifiers (country, paper_id) and news_selection\nIn study 2 and 3, the same news pool has been used. In study 4, a different pool has been used. We take this into account when merging the different studies.\n\n\nCode\n## Combine + add remaining variables\nbago_2022 &lt;- bind_rows(d2, d3, d4) |&gt; \n  mutate(scale = \"binary\", \n         country = \"United States\",\n         paper_id = \"bago_2022\", \n         # news id \n         news_id = ifelse(experiment_id %in% c(2,3), news_id, \n                          paste0(experiment_id, \"_\", news_id)\n         ), \n         news_selection = \"researchers\", \n         ) |&gt; \n# add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n# check\n# bago_2022 |&gt;\n#   group_by(news_id) |&gt;\n#   summarize(n_observations = n()) |&gt; \n#   arrange(as.numeric(news_id))\n\nbago_2022 |&gt; \n  group_by(experiment_id, condition, intervention_label) |&gt; \n  count()\n\n\n# A tibble: 7 × 4\n# Groups:   experiment_id, condition, intervention_label [7]\n  experiment_id condition intervention_label     n\n          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;int&gt;\n1             2 control   &lt;NA&gt;                5408\n2             2 treatment reappraisal         5408\n3             2 treatment suppression         5296\n4             3 control   &lt;NA&gt;               23984\n5             3 treatment suppression        24032\n6             4 control   &lt;NA&gt;               24103\n7             4 treatment suppression        24233"
  },
  {
    "objectID": "data/papers/bago_2022/bago_2022.html#write-out-data",
    "href": "data/papers/bago_2022/bago_2022.html#write-out-data",
    "title": "Emotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.",
    "section": "Write out data",
    "text": "Write out data\n\n\nCode\nsave_data(bago_2022)"
  },
  {
    "objectID": "data/papers/brashier_2021/brashier_2021.html",
    "href": "data/papers/brashier_2021/brashier_2021.html",
    "title": "Timing Matters When Correcting Fake News.",
    "section": "",
    "text": "Brashier, Nadia M., Gordon Pennycook, Adam J. Berinsky, and David G. Rand. 2021. “Timing Matters When Correcting Fake News.” Proceedings of the National Academy of Sciences 118 (5): e2020043118. https://doi.org/10.1073/pnas.2020043118."
  },
  {
    "objectID": "data/papers/brashier_2021/brashier_2021.html#reference",
    "href": "data/papers/brashier_2021/brashier_2021.html#reference",
    "title": "Timing Matters When Correcting Fake News.",
    "section": "",
    "text": "Brashier, Nadia M., Gordon Pennycook, Adam J. Berinsky, and David G. Rand. 2021. “Timing Matters When Correcting Fake News.” Proceedings of the National Academy of Sciences 118 (5): e2020043118. https://doi.org/10.1073/pnas.2020043118."
  },
  {
    "objectID": "data/papers/brashier_2021/brashier_2021.html#intervention",
    "href": "data/papers/brashier_2021/brashier_2021.html#intervention",
    "title": "Timing Matters When Correcting Fake News.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'Two-wave panel design: In the treatment conditions, participants saw “true” and “false” tags either immediately before (prebunking), during (veracity_labels), or immediately after (debunking) reading. In the control condition, participants rated the headlines alone, with no tags. One week later, all participants judged the same 36 headlines for accuracy, this time with no veracity information. We will only consider ratings from the first wave. Study 2 is essentially a replication of Study 1, but with a new 3s intervall in the prebunking condition.',\n    intervention_selection = \"veracity_labels\",\n    intervention_selection_description = 'We will select the \"during\" (veracity_labels) condition, that is, the condition in which participants saw a headline with true/false tags. The debunking condition is relevant in the study context for long-term effects, but but should not affect ratings of the first wave, which we will look at. As for prebunking, it is unclear how this has been done, from the paper.',\n    originally_identified_treatment_effect = TRUE, \n    control_format = \"picture, source\"\n      )\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\nintervention_selection_description\n\n\n\n\nTwo-wave panel design: In the treatment conditions, participants saw “true” and “false” tags either immediately before (prebunking), during (veracity_labels), or immediately after (debunking) reading. In the control condition, participants rated the headlines alone, with no tags. One week later, all participants judged the same 36 headlines for accuracy, this time with no veracity information. We will only consider ratings from the first wave. Study 2 is essentially a replication of Study 1, but with a new 3s intervall in the prebunking condition.\nWe will select the \"during\" (veracity_labels) condition, that is, the condition in which participants saw a headline with true/false tags. The debunking condition is relevant in the study context for long-term effects, but but should not affect ratings of the first wave, which we will look at. As for prebunking, it is unclear how this has been done, from the paper.\n\n\n\n\n\n\n\n\nNotes\nThe authors found a positive treatment effect on news discernment for the label condition:\n\n“We found consistent evidence that the timing of fact-checks matters: “True” and “false” tags that appeared immediately after headlines (debunking) reduced misclassification of headlines 1 wk later by 25.3%, compared to an 8.6% reduction when tags appeared during exposure (labeling), and a 6.6% increase (Experiment 1) or 5.7% reduction (Experiment 2) when tags appeared beforehand (prebunking).”"
  },
  {
    "objectID": "data/papers/brashier_2021/brashier_2021.html#data-cleaning",
    "href": "data/papers/brashier_2021/brashier_2021.html#data-cleaning",
    "title": "Timing Matters When Correcting Fake News.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nStudy 1\n\n\nCode\nd &lt;- read_excel(\"brashier_2021-study_1.xlsx\", \n                                sheet = \"Sorted\")\n\n\nNew names:\n• `` -&gt; `...78`\n\n\nCode\nhead(d)\n\n\n# A tibble: 6 × 78\n      P Party `CRT Correct` `Poli Knowl` Condition `A1 - Initial` `A2 - Initial`\n  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 10001 Dem               1            3 Before                 2              1\n2 10002 Ind               1            1 Before                 1              1\n3 10003 Ind               5            4 Before                 1              1\n4 10004 Dem               5            2 Before                 3              1\n5 10005 Ind               5            2 Before                 1              1\n6 10006 Dem               7            4 Before                 4              1\n# ℹ 71 more variables: `A3 - Initial` &lt;dbl&gt;, `A4 - Initial` &lt;dbl&gt;,\n#   `A5 - Initial` &lt;dbl&gt;, `A6 - Initial` &lt;dbl&gt;, `A7 - Initial` &lt;dbl&gt;,\n#   `A8 - Initial` &lt;dbl&gt;, `A9 - Initial` &lt;dbl&gt;, `A10 - Initial` &lt;dbl&gt;,\n#   `A11 - Initial` &lt;dbl&gt;, `A12 - Initial` &lt;dbl&gt;, `A13 - Initial` &lt;dbl&gt;,\n#   `A14 - Initial` &lt;dbl&gt;, `A15 - Initial` &lt;dbl&gt;, `A16 - Initial` &lt;dbl&gt;,\n#   `A17 - Initial` &lt;dbl&gt;, `A18 - Initial` &lt;dbl&gt;, `B1 - Initial` &lt;dbl&gt;,\n#   `B2 - Initial` &lt;dbl&gt;, `B3 - Initial` &lt;dbl&gt;, `B4 - Initial` &lt;dbl&gt;, …\n\n\n\naccuracy_raw, veracity\nThe data comes in wide format, with one column per headline. We first change this to long format.\n\n\nCode\n# Gather the wide columns into long format\ndata_long &lt;- d |&gt; \n  pivot_longer(\n    cols = matches(\"^(A|B)[0-9]+\\\\s-\\\\s(Initial|Final)$\"),\n    names_to = c(\"prefix\", \"item\", \"time\"),\n    names_pattern = \"(A|B)([0-9]+) - (Initial|Final)\",\n    values_to = \"accuracy_raw\"\n  )\n\n\nFrom the stata code provided by the authors, we know that variables preceded by “A” are false news, and those preceded by “B” are true news.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(\n    veracity = if_else(prefix == \"A\", \"false\", \"true\")\n    )\n\n# plausibility check\n# data_long |&gt; \n#   group_by(veracity) |&gt; \n#   summarise(mean_accuracy = mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nCode\ntable(data_long$accuracy_raw, useNA = \"always\")\n\n\n\n    0     1     2     3     4  &lt;NA&gt; \n    1 42563 14404 23738 25926  9432 \n\n\nOne person gave an accuracy rating of 0. This is likely a coding error, we’ll remove this observation.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  filter(accuracy_raw != 0) \n\n\n\n\nnews_id\n\n\nCode\ntable(data_long$item)\n\n\n\n   1   10   11   12   13   14   15   16   17   18    2    3    4    5    6    7 \n5924 5924 5924 5924 5924 5924 5924 5924 5924 5924 5924 5924 5924 5924 5923 5924 \n   8    9 \n5924 5924 \n\n\nThere are only 18 different item identifiers. That is presumably, because these numbers only identify items within each veracity category.\n\n\nCode\ndata_long |&gt; \n  group_by(veracity) |&gt; \n  summarise(n_distinct(item))\n\n\n# A tibble: 2 × 2\n  veracity `n_distinct(item)`\n  &lt;chr&gt;                 &lt;int&gt;\n1 false                    18\n2 true                     18\n\n\nFor our news identifier, we therefore combine the veracity variable with these identifiers.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(news_id = paste0(veracity, \"_\", item))\n\n\n\n\nTime point\nTwo-wave panel design: In the treatment conditions, participants saw “true” and “false” tags immediately before, during, or immediately after reading. In the control condition, participants rated the headlines alone, with no tags. One week later, all participants judged the same 36 headlines for accuracy, this time with no veracity information.\nIn our study, we want to exclude the ratings at the second time point (“Final”), because previous exposure to the headlines would make the results not comparable with other studies, where participants haven’t been (at least in a study context) exposed to the headlines previously.\n\n\nCode\ntable(data_long$time)\n\n\n\n  Final Initial \n  48600   58031 \n\n\nCode\ndata_long &lt;- data_long |&gt; \n  filter(time == \"Initial\")\n\n\n\n\nConcordance (concordance, partisan_identity, news_slant)\nFrom the stata code, we know the political slant of the headlines. We combine this with participants partisan identity to code concordance\n\n\nCode\n# Then code whether the headline is pro-Democratic:\npro_democrat_items &lt;- c(2, 6, 9, 11, 13, 14, 15, 17, 18, 19, 21, 26, 27, 30, 31, 33, 34, 36)\n\ndata_long &lt;- data_long |&gt; \n  mutate(\n    news_slant = ifelse(news_id %in% pro_democrat_items, \"democrat\", \"republican\"),\n    partisan_identity = case_when(\n      Party == \"Dem\" ~ \"democrat\", \n      Party == \"Repub\" ~ \"republican\", \n      TRUE ~ NA_character_), \n    # Make concordance variable\n    concordance = ifelse(partisan_identity == news_slant, \"concordant\", \"discordant\")\n    )\n\n\n\n\nConditions (intervention_label, condition)\n\n\nCode\ndata_long |&gt; \n  distinct(Condition)\n\n\n# A tibble: 4 × 1\n  Condition\n  &lt;chr&gt;    \n1 Before   \n2 During   \n3 After    \n4 Control  \n\n\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(intervention_label = case_when(\n           Condition == \"Before\" ~ \"prebunking\", \n           Condition == \"During\" ~ \"veracity_labels\", \n           Condition == \"After\" ~ \"debunking\", \n           TRUE ~ NA_character_\n         ),\n         condition = ifelse(Condition == \"Control\", \"control\", \"treatment\")) \n\n#check\ntable(data_long$intervention_label, useNA = \"always\")\n\n\n\n      debunking      prebunking veracity_labels            &lt;NA&gt; \n          14580           14544           14291           14616 \n\n\n\n\nage\nThere is no age variable in the data.\n\n\nyear\nThere is no date variable in the data. It’s not clear from the paper, the supplement, or the pre-registraiton when the data has been collected. We have to rely on the publication date.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(year = 2021)\n\n\n\n\nscale\n\n\nCode\ntable(data_long$accuracy_raw, useNA = \"always\")\n\n\n\n    1     2     3     4  &lt;NA&gt; \n22571  7442 12455 15563     0 \n\n\n\n\nCode\ndata_long &lt;- data_long |&gt;\n  mutate(scale = 4)\n\n\n\n\nIdentifiers (subject_id, experiment_id)\n\n\nCode\nd1 &lt;- data_long |&gt; \n  mutate(subject_id = P, \n         experiment_id = 1) \n\n\n\n\n\nStudy 2\n\n\nCode\nd &lt;- read_excel(\"brashier_2021-study_2.xlsx\", \n                                sheet = \"Sorted\")\nhead(d)\n\n\n# A tibble: 6 × 77\n      P Party `CRT Correct` `Poli Knowl` Condition `A1 - Initial` `A2 - Initial`\n  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 20001 Dem               2            3 Before                 1              1\n2 20002 Ind               2            5 Before                 1              1\n3 20003 Dem               0            3 Before                 2              1\n4 20004 Ind               4            5 Before                 1              1\n5 20005 Dem               5            4 Before                 1              2\n6 20006 Repub             7            5 Before                 1              1\n# ℹ 70 more variables: `A3 - Initial` &lt;dbl&gt;, `A4 - Initial` &lt;dbl&gt;,\n#   `A5 - Initial` &lt;dbl&gt;, `A6 - Initial` &lt;dbl&gt;, `A7 - Initial` &lt;dbl&gt;,\n#   `A8 - Initial` &lt;dbl&gt;, `A9 - Initial` &lt;dbl&gt;, `A10 - Initial` &lt;dbl&gt;,\n#   `A11 - Initial` &lt;dbl&gt;, `A12 - Initial` &lt;dbl&gt;, `A13 - Initial` &lt;dbl&gt;,\n#   `A14 - Initial` &lt;dbl&gt;, `A15 - Initial` &lt;dbl&gt;, `A16 - Initial` &lt;dbl&gt;,\n#   `A17 - Initial` &lt;dbl&gt;, `A18 - Initial` &lt;dbl&gt;, `B1 - Initial` &lt;dbl&gt;,\n#   `B2 - Initial` &lt;dbl&gt;, `B3 - Initial` &lt;dbl&gt;, `B4 - Initial` &lt;dbl&gt;, …\n\n\n\naccuracy_raw, veracity\nThe data comes in wide format, with one column per headline. We first change this to long format.\n\n\nCode\n# Gather the wide columns into long format\ndata_long &lt;- d |&gt; \n  pivot_longer(\n    cols = matches(\"^(A|B)[0-9]+\\\\s-\\\\s(Initial|Final)$\"),\n    names_to = c(\"prefix\", \"item\", \"time\"),\n    names_pattern = \"(A|B)([0-9]+) - (Initial|Final)\",\n    values_to = \"accuracy_raw\"\n  )\n\n\nFrom the stata code provided by the authors, we know that variables preceeded by “A” are false news, and those preceeded by “B” are true news.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(\n    veracity = if_else(prefix == \"A\", \"false\", \"true\")\n    )\n\n# plausibility check\n# data_long |&gt; \n#   group_by(veracity) |&gt; \n#   summarise(mean_accuracy = mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nCode\ntable(data_long$accuracy_raw, useNA = \"always\")\n\n\n\n    0     1     2     3     4  &lt;NA&gt; \n   31 40700 14322 25513 27038 11628 \n\n\nThis time, 31 people gave an accuracy rating of 0. Again, this is likely a coding error, and we’ll remove these observations.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  filter(accuracy_raw != 0) \n\n\n\n\nnews_id\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(news_id = paste0(veracity, \"_\", item))\n\n\n\n\nTime point\nAs before, we want to exclude the ratings at the second time point (“Final”).\n\n\nCode\ntable(data_long$time)\n\n\n\n  Final Initial \n  47988   59585 \n\n\nCode\ndata_long &lt;- data_long |&gt; \n  filter(time == \"Initial\")\n\n\n\n\nConcordance (concordance, partisan_identity, news_slant)\nFrom the stata code, we know the political slant of the headlines. We combine this with participants partisan identity to code concordance\n\n\nCode\n# Then code whether the headline is pro-Democratic:\npro_democrat_items &lt;- c(2, 6, 9, 11, 13, 14, 15, 17, 18, 19, 21, 26, 27, 30, 31, 33, 34, 36)\n\ndata_long &lt;- data_long |&gt; \n  mutate(\n    news_slant = ifelse(news_id %in% pro_democrat_items, \"democrat\", \"republican\"),\n    partisan_identity = case_when(\n      Party == \"Dem\" ~ \"democrat\", \n      Party == \"Repub\" ~ \"republican\", \n      TRUE ~ NA_character_), \n    # Make concordance variable\n    concordance = ifelse(partisan_identity == news_slant, \"concordant\", \"discordant\")\n    )\n\n\n\n\nConditions (intervention_label, condition)\n\n\nCode\ndata_long |&gt; \n  distinct(Condition)\n\n\n# A tibble: 4 × 1\n  Condition\n  &lt;chr&gt;    \n1 Before   \n2 During   \n3 After    \n4 Control  \n\n\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(intervention_label = case_when(\n           Condition == \"Before\" ~ \"prebunking\", \n           Condition == \"During\" ~ \"veracity_labels\", \n           Condition == \"After\" ~ \"debunking\", \n           TRUE ~ NA_character_\n         ),\n         condition = ifelse(Condition == \"Control\", \"control\", \"treatment\")) \n\n\n\n\nage\nThere is no age variable in the data.\n\n\nyear\nThere is no date variable in the data. It’s not clear from the paper, the supplement, or the pre-registraiton when the data has been collected. We have to rely on the publication date.\n\n\nCode\ndata_long &lt;- data_long |&gt; \n  mutate(year = 2021)\n\n\n\n\nscale\n\n\nCode\ntable(data_long$accuracy_raw, useNA = \"always\")\n\n\n\n    1     2     3     4  &lt;NA&gt; \n21810  7705 13961 16109     0 \n\n\n\n\nCode\ndata_long &lt;- data_long |&gt;\n  mutate(scale = 4)\n\n\n\n\nIdentifiers (subject_id, experiment_id)\n\n\nCode\nd2 &lt;- data_long |&gt; \n  mutate(subject_id = P, \n         experiment_id = 2) \n\n\n\n\n\nCombine and add identifiers (country, paper_id)\nWe combine both studies.\n\n\nCode\n## Combine + add remaining variables\nbrashier_2021 &lt;- bind_rows(d1, d2) |&gt; \n  mutate(country = \"United States\",\n         paper_id = \"brashier_2021\") |&gt; \n  # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n\nNew names:\n• `...78` -&gt; `...6`\n\n\nCode\n# check\n# brashier_2021 |&gt;\n#   group_by(paper_id, experiment_id) |&gt;\n#   summarize(n_observations = n()) \n\n\nSince in both studies the same news have been used (with the same labels), we can just keep the labels\n\n\nCode\nbrashier_2021 |&gt; \n  group_by(news_id) |&gt; \n  count()\n\n\n# A tibble: 36 × 2\n# Groups:   news_id [36]\n   news_id      n\n   &lt;chr&gt;    &lt;int&gt;\n 1 false_1   3267\n 2 false_10  3267\n 3 false_11  3268\n 4 false_12  3267\n 5 false_13  3266\n 6 false_14  3267\n 7 false_15  3268\n 8 false_16  3266\n 9 false_17  3266\n10 false_18  3268\n# ℹ 26 more rows\n\n\n\nnews_selection\n\n\nCode\n## Combine + add remaining variables\nbrashier_2021 &lt;- brashier_2021 |&gt; \n  mutate(news_selection = \"researchers\")"
  },
  {
    "objectID": "data/papers/brashier_2021/brashier_2021.html#write-out-data",
    "href": "data/papers/brashier_2021/brashier_2021.html#write-out-data",
    "title": "Timing Matters When Correcting Fake News.",
    "section": "Write out data",
    "text": "Write out data\n\n\nCode\nsave_data(brashier_2021)"
  },
  {
    "objectID": "data/papers/altay_2023/altay_2023.html",
    "href": "data/papers/altay_2023/altay_2023.html",
    "title": "People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.",
    "section": "",
    "text": "Altay, Sacha, and Fabrizio Gilardi. 2024. “People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.” PNAS Nexus 3 (10): pgae403. https://doi.org/10.1093/pnasnexus/pgae403."
  },
  {
    "objectID": "data/papers/altay_2023/altay_2023.html#reference",
    "href": "data/papers/altay_2023/altay_2023.html#reference",
    "title": "People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.",
    "section": "",
    "text": "Altay, Sacha, and Fabrizio Gilardi. 2024. “People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.” PNAS Nexus 3 (10): pgae403. https://doi.org/10.1093/pnasnexus/pgae403."
  },
  {
    "objectID": "data/papers/altay_2023/altay_2023.html#intervention",
    "href": "data/papers/altay_2023/altay_2023.html#intervention",
    "title": "People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'In Study 1, participants were randomly assigned to one of the five following conditions: (i) the Control Condition in which no headline was labeled, (ii) the correct label condition in which all AI-generated headlines were labeled (`intervention_label` = \"Correct\"), (iii) the missing label condition in which only half of AI-generated headlines were labeled (`intervention_label` = \"Missing\"), (iv) the noisy label condition in which half of AI-generated headlines were labeled and half of human-generated headlines were mislabeled (`intervention_label` = \"Noise \"), and (v) the false label condition in which false headlines were labeled as false (`intervention_label` = \"FalseLabel\")',\n    intervention_selection = \"FalseLabel\",\n    intervention_selection_description = 'The paper\\'s main goal is to test how AI labels affect accuracy judgments. However, this is not the main interest of our study. We will therefor reduce the treatment to the condition in Study 1 where false headlines are labeled as false (`intervention_label` = \"FalseLabel\").',\n    #the authors did not measure discernment \n    originally_identified_treatment_effect = NA,\n    control_format = \"picture, lede\")\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\nintervention_selection_description\n\n\n\n\nIn Study 1, participants were randomly assigned to one of the five following conditions: (i) the Control Condition in which no headline was labeled, (ii) the correct label condition in which all AI-generated headlines were labeled (`intervention_label` = \"Correct\"), (iii) the missing label condition in which only half of AI-generated headlines were labeled (`intervention_label` = \"Missing\"), (iv) the noisy label condition in which half of AI-generated headlines were labeled and half of human-generated headlines were mislabeled (`intervention_label` = \"Noise \"), and (v) the false label condition in which false headlines were labeled as false (`intervention_label` = \"FalseLabel\")\nThe paper's main goal is to test how AI labels affect accuracy judgments. However, this is not the main interest of our study. We will therefor reduce the treatment to the condition in Study 1 where false headlines are labeled as false (`intervention_label` = \"FalseLabel\").\n\n\n\n\n\n\n\n\nNotes\nIn Study 2, all treatment conditions are only about AI labels. These seem not directly relevant to our study. We will therefore exclude Study 2.\n\n“In Study 2 […] we introduced three new conditions in which participants were provided with definitions explaining what it meant for a headline to be AI-generated. In the Weak Condition, participants were told that AI was used to improve the clarity of the text and adapt its style. In the Medium Condition, participants were told that AI contributed more substantially by writing a first draft of the article, while in the Strong Condition AI chose the topic of the article and wrote the whole article.”\n\nThe authors do not report an effect on discernment (only an effect is on all accuracy ratings false and true news).\n\nRegarding our condition of interest–false labels–the authors find that: “We found that the false labels reduced accuracy and sharing ratings by 0.56 points [-0.70, -0.42]. The false labels had a similar effect on the perceived accuracy of the headlines (b = 0.58 [-0.75, -0.41]) and sharing intentions (b = 0.52 [-0.73, -0.30]).”"
  },
  {
    "objectID": "data/papers/altay_2023/altay_2023.html#data-cleaning",
    "href": "data/papers/altay_2023/altay_2023.html#data-cleaning",
    "title": "People Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nRead data and inspect key variables.\n\n\nCode\nd &lt;- read_excel(\"Altay_2023-study_1.xlsx\")\n\n# inspect key variables to get an overview\nd |&gt; \n  select(PROLIFIC_PID, True_False, AI_Human, Condition, Conditions, DV, Ratings, News_number) |&gt; \n  arrange(PROLIFIC_PID)\n\n\n# A tibble: 31,536 × 8\n   PROLIFIC_PID           True_False AI_Human Condition Conditions DV    Ratings\n   &lt;chr&gt;                  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;\n 1 54846df3fdf99b0379939… false      AI       FalseLab… FalseLabel Accu…       2\n 2 54846df3fdf99b0379939… true       AI       FalseLab… FalseLabel Accu…       5\n 3 54846df3fdf99b0379939… false      AI       FalseLab… FalseLabel Accu…       2\n 4 54846df3fdf99b0379939… true       AI       FalseLab… FalseLabel Accu…       5\n 5 54846df3fdf99b0379939… false      AI       FalseLab… FalseLabel Accu…       2\n 6 54846df3fdf99b0379939… true       AI       FalseLab… FalseLabel Accu…       5\n 7 54846df3fdf99b0379939… false      AI       FalseLab… FalseLabel Accu…       2\n 8 54846df3fdf99b0379939… true       AI       FalseLab… FalseLabel Accu…       6\n 9 54846df3fdf99b0379939… false      human    FalseLab… FalseLabel Accu…       2\n10 54846df3fdf99b0379939… true       human    FalseLab… FalseLabel Accu…       6\n# ℹ 31,526 more rows\n# ℹ 1 more variable: News_number &lt;dbl&gt;\n\n\n\nConditions (intervention_label, condition)\nGet an overview of conditions. There are two candidate variables (Condition and Conditions).\n\n\nCode\ntable(d$Condition, useNA = \"always\")\n\n\n\n   Control    Correct FalseLabel    Missing      Noise       &lt;NA&gt; \n      6320       6288       6288       6304       6336          0 \n\n\nCode\ntable(d$Conditions, useNA = \"always\")\n\n\n\n          Control           Correct        FalseLabel           Missing \n             6320              6288              6288              6304 \nNoise_mislabelled  Noise_unlabelled              &lt;NA&gt; \n             1584              4752                 0 \n\n\nThe Conditions variable is slightly more detailed, with two noise conditions. However, since in the paper the authors only report 5 conditions corresponding to the Condition variable, we will rely on that one.\n\n\nCode\nd &lt;- d |&gt; \n  mutate(\n    # make sure that the control conditions has no intervention label\n    intervention_label = ifelse(str_detect(Condition, \"Control\"),\n                                NA,\n                                Condition), \n    # keep different labels for control conditions, code treatment as \"treatment\"\n    condition = ifelse(Condition == \"Control\", \"control\", \"treatment\")\n    )\n\n# check\n# d |&gt;\n#   group_by(condition, intervention_label) |&gt;\n#   count()\n\n\n\n\naccuracy_raw, scale\nCheck the dependent variable.\n\n\nCode\ntable(d$DV, useNA= \"always\")\n\n\n\nAccuracy  Sharing     &lt;NA&gt; \n   15792    15744        0 \n\n\nThe data is in long format data, such tat Ratings codes the outcome score for both sharing and accuracy. We reduce the data to only accuracy ratings.\n\n\nCode\nd |&gt; \n  group_by(DV) |&gt; \n  reframe(unique(Ratings))\n\n\n# A tibble: 12 × 2\n   DV       `unique(Ratings)`\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Accuracy                 1\n 2 Accuracy                 2\n 3 Accuracy                 4\n 4 Accuracy                 3\n 5 Accuracy                 5\n 6 Accuracy                 6\n 7 Sharing                  4\n 8 Sharing                  5\n 9 Sharing                  1\n10 Sharing                  2\n11 Sharing                  3\n12 Sharing                  6\n\n\n\n\nCode\n# long format data, `Ratings` codes the outcome for both sharing and accuracy, \n# so we have to filter DV == Accuracy. \n# Also, remove some treatment conditions that are irrelevant for our study\nd &lt;- d |&gt; \n  filter(DV == \"Accuracy\") |&gt; \n  mutate(accuracy_raw = Ratings, \n         scale = 6)\n\n\n\n\nveracity\n\n\nCode\nd &lt;- d |&gt; \n  mutate(\n    veracity = ifelse(True_False == \"false\", \"false\", \"true\")\n    )\n\n\n\n\nnews_id, news_selection\n\n\nCode\nd |&gt; \n  group_by(News_number) |&gt; \n  count()\n\n\n# A tibble: 8 × 2\n# Groups:   News_number [8]\n  News_number     n\n        &lt;dbl&gt; &lt;int&gt;\n1           1  1974\n2           2  1974\n3           3  1974\n4           4  1974\n5           5  1974\n6           6  1974\n7           7  1974\n8           8  1974\n\n\nThere are only 8 different news ids. However, from the paper, we know that there were 8 different news items per veracity condition (i.e. 8 true and 8 false items). We thus build a new news identifier variable combining the two.\n\n\nCode\nd &lt;- d |&gt; \n  mutate(news_id = paste0(veracity, \"_\", News_number), \n         news_selection = \"researchers and AI\")\n\n\n\n\npartisan_identity\n\n\nCode\nd &lt;- d |&gt; \n  mutate(partisan_identity = tolower(Political_orientation))\n\n\n\n\nIdentifiers (country, paper_id, subject_id, experiment_id) and age\n\n\nCode\n# make final data\naltay_2023 &lt;- d |&gt; \n  mutate(\n    subject_id = PROLIFIC_PID,\n    experiment_id = 1,\n    age = Age,\n    country = \"United States\",\n    paper_id = \"altay_2023\") |&gt; \n  # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n\n# check conditions\n# Altay_2023 |&gt;\n#   group_by(condition) |&gt;\n#   reframe(unique(intervention_label))\n\n\n\n\nWrite out data\n\n\nCode\nsave_data(altay_2023)"
  },
  {
    "objectID": "data/papers/dias_2020/dias_2020.html",
    "href": "data/papers/dias_2020/dias_2020.html",
    "title": "Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.",
    "section": "",
    "text": "Dias, Nicholas, Gordon Pennycook, and David G. Rand. 2020. “Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.” Harvard Kennedy School Misinformation Review, January. https://doi.org/10.37016/mr-2020-001."
  },
  {
    "objectID": "data/papers/dias_2020/dias_2020.html#reference",
    "href": "data/papers/dias_2020/dias_2020.html#reference",
    "title": "Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.",
    "section": "",
    "text": "Dias, Nicholas, Gordon Pennycook, and David G. Rand. 2020. “Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.” Harvard Kennedy School Misinformation Review, January. https://doi.org/10.37016/mr-2020-001."
  },
  {
    "objectID": "data/papers/dias_2020/dias_2020.html#intervention",
    "href": "data/papers/dias_2020/dias_2020.html#intervention",
    "title": "Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'Study 1: In the control condition, participants saw Facebook-like news posts, with the source domain shwon in gray text. Two treatment conditions: in one the logo of publisher outlet was shown in a bright banner (logo banner); in the other no source was shown (neither gray text, nor logo banner). Study 2: Like study 1, but without the \"no source\" condition.',\n    control_format = \"facebook\",\n    control_selection = \"facebook\",\n    control_selection_description = \"For Study 1, we will use the Facebook like condition (facebook) and NOT the condition without a source (no_source) as a control group, since it matches the control group of Study 2, is more comparable with other studies\\' control groups, and is closer to real-world settings.\",\n    originally_identified_treatment_effect = FALSE)\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\ncontrol_selection_description\n\n\n\n\nStudy 1: In the control condition, participants saw Facebook-like news posts, with the source domain shwon in gray text. Two treatment conditions: in one the logo of publisher outlet was shown in a bright banner (logo banner); in the other no source was shown (neither gray text, nor logo banner). Study 2: Like study 1, but without the \"no source\" condition.\nFor Study 1, we will use the Facebook like condition (facebook) and NOT the condition without a source (no_source) as a control group, since it matches the control group of Study 2, is more comparable with other studies' control groups, and is closer to real-world settings.\n\n\n\n\n\n\n\n\nNotes\nStudies 3, 4 and 5 are not relevant, as participants did not provide accuracy ratings, but instead rated the trustworthiness of different sources. Study 6 would in principle be relevant, as it tests an intervention of showcasing the source. However, other than in Studies 1 and 2, in the baseline control condition of Study 6, the text of the news headline was presented in isolation, i.e. plain text and no a source. The treatment effect here is thus not highlighting a source, but adding a source in the first place. Since many other studies use a facebook format where the source is present, and since this format is more realistic in real-world context, we prefer using this as a baseline. We therefore exclude Study 6.\nSomething that is weird: In Study 1, there is a slightly reduced number of WorkerIDs than there are ResponseIDs, suggesting that some individuals might have taken the survey several times (see below). In Study 2, there is no ResponseID variable, but in the original wide format data there is also one more completed study (i.e. line) than individual WorkerIDs (see below).\nTo err on the side of caution, we exclude all WorkerIDs with multiple survey takes."
  },
  {
    "objectID": "data/papers/dias_2020/dias_2020.html#data-cleaning",
    "href": "data/papers/dias_2020/dias_2020.html#data-cleaning",
    "title": "Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nStudy 1\n\n\nCode\nd &lt;- read_csv(\"dias_2020-study_1.csv\") \n\n\nRows: 563 Columns: 602\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (14): Party_TEXT, IPAddress, StartDate, EndDate, WorkerID, Comments, Le...\ndbl (588): Condition, Fake, Real, Fake_C, Real_C, Fake_L, Real_L, Fake_PC, F...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(d)\n\n\n# A tibble: 6 × 602\n  Condition  Fake  Real Fake_C Real_C Fake_L Real_L Fake_PC Fake_nPC Real_PC\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1         1  1.25  1.17   1      1      1.5    1.33    1.5      1       1.33\n2         1  1.67  2.17   1.5    2      1.83   2.33    1.83     1.5     2.33\n3         1  1.75  3.08   1.33   2.83   2.17   3.33    2.17     1.33    3.33\n4         1  1.25  2.42   1      3      1.5    1.83    1.5      1       1.83\n5         1  1.17  3.17   1      2.83   1.33   3.5     1.33     1       3.5 \n6         1  1.58  2      1.33   2      1.83   2       1.83     1.33    2   \n# ℹ 592 more variables: Real_nPC &lt;dbl&gt;, Discernment &lt;dbl&gt;, C_Discernment &lt;dbl&gt;,\n#   L_Discernment &lt;dbl&gt;, PC_Discernment &lt;dbl&gt;, nPC_Discernment &lt;dbl&gt;,\n#   CRT_split &lt;dbl&gt;, ClintonTrump &lt;dbl&gt;, Media_Leaders &lt;dbl&gt;, Media_Bias &lt;dbl&gt;,\n#   Fake_SM &lt;dbl&gt;, Real_SM &lt;dbl&gt;, Fake_SM_C &lt;dbl&gt;, Real_SM_C &lt;dbl&gt;,\n#   Fake_SM_L &lt;dbl&gt;, Real_SM_L &lt;dbl&gt;, Fake_SM_PC &lt;dbl&gt;, Fake_SM_nPC &lt;dbl&gt;,\n#   Real_SM_PC &lt;dbl&gt;, Real_SM_nPC &lt;dbl&gt;, SocialMedia_Chk &lt;dbl&gt;, CRT_ACC &lt;dbl&gt;,\n#   CRT_Rand &lt;dbl&gt;, CRT_Thomson &lt;dbl&gt;, Age &lt;dbl&gt;, Sex &lt;dbl&gt;, Education &lt;dbl&gt;, …\n\n\n\naccuracy_raw, veracity\nThere is no documentation. But from the stata code the authors provide, we know that:\n\nColumns that, in their endings, contain _2 (like Fake1_2 or Real1_2) represent accuracy ratings made by participants for each news item. It’s a mess, apparently each control condition has their own different outcome variable.\nReal and Fake in the titles refer to whether the news item was true (real) or **false (fake`).\n\nWe bring the data into long format and build an accuracy outcome column.\n\n\nCode\nd_long &lt;- d |&gt;\n  pivot_longer(\n    cols = matches(\"^(Real|Fake)\\\\d+_(2(\\\\.\\\\d)?|3(\\\\.\\\\d)?)$\"),  # match _2, _2.0, _2.1, _3, _3.0, _3.1\n    names_to = c(\"veracity\", \"item\", \"measure\"),\n    names_pattern = \"^(Real|Fake)(\\\\d+)_([23](?:\\\\.\\\\d)?)$\", \n    values_to = \"value\"\n  ) |&gt;\n  mutate(\n    measure = case_when(\n      str_starts(measure, \"2\") ~ \"accuracy\",\n      str_starts(measure, \"3\") ~ \"sharing\"\n    )\n  ) |&gt;\n  # remove all NAs, which inevitably are existant, since each condition\n  drop_na(value) |&gt; \n  pivot_wider(names_from = measure, values_from = value) |&gt; \n  rename(accuracy_raw = accuracy)\n\n# check\nd_long |&gt;\n  group_by(Condition) |&gt;\n  summarise(n_participants = n_distinct(WorkerID),\n            n_valid_response = sum(!is.na(accuracy_raw)))\n\n\n# A tibble: 3 × 3\n  Condition n_participants n_valid_response\n      &lt;dbl&gt;          &lt;int&gt;            &lt;int&gt;\n1         1            190             4555\n2         2            181             4340\n3         3            191             4605\n\n\nWe code the veracity variable.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(\n    veracity = if_else(veracity == \"Fake\", \"false\", \"true\")\n    ) \n\n\n\n\nscale\n\n\nCode\ntable(d_long$accuracy_raw, useNA = \"always\")\n\n\n\n   1    2    3    4 &lt;NA&gt; \n4930 3388 3822 1360   11 \n\n\n\n\nCode\nd_long &lt;- d_long |&gt;\n  mutate(scale = 4)\n\n\n\n\nConditions (intervention_label, control_label, condition)\nFrom the stata code we can conclude that in Study 1, the conditions are no_source (coded as 1), facebook (coded 2), highlight_banner (coded as 3). In study 2, the conditions are coded as facebook (coded 1), highlight_banner (coded as 2).\n\n\nCode\n# check\nd_long |&gt; \n  group_by(Condition) |&gt; \n  summarise(mean(accuracy_raw, na.rm=TRUE))\n\n\n# A tibble: 3 × 2\n  Condition `mean(accuracy_raw, na.rm = TRUE)`\n      &lt;dbl&gt;                              &lt;dbl&gt;\n1         1                               2.12\n2         2                               2.08\n3         3                               2.16\n\n\nWe code the condition variable.\n\n\nCode\nd_long &lt;- d_long |&gt;\n  mutate(\n    intervention_label = case_when(\n      Condition == 3 ~ \"highlight_banner\",\n      TRUE ~ NA_character_\n    ),\n    control_label = case_when(\n      Condition == 1 ~ \"no_source\",\n      Condition == 2 ~ \"facebook\",\n      TRUE ~ NA_character_\n    ),\n    condition = if_else(Condition == 3, \"treatment\", \"control\")\n  )\n\n\n\n\nnews_id\nWe have previously coded item, but this is not yet a unique item identifier–these numbers only identify items within each veracity category.\n\n\nCode\nd_long |&gt; \n  group_by(veracity) |&gt; \n  summarise(n_distinct(item))\n\n\n# A tibble: 2 × 2\n  veracity `n_distinct(item)`\n  &lt;chr&gt;                 &lt;int&gt;\n1 false                    12\n2 true                     12\n\n\nFor our news identifier, we therefore combine the veracity variable with these identifiers.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(news_id = paste0(veracity, \"_\", item))\n\n\n\n\nage\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(age = Age\n         )\n\n\n\n\nyear\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = year(mdy(StartDate))\n         )\n\n\n\n\nIdentifiers (subject_id, experiment_id) & removing respondents with mutliple surveys\nThe original wide format data had 563 lines, i.e. completed studies. There is one worker with two surveys, which we will exclude.\n\n\nCode\nd |&gt; \n  group_by(WorkerID) |&gt; \n  summarise(n_surveys_taken = n(), \n            n_different_start_dates = n_distinct(StartDate)) |&gt; \n  filter(n_surveys_taken &gt; 1)\n\n\n# A tibble: 1 × 3\n  WorkerID       n_surveys_taken n_different_start_dates\n  &lt;chr&gt;                    &lt;int&gt;                   &lt;int&gt;\n1 A15LHHN76OW2UM               2                       1\n\n\n\n\nCode\nd_long_remove_doubles &lt;- d_long |&gt;\n  filter(WorkerID != \"A15LHHN76OW2UM\")\n\n# check \nn_distinct(d_long_remove_doubles$WorkerID)\n\n\n[1] 561\n\n\nCode\n# check \nn_distinct(d_long$WorkerID)\n\n\n[1] 562\n\n\n\n\nCode\nd1 &lt;- d_long_remove_doubles |&gt; \n  mutate(subject_id = WorkerID, \n         experiment_id = 1) \n\n\n\n\n\nStudy 2\n\n\nCode\nd &lt;- read_csv(\"dias_2020-study_2.csv\") \n\n\nRows: 1890 Columns: 445\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): ResponseID, ResponseSet, Name, IPAddress, StartDate, EndDate, Wor...\ndbl (427): Condition, Status, StartDateNum, Finished, confirmCode, IDInst, I...\nlgl   (2): ExternalDataReference, Email\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(d)\n\n\n# A tibble: 6 × 445\n  Condition ResponseID   ResponseSet Name  ExternalDataReference Email IPAddress\n      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt; &lt;lgl&gt;                 &lt;lgl&gt; &lt;chr&gt;    \n1         2 R_1NfzxXhNc… Default Re… Anon… NA                    NA    75.91.73…\n2         2 R_ektOPVZf7… Default Re… Anon… NA                    NA    64.22.25…\n3         2 R_uyrfrfz7c… Default Re… Anon… NA                    NA    76.5.191…\n4         2 R_cBcEMRtkk… Default Re… Anon… NA                    NA    172.76.4…\n5         1 R_2Vs7jWOlP… Default Re… Anon… NA                    NA    67.251.1…\n6         2 R_2coorehCL… Default Re… Anon… NA                    NA    73.255.2…\n# ℹ 438 more variables: Status &lt;dbl&gt;, StartDate &lt;chr&gt;, StartDateNum &lt;dbl&gt;,\n#   EndDate &lt;chr&gt;, Finished &lt;dbl&gt;, confirmCode &lt;dbl&gt;, WorkerID &lt;chr&gt;,\n#   IDInst &lt;dbl&gt;, Inst &lt;dbl&gt;, Fake1_S &lt;dbl&gt;, Fake1_2 &lt;dbl&gt;, Fake1_3 &lt;dbl&gt;,\n#   Fake1_RT_1 &lt;dbl&gt;, Fake1_RT_2 &lt;dbl&gt;, Fake1_RT_3 &lt;dbl&gt;, Fake1_RT_4 &lt;dbl&gt;,\n#   Fake2_S &lt;dbl&gt;, Fake2_2 &lt;dbl&gt;, Fake2_3 &lt;dbl&gt;, Fake2_RT_1 &lt;dbl&gt;,\n#   Fake2_RT_2 &lt;dbl&gt;, Fake2_RT_3 &lt;dbl&gt;, Fake2_RT_4 &lt;dbl&gt;, Fake3_S &lt;dbl&gt;,\n#   Fake3_2 &lt;dbl&gt;, Fake3_3 &lt;dbl&gt;, Fake3_RT_1 &lt;dbl&gt;, Fake3_RT_2 &lt;dbl&gt;, …\n\n\n\naccuracy_raw, veracity\n\n\nCode\nd_long &lt;- d |&gt;\n  pivot_longer(\n    cols = matches(\"^(Real|Fake)\\\\d+_(2(\\\\.\\\\d)?|3(\\\\.\\\\d)?)$\"),  # match _2, _2.0, _2.1, _3, _3.0, _3.1\n    names_to = c(\"veracity\", \"item\", \"measure\"),\n    names_pattern = \"^(Real|Fake)(\\\\d+)_([23](?:\\\\.\\\\d)?)$\", \n    values_to = \"value\"\n  ) |&gt;\n  mutate(\n    measure = case_when(\n      str_starts(measure, \"2\") ~ \"accuracy\",\n      str_starts(measure, \"3\") ~ \"sharing\"\n    )\n  ) |&gt;\n  # remove all NAs, which inevitably are existant, since each condition\n  drop_na(value) |&gt; \n  pivot_wider(names_from = measure, values_from = value) |&gt; \n  rename(accuracy_raw = accuracy)\n\n# check\nd_long |&gt;\n  group_by(Condition) |&gt;\n  summarise(n_participants = n_distinct(WorkerID),\n            n_valid_response = sum(!is.na(accuracy_raw)))\n\n\n# A tibble: 2 × 3\n  Condition n_participants n_valid_response\n      &lt;dbl&gt;          &lt;int&gt;            &lt;int&gt;\n1         1            918            22325\n2         2            933            22551\n\n\nWe code the veracity variable.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(\n    veracity = if_else(veracity == \"Fake\", \"false\", \"true\")\n    ) \n\n\n\n\nscale\n\n\nCode\ntable(d_long$accuracy_raw, useNA = \"always\")\n\n\n\n    1     2     3     4  &lt;NA&gt; \n16091 11551 12986  4248    55 \n\n\n\n\nCode\nd_long &lt;- d_long |&gt;\n  mutate(scale = 4)\n\n\n\n\nConditions (intervention_label, control_label, condition)\nIn study 2, the conditions are coded as facebook (coded 1), highlight_banner (coded as 2).\n\n\nCode\n# check\nd_long |&gt; \n  group_by(Condition) |&gt; \n  summarise(mean(accuracy_raw, na.rm=TRUE))\n\n\n# A tibble: 2 × 2\n  Condition `mean(accuracy_raw, na.rm = TRUE)`\n      &lt;dbl&gt;                              &lt;dbl&gt;\n1         1                               2.12\n2         2                               2.12\n\n\nWe code the condition variable.\n\n\nCode\nd_long &lt;- d_long |&gt;\n  mutate(\n    intervention_label = case_when(\n      Condition == 2 ~ \"highlight_banner\",\n      TRUE ~ NA_character_\n    ),\n    control_label = case_when(\n      Condition == 1 ~ \"facebook\",\n      TRUE ~ NA_character_\n    ),\n    condition = if_else(Condition == 2, \"treatment\", \"control\")\n  )\n\n\n\n\nnews_id\nWe have previously coded item, but this is not yet a unique item identifier–these numbers only identify items within each veracity category.\n\n\nCode\nd_long |&gt; \n  group_by(veracity) |&gt; \n  summarise(n_distinct(item))\n\n\n# A tibble: 2 × 2\n  veracity `n_distinct(item)`\n  &lt;chr&gt;                 &lt;int&gt;\n1 false                    12\n2 true                     12\n\n\nFor our news identifier, we therefore combine the veracity variable with these identifiers.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(news_id = paste0(veracity, \"_\", item))\n\n\n\n\nage\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(age = Age\n         )\n\n\n\n\nyear\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = year(mdy(StartDate))\n         )\n\n\n\n\nIdentifiers (subject_id, experiment_id) & removing respondents with mutliple surveys\nThe original wide format data had 1890 lines, i.e. completed studies. First, we get an overview of candidate variables for participant identifiers.\n\n\nCode\nd_long |&gt; \n  summarize(n_distinct(ResponseID),\n            n_distinct(WorkerID))\n\n\n# A tibble: 1 × 2\n  `n_distinct(ResponseID)` `n_distinct(WorkerID)`\n                     &lt;int&gt;                  &lt;int&gt;\n1                     1890                   1845\n\n\nIt seems like the same workers have in some cases done the survey mulitple times.\n\n\nCode\nd_long |&gt; \n  group_by(WorkerID) |&gt; \n  summarise(n_surveys_taken = n_distinct(ResponseID), \n            n_different_start_dates = n_distinct(StartDate)) |&gt; \n  filter(n_surveys_taken &gt; 1)\n\n\n# A tibble: 12 × 3\n   WorkerID       n_surveys_taken n_different_start_dates\n   &lt;chr&gt;                    &lt;int&gt;                   &lt;int&gt;\n 1 A1B2GXPMA7YONT              35                       1\n 2 A1BL5TRC3DHOHD               2                       1\n 3 A1BWO4ZG5OB68S               2                       1\n 4 A26DD205RQG4UA               2                       1\n 5 A2I88USJQLNT2K               2                       1\n 6 A2XVOYY8BDEZXF               2                       2\n 7 A3ESURUKHP67K6               2                       1\n 8 A3TRL4MZMGU22S               2                       1\n 9 A3ZRS4RUCH2OO                2                       2\n10 A9T9UBZBWFZTL                2                       1\n11 AC777GHVJ8U45                2                       1\n12 AUU9JZS6MIQ7C                2                       1\n\n\nIn particular, one worker took the survey 35 times. Unfortunately, we don’t know about the exact time, we only get the day of the survey taken (otherwise we could select the first survey occurence). Not knowing what has been going on there, we will exclude these participants.\n\n\nCode\nd_long_remove_doubles &lt;- d_long |&gt;\n  group_by(WorkerID) |&gt;\n  filter(n_distinct(ResponseID) == 1) |&gt;\n  ungroup()\n\n# check \nn_distinct(d_long_remove_doubles$WorkerID)\n\n\n[1] 1833\n\n\nCode\n# check \nn_distinct(d_long$WorkerID)\n\n\n[1] 1845\n\n\n\n\nCode\nd2 &lt;- d_long_remove_doubles |&gt; \n  mutate(subject_id = WorkerID, \n         experiment_id = 2) \n\n\n\n\n\nCombine Studies\n\nCombine and add identifiers (country, paper_id)\nWe combine both studies.\n\n\nCode\n## Combine + add remaining variables\ndias_2020 &lt;- bind_rows(d1, d2) |&gt; \n  mutate(country = \"United States\",\n         paper_id = \"dias_2020\") |&gt; \n  # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n\n\n\nAdditional news identifiers (recycled_news, recycled_news_reference)\nSince in both studies the same news headlines have been used (with the same labels), we can just keep the labels. We add where the headlines have been taken from.\n\n\nCode\n## Combine + add remaining variables\ndias_2020 &lt;- dias_2020 |&gt; \n  mutate(recycled_news = TRUE, \n         recycled_news_reference = \"Pennycook, G., Bear, A., Collins, E. T., & Rand, D. G. (2020). The Implied Truth Effect: Attaching Warnings to a Subset of Fake News Headlines Increases Perceived Accuracy of Headlines Without Warnings. Management Science, 66(11), 4944–4957. https://doi.org/10.1287/mnsc.2019.3478\") \n\n\n\n\nnews_selection\n\n\nCode\n## Combine + add remaining variables\ndias_2020 &lt;- dias_2020 |&gt; \n  mutate(news_selection = \"researchers\")"
  },
  {
    "objectID": "data/papers/dias_2020/dias_2020.html#write-out-data",
    "href": "data/papers/dias_2020/dias_2020.html#write-out-data",
    "title": "Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.",
    "section": "Write out data",
    "text": "Write out data\n\n\nCode\nsave_data(dias_2020)"
  },
  {
    "objectID": "data/codebook.html",
    "href": "data/codebook.html",
    "title": "Codebook",
    "section": "",
    "text": "Code\n# make codebook\ncodebook &lt;- data.frame(\n  Variable_Name = c(\n    \"paper_id\", \n    \"experiment_id\", \n    \"subject_id\", \n    \"news_id\", \n    \"country\", \n    \"year\", \n    \"veracity\", \n    \"condition\", \n    \"intervention_label\", \n    \"intervention_description\", \n    \"intervention_selection\",\n    \"intervention_selection_description\",\n    \"control_label\",\n    \"control_format\", \n    \"control_selection\",\n    \"control_selection_description\",\n    \"accuracy_raw\", \n    \"scale\", \n    \"originally_identified_treatment_effect\", \n    \"concordance\", \n    \"partisan_identity\", \n    \"news_slant\", \n    \"age\", \n    \"age_range\", \n    \"identified_via\", \n    \"id\", \n    \"unique_experiment_id\", \n    \"accuracy\", \n    \"recycled_news\", \n    \"recycled_news_reference\", \n    \"news_selection\", \n    \"long_term\", \n    \"time_elapsed\"\n  ),\n  Values = c(\n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;\", \n    \"&lt;code&gt;treatment&lt;/code&gt;, &lt;code&gt;control&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;picture&lt;/code&gt;, &lt;code&gt;lede&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt; (multiple possible), &lt;code&gt;facebook&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;\", \n    \"&lt;code&gt;concordant&lt;/code&gt;, &lt;code&gt;discordant&lt;/code&gt;\", \n    \"&lt;code&gt;democrat&lt;/code&gt;, &lt;code&gt;republican&lt;/code&gt;\", \n    \"&lt;code&gt;democrat&lt;/code&gt;, &lt;code&gt;republican&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\",\n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;\", \n    \"&lt;code&gt;FALSE&lt;/code&gt;, &lt;code&gt;TRUE&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\", \n    \"&lt;code&gt;character&lt;/code&gt;\",\n    \"&lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;NA&lt;/code&gt;\", \n    \"&lt;code&gt;integer&lt;/code&gt;\"\n  ),\n  Description = c(\n    \"Identifier for each paper\",\n    \"Identifier for each experiment within a paper; start counting from 1; even if a paper has only one experiment, assign it an identifier\",\n    \"Identifier of individual participants within an experiment\",\n    \n    \"Identifier of news headlines within an experiment\",\n    \"The country of the sample\",\n    \"Ideally year of data collection, otherwise year of publication\",\n    \"Identifying false and true news items\",\n    \"Treatment vs. control\",\n    \"A label for what the intervention consisted of\",\n    \"A detailed description of the intervention\",\n    \"Only use if there is a choice to be made about how to treat different interventions: Pick the intervention_label that corresponds to the condition to keep, or a vector of intervention labels in cases there should be a merge\", \n    \"If multiple interventions tested within a single experiment (and related to a single control group), reasoning as to which intervention to select\",\n    \"A label for what the control group consisted of. Only use if there are multiple control conditions and a choice needs to be made about which to pick\", \n    \"Which format the news were in, in the control condition. This is typically also the format of the treatement condition, but since sometimes the format varies (.e.g. when presence of source is manipulated), we code it as 'control' format. We take for granted that all studies show news headlines. Can take multiple values, either acombination of `picture`, `lede`, and `source`, or some social media format, e.g. 'facebook'.\", \n    \"Only use if there is a choice to be made about how to treat different control conditions: Sometimes when there are multiple interventions, there are also multiple control groups; indicate the control group (or groups, in case of a merge) to keep\",\n    \"A detailed description of the chosen control group and why it has been chosen\", \n    \"Participants' accuracy ratings on the scale used in the original study (either a number, if Likert-type scale, or binary\",\n    \"The scale used in the original study\",\n    \"Whether the authors identified a significant treatment effect (`FALSE` if no, `TRUE` if yes)\",\n    \"Political concordance of news items (concordant or discordant)\",\n    \"Which party participants identify with (either Republican or Democrat). By contrast to concordance, we only code this variable for studies on US participants\",\n    \"The political slant--if any--the news headline had. Limited to either Republican or Democrat, and only coded for studies on US participants\",\n    \"Participant age. In some cases, participant age will not be exact, but within a binned category. In this case, we will take the mid-point of this category for the age variable\",\n    \"Binned age, if only this is provided by the study.\",\n    \"Indicates if a paper was identified by the systematic review or added after\",\n    \"Unique participant ID (merged `paper_id`, `experiment_id`, `subject_id`)\",\n    \"Unique experiment ID (merged `paper_id` and `experiment_id`)\",\n    \"Binary version of `accuracy_raw`; unchanged if originally binary\", \n    \"Whether the set of news items has been taken from another paper\", \n    \"If from another paper, the reference of the paper the news items have been taken from\",\n    \"Who selected the news items? (mostly 'researchers', but can also take other levels)\", \n        \"Some studies evaluate long-term effects of their intervention. We have a specific definition of what counts as a valid evaluation: The long-term effects need to be measured (i) on a new set of news headlines (ii) participants must not be exposed to the treatement again (we don't want to measure cumulative effects, but durability). Long-term evaluations are not included in our main analyses)\", \n        \"In case there are long-term effects, report the elapsed time (in days) between exposure to treatment and follow-up evaluation\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nwrite_csv(codebook, \"codebook.csv\")\n\n\nYou can download the combined individual-level data from all studies on the OSF project page soon. For a codebook, see Table 1, or download the codebook here  codebook.csv.\n\n\nCode\n# Generate the styled table with kableExtra\nkable(codebook, \n      caption = \"Codebook for variables to collect\",\n      col.names = c(\"Variable Name\", \"Values\", \"Description\"),\n      booktabs = TRUE,\n      longtable = TRUE, \n      escape = FALSE, \n      format = \"html\") %&gt;%\n  kable_styling(latex_options = \"repeat_header\",\n                font_size = 10) %&gt;% \n  column_spec(1, bold = TRUE) %&gt;%  # Bold the first column\n  column_spec(2, width = \"25em\") %&gt;%  # Set width for the description column\n  row_spec(0, bold = TRUE)  # Bold the header row\n\n\n\n\nTable 1: Codebook for variables to collect\n\n\n\n\n\n\nVariable Name\nValues\nDescription\n\n\n\n\npaper_id\ncharacter\nIdentifier for each paper\n\n\nexperiment_id\ninteger\nIdentifier for each experiment within a paper; start counting from 1; even if a paper has only one experiment, assign it an identifier\n\n\nsubject_id\ninteger\nIdentifier of individual participants within an experiment\n\n\nnews_id\ninteger\nIdentifier of news headlines within an experiment\n\n\ncountry\ncharacter\nThe country of the sample\n\n\nyear\ninteger\nIdeally year of data collection, otherwise year of publication\n\n\nveracity\ntrue, false\nIdentifying false and true news items\n\n\ncondition\ntreatment, control\nTreatment vs. control\n\n\nintervention_label\ncharacter\nA label for what the intervention consisted of\n\n\nintervention_description\ncharacter\nA detailed description of the intervention\n\n\nintervention_selection\ncharacter\nOnly use if there is a choice to be made about how to treat different interventions: Pick the intervention_label that corresponds to the condition to keep, or a vector of intervention labels in cases there should be a merge\n\n\nintervention_selection_description\ncharacter\nIf multiple interventions tested within a single experiment (and related to a single control group), reasoning as to which intervention to select\n\n\ncontrol_label\ncharacter\nA label for what the control group consisted of. Only use if there are multiple control conditions and a choice needs to be made about which to pick\n\n\ncontrol_format\npicture, lede, source (multiple possible), facebook\nWhich format the news were in, in the control condition. This is typically also the format of the treatement condition, but since sometimes the format varies (.e.g. when presence of source is manipulated), we code it as 'control' format. We take for granted that all studies show news headlines. Can take multiple values, either acombination of `picture`, `lede`, and `source`, or some social media format, e.g. 'facebook'.\n\n\ncontrol_selection\ncharacter\nOnly use if there is a choice to be made about how to treat different control conditions: Sometimes when there are multiple interventions, there are also multiple control groups; indicate the control group (or groups, in case of a merge) to keep\n\n\ncontrol_selection_description\ncharacter\nA detailed description of the chosen control group and why it has been chosen\n\n\naccuracy_raw\ninteger\nParticipants' accuracy ratings on the scale used in the original study (either a number, if Likert-type scale, or binary\n\n\nscale\ncharacter\nThe scale used in the original study\n\n\noriginally_identified_treatment_effect\nTRUE, FALSE\nWhether the authors identified a significant treatment effect (`FALSE` if no, `TRUE` if yes)\n\n\nconcordance\nconcordant, discordant\nPolitical concordance of news items (concordant or discordant)\n\n\npartisan_identity\ndemocrat, republican\nWhich party participants identify with (either Republican or Democrat). By contrast to concordance, we only code this variable for studies on US participants\n\n\nnews_slant\ndemocrat, republican\nThe political slant--if any--the news headline had. Limited to either Republican or Democrat, and only coded for studies on US participants\n\n\nage\ninteger\nParticipant age. In some cases, participant age will not be exact, but within a binned category. In this case, we will take the mid-point of this category for the age variable\n\n\nage_range\ncharacter\nBinned age, if only this is provided by the study.\n\n\nidentified_via\ncharacter\nIndicates if a paper was identified by the systematic review or added after\n\n\nid\ncharacter\nUnique participant ID (merged `paper_id`, `experiment_id`, `subject_id`)\n\n\nunique_experiment_id\ncharacter\nUnique experiment ID (merged `paper_id` and `experiment_id`)\n\n\naccuracy\n0, 1\nBinary version of `accuracy_raw`; unchanged if originally binary\n\n\nrecycled_news\nFALSE, TRUE\nWhether the set of news items has been taken from another paper\n\n\nrecycled_news_reference\ncharacter\nIf from another paper, the reference of the paper the news items have been taken from\n\n\nnews_selection\ncharacter\nWho selected the news items? (mostly 'researchers', but can also take other levels)\n\n\nlong_term\nTRUE, NA\nSome studies evaluate long-term effects of their intervention. We have a specific definition of what counts as a valid evaluation: The long-term effects need to be measured (i) on a new set of news headlines (ii) participants must not be exposed to the treatement again (we don't want to measure cumulative effects, but durability). Long-term evaluations are not included in our main analyses)\n\n\ntime_elapsed\ninteger\nIn case there are long-term effects, report the elapsed time (in days) between exposure to treatment and follow-up evaluation",
    "crumbs": [
      "Data",
      "Codebook"
    ]
  },
  {
    "objectID": "presentations/madrid.html#many-individual-level-interventions-have-been-proposed-to-help-people-spot-false-news",
    "href": "presentations/madrid.html#many-individual-level-interventions-have-been-proposed-to-help-people-spot-false-news",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Many individual-level interventions have been proposed to help people spot false news",
    "text": "Many individual-level interventions have been proposed to help people spot false news\n\nBad news game, literacy tips, labels"
  },
  {
    "objectID": "presentations/madrid.html#some-misinformation-interventions-have-been-shown-to-foster-general-skepticism.",
    "href": "presentations/madrid.html#some-misinformation-interventions-have-been-shown-to-foster-general-skepticism.",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Some misinformation interventions have been shown to foster general skepticism.",
    "text": "Some misinformation interventions have been shown to foster general skepticism.\n\n\n\n\nModirrousta-Galian, A., & Higham, P. A. (2023). Gamified inoculation interventions do not improve discrimination between true and fake news: Reanalyzing existing research with receiver operating characteristic analysis. Journal of Experimental Psychology: General."
  },
  {
    "objectID": "presentations/madrid.html#how-effective-are-misinformation-interventions-another-meta-analysis",
    "href": "presentations/madrid.html#how-effective-are-misinformation-interventions-another-meta-analysis",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "How effective are misinformation interventions ? A(nother) meta-analysis",
    "text": "How effective are misinformation interventions ? A(nother) meta-analysis\n\nIdentify relevant studies\nCollect raw data.\nClean and bring into common format.\nAnalyze using Signal Detection Theory framework"
  },
  {
    "objectID": "presentations/madrid.html#signal-detection-theory",
    "href": "presentations/madrid.html#signal-detection-theory",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Signal Detection Theory",
    "text": "Signal Detection Theory\nMeasures:\n\ndiscriminability (d’) (~discernment)\nresponse criterion (c) (~skepticism bias)\n\nOutcomes:\n\n\\(\\Delta d'\\) = d’(treamtment) - d’(control)\n\\(\\Delta c\\) = c (treatment) - c (control)"
  },
  {
    "objectID": "presentations/madrid.html#what-we-have-so-far",
    "href": "presentations/madrid.html#what-we-have-so-far",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "What we have so far…",
    "text": "What we have so far…"
  },
  {
    "objectID": "presentations/madrid.html#section",
    "href": "presentations/madrid.html#section",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "",
    "text": "paper_id\nexperiment_id\nParticipants\nNews headlines\nIntervention\n\n\n\n\n1\naltay_2023\n1\n198\n16\nFalseLabel\n\n\n2\nbadrinathan_2021\n1\n818\n14\nliteracy\n\n\n3\nbago_2022\n2\n331\n24\nsuppression\n\n\n4\nbago_2022\n3\n1502\n24\nsuppression\n\n\n5\nbago_2022\n4\n1520\n24\nsuppression\n\n\n6\nbrashier_2021\n1\n397\n36\nveracity_labels\n\n\n7\nbrashier_2021\n2\n422\n36\nveracity_labels\n\n\n8\nclayton_2020\n1\n429\n9\nfalse_no_warning\n\n\n9\ndias_2020\n1\n190\n24\nhighlight_banner\n\n\n10\ndias_2020\n2\n923\n24\nhighlight_banner\n\n\n11\nguess_2020\n1\n4907\n12\nliteracy\n\n\n12\nguess_2020\n2\n3744\n12\nliteracy\n\n\n13\nguess_2020\n3\n3273\n12\nliteracy"
  },
  {
    "objectID": "presentations/madrid.html#section-1",
    "href": "presentations/madrid.html#section-1",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "",
    "text": "Figure 1: Forest plot for delta d. The figure displays all effect sizes. Effects are weighed by their sample size. Effect sizes are calculated as z-scores. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the paper level."
  },
  {
    "objectID": "presentations/madrid.html#section-2",
    "href": "presentations/madrid.html#section-2",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "",
    "text": "Figure 2: Forest plots for delta c. The figure displays all effect sizes. Effects are weighed by their sample size. Effect sizes are calculated as z-scores. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the paper level."
  },
  {
    "objectID": "presentations/madrid.html#conclusion",
    "href": "presentations/madrid.html#conclusion",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nPeople discern rather well between true and false news\n\n\n\n\nIf they err, they tend to be more skeptical of true news than gullible towards false news\n\n\n\n\nIt is not (yet) clear how effective individual-level misinformation interventions are:\n\ndo they help people discriminate better?\ndo they just make people more skeptical?\nwhich are the most/least effective interventions?"
  },
  {
    "objectID": "presentations/madrid.html#thank-you",
    "href": "presentations/madrid.html#thank-you",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Thank you",
    "text": "Thank you\n\n\n Jan Pfänder\n Sacha Altay\n\nSlides are available here:"
  },
  {
    "objectID": "presentations/madrid.html#political-concordance",
    "href": "presentations/madrid.html#political-concordance",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Political concordance",
    "text": "Political concordance"
  },
  {
    "objectID": "presentations/madrid.html#signal-detection-theory-1",
    "href": "presentations/madrid.html#signal-detection-theory-1",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Signal Detection theory",
    "text": "Signal Detection theory\n\n\n\n\nTable 1: Accuracy ratings in Signal Detection Theory terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant response\n\n\n\n\nStimulus\nAccurate\nNot Accurate\nSDT Metric\n\n\n\n\nTrue news (target)\nHit\nMiss\nHit rate (HR) = Hits / (Hits + Misses)\n\n\nFalse news (distractor)\nFalse alarm\nCorrect rejection\nFalse alarm rate (FAR) = False Alarms / (False Alarms + Correct Rejections)\n\n\n\n\n\n\n\n\n\n\n\n\\[\nd' = z(HR) - z(FAR)\n\\]\n\n\\[\nc = -\\frac{1}{2}(\\text{zHR} + \\text{zFAR})\n\\]"
  },
  {
    "objectID": "presentations/madrid.html#comparison-sdt-and-discernment-skepticism-bias",
    "href": "presentations/madrid.html#comparison-sdt-and-discernment-skepticism-bias",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Comparison SDT and Discernment/ Skepticism bias",
    "text": "Comparison SDT and Discernment/ Skepticism bias"
  },
  {
    "objectID": "presentations/madrid.html#selection-bias",
    "href": "presentations/madrid.html#selection-bias",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Selection bias",
    "text": "Selection bias"
  },
  {
    "objectID": "preregistration/simulation.html",
    "href": "preregistration/simulation.html",
    "title": "Data simulation for preregistration",
    "section": "",
    "text": "Code\n# load plot theme\nsource(\"../R/plot_theme.R\") \n\n# load other functions\nsource(\"../R/custom_functions.R\")\nCode\n# Set a seed\nset.seed(1234)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#data-generating-process",
    "href": "preregistration/simulation.html#data-generating-process",
    "title": "Data simulation for preregistration",
    "section": "Data generating process",
    "text": "Data generating process\n\nVariables\n\npaper_id (identifier of the published paper)\nexperiment_id (identifier of experiments within a paper)\nsubject_id (identifier of subjects within an experiment)\nveracity (true vs. fake news)\naccuracy (numeric accuracy rating, ranging from 0 to 1)1\ncondition (control vs. intervention)\nn (number of participants per sample)\n\n\n\nDesign\nThe important parts of the design are:\n\nRandom factors :\n\nsubject_id\nexperiment_id\npaper_id\n\nFixed factor 1: veracity (levels = false, true)\n\nwithin subject_id\nwithin experiment_id\nwithin paper_id\n\nFixed factor 2: condition (levels: control, intervention)\n\nbetween subject_id\nwithin experiment_id\nwithin paper_id\n\nFixed factor 3: condition\\*veracity\n\nbetween subject_id\nwithin experiment_id\nwithin paper_id\n\n\nWe assume that,\n\nveracity varies within participants, i.e. all participants see both fake and true news\ncondition varies between participants, i.e. a participant is either in the control group or a treatment group\nan experiment involves only one control group, but a varying number of treatment groups (up to 3)\nwe assume random effects only for participants (subject_id), experiments (experiment_id) and papers (paper_id). We do not assume, random effects for different intervention types, nor for different news headlines\nthe sample size per experimental condition is 100\neach participant rates 12 news items in total, with equal numbers of fake and true items\n\n\n\nModel\nWe use a generalized linear mixed model (GLMM) to generate accuracy responses. In this model, our parameters are z-values. For the interpretation of our model parameters below in terms of Signal Detection Theory (SDT) outcomes (sensitivity “d prime” and the response bias “c”), it is crucial that we use deviation coding for our veracity variable (fake = -0.5, true = 0.5), and for condition (-0.5 = control, 0.5 = intervention).\n\\[\\begin{align*}\n\\eta_i &= (\\beta_0 + b_{0_\\text{Subject}} + b_{0_\\text{Experiment}} + b_{0_\\text{Paper}}) \\\\\n&\\quad+ (\\beta_v + b_{v_\\text{Subject}} + b_{v_\\text{Experiment}} + b_{v_\\text{Paper}}) \\text{Veracity} \\\\\n&\\quad+ (\\beta_c + b_{c_\\text{Experiment}} + b_{c_\\text{Paper}}) \\text{Condition} \\\\\n&\\quad+ (\\beta_{cv} + b_{cv_\\text{Experiment}} + b_{cv_\\text{Paper}}) \\text{Condition*Veracity} + \\epsilon\n\\end{align*}\\]\nwith2:\n\n(b_{0_{}} N(0, _{_0}))\n(b_{v_{}} N(0, _{_v}))\n(b_{0_{}} N(0, _{_0}))\n(b_{v_{}} N(0, _{_v}))\n(b_{c_{}} N(0, _{_c}))\n(b_{cv_{}} N(0, _{_cv}))\n(b_{0_{}} N(0, _{_0}))\n(b_{v_{}} N(0, _{_v}))\n(b_{c_{}} N(0, _{_c}))\n(b_{cv_{}} N(0, _{_cv}))\n(N(0, ))\n\nwhere:\n\n_i is a z-score that can be transformed to a probability for an accuracy answer, using the probit function\n(_0) represents - the average response bias (c), pooled across conditions\n(b_{0_{}}), (b_{0_{}}), and (b_{0_{}}) are the subject-, experiment-, and paper-specific deviations from the average intercept.\n(_v) represents the average sensitivity (d’) across the conditions\n(b_{v_{}}), (b_{v_{}}), and (b_{v_{}}) are the subject-, experiment-, and paper-specific deviations from the average sensitivity (d’)\n(_c) reflects -\\(\\Delta c\\), i.e. the change in -response bias between control and treatment\n(b_{c_{}}) and (b_{c_{}}) are the experiment-, and paper-specific deviations from -\\(\\Delta c\\)\n(_{cv}) reflects \\(\\Delta d'\\), i.e. the change in sensitivity between treatment and control\n(b_{cv_{}}) and (b_{cv_{}}) are the experiment-, and paper-specific deviations from the average \\(\\Delta d'\\)\n\\(\\epsilon\\) is the error term that represents noise not accounted for by the random effects\n\\(\\sigma\\) is the standard deviation of the normal distribution that of the error term.\n\\(\\tau\\)’s are the respective standard deviations of the normal distributions of the random effects. We assume that the deviations of subjects are normally distributed around the average effects (hence a mean of 0). Some subjects will have a positive deviation (i.e. larger values than the average); some will have a negative offset (i.e. smaller values than the average).\n\nThe modelling of random effects as described above was simplified. It suggested that all random effects are described by independent normal distributions. But in fact, should expect that all random effects, of for example a single subject, are correlated. For example, subjects with a relatively small intercept (i.e. assigning very low accuracy when to false news) might assign all the more accuracy to true news (i.e shows a larger effect for veracity). In this case, the random effect distributions of the intercept and the effect of veracity for subjects are negatively correlated.\nFor each random effect factor (subjects, experiments, papers) we therefore model multivariate normal distributions.\n[ b_{0_{}}, b_{v_{}} N(, 0 , ) ]\nwith variance-covariance matrix\n[ =\n\\[\\begin{bmatrix} \\tau_{0_{\\text{subj}}}^2 & \\rho_{0v_{\\text{subj}}} \\tau_{0_{\\text{subj}}} \\tau_{v_{\\text{subj}}} \\\\ \\rho_{0v_{\\text{subj}}} \\tau_{0_{\\text{subj}}} \\tau_{v_{\\text{subj}}} & \\tau_{v_{\\text{subj}}}^2 \\end{bmatrix}\\]\n]\n[ b_{0_{}}, b_{v_{}}, b_{c_{}}, b_{cv_{}} N(, 0, 0 , ) ]\nwith variance-covariance matrix\n[ =\n\\[\\begin{bmatrix}\n\\tau_{0_{\\text{exp}}}^2 & \\rho_{0v_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{v_{\\text{exp}}} & \\rho_{0c_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{c_{\\text{exp}}} & \\rho_{0cv_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} \\\\\n\\rho_{0v_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{v_{\\text{exp}}} & \\tau_{v_{\\text{exp}}}^2 & \\rho_{vc_{\\text{exp}}} \\tau_{v_{\\text{exp}}} \\tau_{c_{\\text{exp}}} & \\rho_{vcv_{\\text{exp}}} \\tau_{v_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} \\\\\n\\rho_{0c_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{c_{\\text{exp}}} & \\rho_{vc_{\\text{exp}}} \\tau_{v_{\\text{exp}}} \\tau_{c_{\\text{exp}}} & \\tau_{c_{\\text{exp}}}^2  & \\rho_{ccv_{\\text{exp}}} \\tau_{c_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} \\\\\n\\rho_{0cv_{\\text{exp}}} \\tau_{0_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} & \\rho_{vcv_{\\text{exp}}} \\tau_{v_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} & \\rho_{ccv_{\\text{exp}}} \\tau_{c_{\\text{exp}}} \\tau_{cv_{\\text{exp}}} & \\tau_{cv_{\\text{exp}}}^2\n\\end{bmatrix}\\]\n]\n[ b_{0_{}}, b_{v_{}}, b_{c_{}}, b_{cv_{}} N(, 0, 0 , ) ]\nwith variance-covariance matrix\n[ =\n\\[\\begin{bmatrix}\n\\tau_{0_{\\text{paper}}}^2 & \\rho_{0v_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{v_{\\text{paper}}} & \\rho_{0c_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{c_{\\text{paper}}} & \\rho_{0cv_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} \\\\\n\\rho_{0v_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{v_{\\text{paper}}} & \\tau_{v_{\\text{paper}}}^2 & \\rho_{vc_{\\text{paper}}} \\tau_{v_{\\text{paper}}} \\tau_{c_{\\text{paper}}} & \\rho_{vcv_{\\text{paper}}} \\tau_{v_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} \\\\\n\\rho_{0c_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{c_{\\text{paper}}} & \\rho_{vc_{\\text{paper}}} \\tau_{v_{\\text{paper}}} \\tau_{c_{\\text{paper}}} & \\tau_{c_{\\text{paper}}}^2  & \\rho_{ccv_{\\text{paper}}} \\tau_{c_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} \\\\\n\\rho_{0cv_{\\text{paper}}} \\tau_{0_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} & \\rho_{vcv_{\\text{paper}}} \\tau_{v_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} & \\rho_{ccv_{\\text{paper}}} \\tau_{c_{\\text{paper}}} \\tau_{cv_{\\text{paper}}} & \\tau_{cv_{\\text{paper}}}^2\n\\end{bmatrix}\\]\n]\nwhere\n\n\\(\\tau\\)’s are the standard deviations of the random effects distributions\n\\(\\rho\\)’s are the correlation coefficients.\n\nWe then transform the z-values \\(\\eta_i\\) to probabilities using the probit function.\n\\[p_i = \\Phi(\\eta_i)\\] where \\(\\Phi\\) is the cumulative distribution function (CDF) of the standard normal distribution (or probit function). It maps the z-scores (\\(\\eta_i\\)) to probabilities.\n\\[\n\\Phi(\\eta_i) = \\int_{-\\infty}^{\\eta_i} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}} \\, dz\n\\]\nThe integral calculates the area under the standard normal curve from negative infinity up to the value \\(\\eta_i\\) and thus translates the z-value into a cumulative probability, which represents the likelihood of observing a value less than or equal to \\(\\eta_i\\).\nBased on these probabilities, we simulate binary response (accurate = 1 or not accurate = 0), by using a Bernoulli (i.e. a binomial with one trial) function.\n\\[y_i \\sim Bernoulli(p_i)\\]\n\n\nParameters\nWe use the following prefixes to designate model parameters and sampled values:\n\nbeta_*: fixed effect parameters\nsubj_*: random effect parameters associated with subjects\nexp_*: random effect parameters associated with experiment\npaper_*: random effect parameters associated with paper\nSU_*: sampled values for subject random effects\nEXP_*: sampled values for experiment random effects\nPAP_*: sampled values for paper random effects\nB_*: sums of added beta’s\ne_*: residual sd\n\nWe use the following suffices to refer to certain variables:\n\n*_0: intercept\n*_v: veracity\n*_c: condition\n*_cv: condition*veracity (interaction)\n\nOther terms:\n\n*_rho: correlations for that group’s random effects\nn_*: sample size\nsigma: residual (error) sd\n\nBelow, we set our parameters. The values are probably off, but the main point is to see how well our analysis does in uncovering them.\n\n\nCode\n# parameters\n\n# fixed effects\nbeta_0   = -0 # intercept (- average response bias )\nbeta_v   = 1 # average sensitivity (d prime) \nbeta_c   = -0.5 # - delta response bias (i.e. - delta c)\nbeta_cv   = 0.5 # delta sensitivity (i.e. delta d prime)\n# random effects\n# subjects\nsubj_0   = 0.1 # by-subject intercept sd\nsubj_v   = 0.1 # by-subject sensitivity (d prime) sd\nsubj_rho_0v = 0 # correlation between intercept (- average response bias) and sensitivity (d prime)\n# experiments\nexp_0   = 0.1 # by-experiment intercept sd\nexp_v   = 0.1 # by-experiment sensitivity (d prime) sd\nexp_c   = 0.1 # by-experiment - delta response bias (i.e. - delta c) sd\nexp_cv   = 0.1 # by-experiment delta sensitivity (i.e. delta d prime) sd\nexp_rho_0v = 0 # correlation between intercept (- average response bias) and sensitivity (d prime)\nexp_rho_0c = 0 # correlation between intercept and - delta response bias (i.e. - delta c)\nexp_rho_0cv = 0 # correlation between intercept (- average response bias) and delta sensitivity (i.e. delta d prime)\nexp_rho_vc = 0 # correlation between sensitivity (d prime) and - delta response bias (i.e. - delta c)\nexp_rho_vcv = 0 # correlation between sensitivity (d prime) and delta sensitivity (i.e. delta d prime)\nexp_rho_ccv = 0 # correlation between - delta response bias (i.e. - delta c) and delta sensitivity (i.e. delta d prime)\n# papers\npaper_0   = 0.1 # analogous to experiment random effects\npaper_v   = 0.1 \npaper_c   = 0.1 \npaper_cv   = 0.1 \npaper_rho_0v = 0 \npaper_rho_0c = 0 \npaper_rho_0cv = 0 \npaper_rho_vc = 0 \npaper_rho_vcv = 0 \npaper_rho_ccv = 0 \nsigma    = 0.5 # residual (error) sd (i.e. all variation that the model cannot account for)\n# simulation related\nn_subj  = 100 # number of subjects for each experimental arm (i.e. an experiment with a control and one intervention group has n = 400)\nn_fake  =  5 # number of fake news items \nn_true  =  5 # number of true news items\nn_max_conditions = 4 # maximum number of possible conditions for a single experiment\nn_max_experiments = 4 # maximum number of possible experiments within a single paper\nn_papers = 10 # number of papers in the meta analysis\n\n\nWe store a complete list of parameters in a list so that we can call them for functions later.\n\n\nCode\n# complete list of parameters (including some that will be introduced later)\nparameters &lt;- list(\n  # fixed effects\n  beta_0   = -0.5, # intercept (- average response bias )\n  beta_v   = 1, # average sensitivity (d prime) \n  beta_c   = - 0.1, # - delta response bias (i.e. - delta c)\n  beta_cv   = 0.1, # delta sensitivity (i.e. delta d prime)\n  # random effects\n  # subjects\n  subj_0   = 0.1, # by-subject intercept sd\n  subj_v   = 0.1, # by-subject sensitivity (d prime) sd\n  subj_rho_0v = 0, # correlation between intercept (- average response bias) and sensitivity (d prime)\n  # experiments\n  exp_0   = 0.1, # by-experiment intercept sd\n  exp_v   = 0.1, # by-experiment sensitivity (d prime) sd\n  exp_c   = 0.1, # by-experiment - delta response bias (i.e. - delta c) sd\n  exp_cv   = 0.1, # by-experiment delta sensitivity (i.e. delta d prime) sd\n  exp_rho_0v = 0, # correlation between intercept (- average response bias) and sensitivity (d prime)\n  exp_rho_0c = 0, # correlation between intercept and - delta response bias (i.e. - delta c)\n  exp_rho_0cv = 0, # correlation between intercept (- average response bias) and delta sensitivity (i.e. delta d prime)\n  exp_rho_vc = 0, # correlation between sensitivity (d prime) and - delta response bias (i.e. - delta c)\n  exp_rho_vcv = 0, # correlation between sensitivity (d prime) and delta sensitivity (i.e. delta d prime)\n  exp_rho_ccv = 0, # correlation between - delta response bias (i.e. - delta c) and delta sensitivity (i.e. delta d prime)\n  # papers\n  paper_0   = 0.1, # analogous to experiment random effects\n  paper_v   = 0.1, \n  paper_c   = 0.1, \n  paper_cv   = 0.1, \n  paper_rho_0v = 0, \n  paper_rho_0c = 0, \n  paper_rho_0cv = 0, \n  paper_rho_vc = 0, \n  paper_rho_vcv = 0, \n  paper_rho_ccv = 0, \n  sigma    = 0.5, # residual (error) sd (i.e. all variation that the model cannot account for)\n  # simulation related\n  n_subj  = 100, # number of subjects for each experimental arm (i.e. an experiment with a control and one intervention group has n = 200)\n  n_fake  =  5, # number of fake news items \n  n_true  =  5, # number of true news items\n  n_max_conditions = 4, # maximum number of possible conditions for a single experiment\n  n_max_experiments = 4, # maximum number of possible experiments within a single paper\n  n_papers = 10 # number of papers in the meta analysis\n)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#simulate-a-single-sample",
    "href": "preregistration/simulation.html#simulate-a-single-sample",
    "title": "Data simulation for preregistration",
    "section": "Simulate a single sample",
    "text": "Simulate a single sample\n\nStimuli (news items)\n\n\nCode\n# simulate a sample of items\nn_items &lt;- n_fake + n_true\n\nitems &lt;- data.frame(\n  item_id = seq_len(n_items),\n  veracity = rep(c(\"fake\", \"true\"), c(n_fake, n_true)),\n  # get a numeric version of veracity that is effect-coded (i.e. not 0 vs. 1, \n  # but -0.5 and 0.5)\n  veracity_effect_code = rep(c(-0.5, 0.5), c(n_fake, n_true))\n)\n\n\n\n\nSujects\nWe use the function MASS::mvrnorm to calculate the variance-covariance matrix between the two by-subject random effects.\n\n\nCode\n# simulate a sample of subjects\n\n# calculate random intercept / random slope covariance\ncovar &lt;- subj_rho_0v * subj_0 * subj_v\n\n# put values into variance-covariance matrix\ncov_mx  &lt;- matrix(\n  c(subj_0^2, covar,\n    covar,   subj_v^2),\n  nrow = 2, byrow = TRUE)\n\n# generate the by-subject random effects\nsubject_rfx &lt;- MASS::mvrnorm(n = n_subj,\n                             mu = c(SU_0 = 0, SU_v = 0),\n                             Sigma = cov_mx)\n\n# combine with subject IDs\nsubjects &lt;- data.frame(subject_id = seq_len(n_subj),\n                       subject_rfx)\n\n\nCheck values.\n\n\nCode\ndata.frame(\n  parameter = c(\"subj_0\", \"subj_v\", \"subj_rho_0v\"),\n  value = c(subj_0, subj_v, subj_rho_0v),\n  simulated = c(\n    sd(subjects$SU_0),\n    sd(subjects$SU_v), \n    cor(subjects$SU_0, subjects$SU_v)\n  )\n)\n\n\n    parameter value  simulated\n1      subj_0   0.1 0.10321873\n2      subj_v   0.1 0.10044053\n3 subj_rho_0v   0.0 0.02538285\n\n\n\n\nTrials (Subjects x Stimuli)\nWe combine the two data frames subjects and items we generated. We also draw a residual error for each trial/observation (e_s for error term for each combination of subject and stimulus).\n\n\nCode\n# cross subject and item IDs; add an error term\n# nrow(.) is the number of rows in the table\ntrials &lt;- crossing(subjects, items)  %&gt;%\n  mutate(e_s = rnorm(nrow(.), mean = 0, sd = sigma))\n\n\n\n\nFunction\nWe put all the previous steps in a function.\n\n\nCode\n# set up the custom data simulation function\ndraw_sample &lt;- function(\n# fixed effects\n  beta_0,\n  beta_v,\n  beta_c,\n  beta_cv,\n  # random effects\n  subj_0,\n  subj_v,\n  subj_rho_0v,\n  # simulation related\n  n_subj,\n  n_fake,\n  n_true,\n  sigma,\n  ...  # This allows additional parameters to be ignored\n) { \n  \n  # simulate a sample of items\n  n_items &lt;- n_fake + n_true\n  \n  items &lt;- data.frame(\n    item_id = seq_len(n_items),\n    veracity = rep(c(\"fake\", \"true\"), c(n_fake, n_true)),\n    # get a numeric version of veracity that is effect-coded (i.e. not 0 vs. 1, \n    # but -0.5 and 0.5)\n    veracity_effect_code = rep(c(-0.5, 0.5), c(n_fake, n_true))\n  )\n  \n  # simulate a sample of subjects\n  \n  # calculate random intercept / random slope covariance\n  covar &lt;- subj_rho_0v * subj_0 * subj_v\n  \n  # put values into variance-covariance matrix\n  cov_mx  &lt;- matrix(\n    c(subj_0^2, covar,\n      covar,   subj_v^2),\n    nrow = 2, byrow = TRUE)\n  \n  # generate the by-subject random effects\n  subject_rfx &lt;- MASS::mvrnorm(n = n_subj,\n                               mu = c(SU_0 = 0, SU_v = 0),\n                               Sigma = cov_mx)\n  \n  # combine with subject IDs\n  subjects &lt;- data.frame(subject_id = seq_len(n_subj),\n                         subject_rfx)\n  \n  # cross subject and item IDs and calculate accuracy\n  crossing(subjects, items)  %&gt;%\n    mutate(e_s = rnorm(nrow(.), mean = 0, sd = sigma))\n}\n\n# test function\n# test &lt;- do.call(draw_sample, parameters)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#simulate-an-experiment",
    "href": "preregistration/simulation.html#simulate-an-experiment",
    "title": "Data simulation for preregistration",
    "section": "Simulate an experiment",
    "text": "Simulate an experiment\nWe know how to generate a sample, which corresponds to all participants assigned to the same experimental condition. However, an experiment consists of several conditions, and thus several samples.\nFor example, we might have three conditions, one control group, and two intervention groups.\n\n\nCode\nn_conditions &lt;- 3\n\nn_interventions &lt;- n_conditions - 1 # we assume always one control group\n\n# draw control condition\ncontrol &lt;- do.call(draw_sample, parameters) %&gt;% \n  mutate(condition = \"control\")\n\n# draw interventions\ninterventions &lt;-  1:n_interventions %&gt;%\n    map_dfr(function(x) {\n\n      # To keep track of progress\n      print(paste(\"drawing intervention number \", x))\n      \n      single_intervention &lt;- do.call(draw_sample, parameters) %&gt;% \n        mutate(condition = \"intervention\", \n               intervention_id = x)\n      \n      return(single_intervention)\n    })\n\n\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n\n\nCode\n# combine control and interventions data\nexperiment &lt;- bind_rows(control, interventions)\n\n\n\nFunction\nIn our function, we allow the number of conditions to vary.\n\n\nCode\n# simulate an experiment\ndraw_experiment &lt;- function(n_max_conditions = NULL, ...) {\n  \n  # generate random number of conditions\n  possible_n_conditions &lt;- seq(from = 2, to = n_max_conditions, by = 1)\n  n_conditions &lt;- sample(possible_n_conditions, size = 1)\n  \n  n_interventions &lt;- n_conditions - 1 # we assume always one control group\n  \n  # draw control condition\n  control &lt;- draw_sample(...) %&gt;% \n    mutate(condition = \"control\")\n  \n# draw interventions\ninterventions &lt;-  1:n_interventions %&gt;%\n    map_dfr(function(x) {\n\n      # To keep track of progress\n      print(paste(\"drawing intervention number \", x))\n      \n      single_intervention &lt;- draw_sample(...) %&gt;% \n        mutate(condition = \"intervention\", \n               intervention_id = x)\n      \n      return(single_intervention)\n    })\n\n# combine control and interventions data\nexperiment &lt;- bind_rows(control, interventions)\n\n} \n# test  \n# test &lt;- do.call(draw_experiment, parameters)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#simulate-various-experiments-within-a-single-paper",
    "href": "preregistration/simulation.html#simulate-various-experiments-within-a-single-paper",
    "title": "Data simulation for preregistration",
    "section": "Simulate various experiments (within a single paper)",
    "text": "Simulate various experiments (within a single paper)\nA single paper can contain multiple experiments. For example, we might have three experiments.\n\n\nCode\nn_experiments &lt;- 3\n\n# draw experiments\nexperiments &lt;-  1:n_experiments %&gt;%\n    map_dfr(function(x) {\n\n      # To keep track of progress\n      print(paste(\"drawing experiment number \", x))\n      \n      single_experiment &lt;- do.call(draw_experiment, parameters) %&gt;% \n        mutate(experiment_id = x)\n      \n      return(single_experiment)\n    })\n\n\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n\n\nCode\nexperiments %&gt;% \n  group_by(experiment_id, intervention_id) %&gt;% \n  count()\n\n\n# A tibble: 9 × 3\n# Groups:   experiment_id, intervention_id [9]\n  experiment_id intervention_id     n\n          &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n1             1               1  1000\n2             1               2  1000\n3             1               3  1000\n4             1              NA  1000\n5             2               1  1000\n6             2               2  1000\n7             2              NA  1000\n8             3               1  1000\n9             3              NA  1000\n\n\nWe assume experiments to have random effects.\n\n\nCode\n# Calculate covariances\ncovar_0v_exp &lt;- exp_rho_0v * exp_0 * exp_v\ncovar_0c_exp &lt;- exp_rho_0c * exp_0 * exp_c\ncovar_0cv_exp &lt;- exp_rho_0cv * exp_0 * exp_cv\ncovar_vc_exp &lt;- exp_rho_vc * exp_v * exp_c\ncovar_vcv_exp &lt;- exp_rho_vcv * exp_v * exp_cv\ncovar_cc_exp &lt;- exp_rho_ccv * exp_c * exp_cv\n\n# Create the variance-covariance matrix\ncov_mx_exp &lt;- matrix(\n  c(exp_0^2, covar_0v_exp, covar_0c_exp, covar_0cv_exp,\n    covar_0v_exp, exp_v^2, covar_vc_exp, covar_vcv_exp,\n    covar_0c_exp, covar_vc_exp, exp_c^2, covar_cc_exp,\n    covar_0cv_exp, covar_vcv_exp, covar_cc_exp, exp_cv^2),\n  nrow = 4, byrow = TRUE\n)\n\n# Generate the by-experiment random effects\nexp_rfx &lt;- MASS::mvrnorm(n = n_experiments,\n                         mu = c(EXP_0 = 0, EXP_v = 0, EXP_c = 0, EXP_cv = 0),\n                         Sigma = cov_mx_exp)\n\n# Check the value of n_experiments\nif (n_experiments == 1) {\n  # Reshape to wide format\n  exp_rfx &lt;- data.frame(exp_rfx) %&gt;%\n    mutate(effect = rownames(.)) %&gt;% \n    pivot_wider(names_from = effect, values_from = \"exp_rfx\")\n}\n\n# Combine with experiment IDs\nexp_rfx &lt;- data.frame(experiment_id = seq_len(n_experiments), exp_rfx)\n\n# add random effects to experiment data\nexperiments &lt;- left_join(experiments, exp_rfx, by = \"experiment_id\")\n\n\n\nFunction\n\n\nCode\n# simulate multiple experiments\ndraw_multiple_experiments &lt;- function(\n    n_max_experiments, \n    exp_0, # by-experiment intercept sd\n    exp_v, # by-experiment sensitivity (d prime) sd\n    exp_c, # by-experiment - delta response bias (i.e. - delta c) sd\n    exp_cv, # by-experiment delta sensitivity (i.e. delta d prime) sd\n    exp_rho_0v, # correlation between intercept (- average response bias) and sensitivity (d prime)\n    exp_rho_0c, # correlation between intercept and - delta response bias (i.e. - delta c)\n    exp_rho_0cv, # correlation between intercept (- average response bias) and delta sensitivity (i.e. delta d prime)\n    exp_rho_vc, # correlation between sensitivity (d prime) and - delta response bias (i.e. - delta c)\n    exp_rho_vcv, # correlation between sensitivity (d prime) and delta sensitivity (i.e. delta d prime)\n    exp_rho_ccv, # correlation between - delta response bias (i.e. - delta c) and delta sensitivity (i.e. delta d prime)\n    ...) {\n  \n  # generate random number of experiments\n  possible_n_experiments &lt;- seq(from = 1, to = n_max_experiments, by = 1)\n  n_experiments &lt;- sample(possible_n_experiments, size = 1)\n  \n  # draw experiments\n  experiments &lt;-  1:n_experiments %&gt;%\n    map_dfr(function(x) {\n      \n      # To keep track of progress\n      print(paste(\"drawing experiment number \", x))\n      \n      single_experiment &lt;- draw_experiment(...) %&gt;% \n        mutate(experiment_id = x)\n      \n      return(single_experiment)\n    })\n  \n  # Calculate covariances\n  covar_0v_exp &lt;- exp_rho_0v * exp_0 * exp_v\n  covar_0c_exp &lt;- exp_rho_0c * exp_0 * exp_c\n  covar_0cv_exp &lt;- exp_rho_0cv * exp_0 * exp_cv\n  covar_vc_exp &lt;- exp_rho_vc * exp_v * exp_c\n  covar_vcv_exp &lt;- exp_rho_vcv * exp_v * exp_cv\n  covar_cc_exp &lt;- exp_rho_ccv * exp_c * exp_cv\n  \n  # Create the variance-covariance matrix\n  cov_mx_exp &lt;- matrix(\n    c(exp_0^2, covar_0v_exp, covar_0c_exp, covar_0cv_exp,\n      covar_0v_exp, exp_v^2, covar_vc_exp, covar_vcv_exp,\n      covar_0c_exp, covar_vc_exp, exp_c^2, covar_cc_exp,\n      covar_0cv_exp, covar_vcv_exp, covar_cc_exp, exp_cv^2),\n    nrow = 4, byrow = TRUE\n  )\n  \n  # Generate the by-experiment random effects\n  exp_rfx &lt;- MASS::mvrnorm(n = n_experiments,\n                           mu = c(EXP_0 = 0, EXP_v = 0, EXP_c = 0, EXP_cv = 0),\n                           Sigma = cov_mx_exp)\n  \n  # Check the value of n_experiments\n  if (n_experiments == 1) {\n    # if n == 1, mvnorm seems to return a vector, which data.frame then turns \n    # into long format data (instead of wide-format when n_experiments &gt; 1)\n    # Reshape to wide format\n    exp_rfx &lt;- data.frame(exp_rfx) %&gt;%\n      mutate(effect = rownames(.)) %&gt;% \n      pivot_wider(names_from = effect, values_from = \"exp_rfx\")\n  }\n  \n  # Combine with experiment IDs\n  exp_rfx &lt;- data.frame(experiment_id = seq_len(n_experiments), exp_rfx)\n  \n  # add random effects to experiment data\n  experiments &lt;- left_join(experiments, exp_rfx, by = \"experiment_id\")\n  \n  return(experiments)\n}\n\n# test \n# test &lt;- do.call(draw_multiple_experiments, parameters)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#simulate-various-papers",
    "href": "preregistration/simulation.html#simulate-various-papers",
    "title": "Data simulation for preregistration",
    "section": "Simulate various papers",
    "text": "Simulate various papers\nFinally, we have various papers. As for experiments, we expect random effects for papers (because of authors, country, time, etc.).\nFor example, we might have 20 papers.\n\n\nCode\nn_papers &lt;- 20\n\n# draw papers\npapers &lt;-  1:n_papers %&gt;%\n    map_dfr(function(x) {\n\n      # To keep track of progress\n      print(paste(\"drawing paper number \", x))\n      \n      single_paper &lt;- do.call(draw_multiple_experiments, parameters) %&gt;% \n        mutate(paper_id = x)\n      \n      return(single_paper)\n    })\n\n\n[1] \"drawing paper number  1\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  2\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  3\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing paper number  4\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  5\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing paper number  6\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  7\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  8\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  9\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing paper number  10\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  11\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing paper number  12\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  4\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  13\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  4\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  14\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing paper number  15\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  16\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing paper number  17\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  18\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  3\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing experiment number  4\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  19\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing paper number  20\"\n[1] \"drawing experiment number  1\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n[1] \"drawing intervention number  3\"\n[1] \"drawing experiment number  2\"\n[1] \"drawing intervention number  1\"\n[1] \"drawing intervention number  2\"\n\n\nAnd the random effects random effects.\n\n\nCode\n# Calculate covariances\ncovar_0v_paper &lt;- paper_rho_0v * paper_0 * paper_v\ncovar_0c_paper &lt;- paper_rho_0c * paper_0 * paper_c\ncovar_0cv_paper &lt;- paper_rho_0cv * paper_0 * paper_cv\ncovar_vc_paper &lt;- paper_rho_vc * paper_v * paper_c\ncovar_vcv_paper &lt;- paper_rho_vcv * paper_v * paper_cv\ncovar_cc_paper &lt;- paper_rho_ccv * paper_c * paper_cv\n\n# Create the variance-covariance matrix\ncov_mx_paper &lt;- matrix(\n  c(paper_0^2, covar_0v_paper, covar_0c_paper, covar_0cv_paper,\n    covar_0v_paper, paper_v^2, covar_vc_paper, covar_vcv_paper,\n    covar_0c_paper, covar_vc_paper, paper_c^2, covar_cc_paper,\n    covar_0cv_paper, covar_vcv_paper, covar_cc_paper, paper_cv^2),\n  nrow = 4, byrow = TRUE\n)\n\n# Generate the by-paper random effects\npaper_rfx &lt;- MASS::mvrnorm(n = n_papers,\n                         mu = c(PAP_0 = 0, PAP_v = 0, PAP_c = 0, PAP_cv = 0),\n                         Sigma = cov_mx_paper)\n\n# Combine with paper IDs\npaper_rfx &lt;- data.frame(paper_id = seq_len(n_papers), paper_rfx)\n\n# add random effects to paper data\npapers &lt;- left_join(papers, paper_rfx)\n\n\nFinally, we want to be able to uniquely identify participants and experiments across papers (for now, there is a participant labeled “1” in each experiment, and there is an experiment “1” in each paper).\n\n\nCode\npapers &lt;- papers %&gt;% \n  mutate(    \n    # unique experiment identifier\n    unique_experiment_id = paste(paper_id, experiment_id, sep = \"_\"), \n    # unique participant identifier\n    unique_subject_id = paste0(paper_id, \"_\", experiment_id, intervention_id, \"_\", subject_id), \n    # unique intervention identifier\n    unique_intervention_id = paste(paper_id, experiment_id, intervention_id, sep = \"_\"))\n\n\n\nFunction\n\n\nCode\n# simulate papers\ndraw_papers &lt;- function(\n    paper_0, # analogous to experiment random effects\n    paper_v,\n    paper_c,\n    paper_cv,\n    paper_rho_0v,\n    paper_rho_0c,\n    paper_rho_0cv,\n    paper_rho_vc,\n    paper_rho_vcv, \n    paper_rho_ccv,\n    n_papers = 10, # number of papers in the meta analysis\n    ...) {\n  \n  # draw papers\n  papers &lt;-  1:n_papers %&gt;%\n    map_dfr(function(x) {\n      \n      # To keep track of progress\n      print(paste(\"drawing paper number \", x))\n      \n      single_paper &lt;- do.call(draw_multiple_experiments, parameters) %&gt;% \n        mutate(paper_id = x)\n      \n      return(single_paper)\n    })\n  \n  # Calculate covariances\n  covar_0v_paper &lt;- paper_rho_0v * paper_0 * paper_v\n  covar_0c_paper &lt;- paper_rho_0c * paper_0 * paper_c\n  covar_0cv_paper &lt;- paper_rho_0cv * paper_0 * paper_cv\n  covar_vc_paper &lt;- paper_rho_vc * paper_v * paper_c\n  covar_vcv_paper &lt;- paper_rho_vcv * paper_v * paper_cv\n  covar_cc_paper &lt;- paper_rho_ccv * paper_c * paper_cv\n  \n  # Create the variance-covariance matrix\n  cov_mx_paper &lt;- matrix(\n    c(paper_0^2, covar_0v_paper, covar_0c_paper, covar_0cv_paper,\n      covar_0v_paper, paper_v^2, covar_vc_paper, covar_vcv_paper,\n      covar_0c_paper, covar_vc_paper, paper_c^2, covar_cc_paper,\n      covar_0cv_paper, covar_vcv_paper, covar_cc_paper, paper_cv^2),\n    nrow = 4, byrow = TRUE\n  )\n  \n  # Generate the by-paper random effects\n  paper_rfx &lt;- MASS::mvrnorm(n = n_papers,\n                             mu = c(PAP_0 = 0, PAP_v = 0, PAP_c = 0, PAP_cv = 0),\n                             Sigma = cov_mx_paper)\n  \n  # Combine with paper IDs\n  paper_rfx &lt;- data.frame(paper_id = seq_len(n_papers), paper_rfx)\n  \n  # add random effects to paper data\n  papers &lt;- left_join(papers, paper_rfx) %&gt;% \n  mutate(    \n    # unique experiment identifier\n    unique_experiment_id = paste(paper_id, experiment_id, sep = \"_\"), \n    # unique participant identifier\n    unique_subject_id = paste0(paper_id, \"_\", experiment_id, intervention_id, \"_\", subject_id), \n    # unique intervention identifier\n    unique_intervention_id = paste(paper_id, experiment_id, intervention_id, sep = \"_\"))\n}\n\n# test \n# test &lt;- do.call(draw_papers, parameters)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#model-accuracy-ratings",
    "href": "preregistration/simulation.html#model-accuracy-ratings",
    "title": "Data simulation for preregistration",
    "section": "Model accuracy ratings",
    "text": "Model accuracy ratings\nAt this point, we have simulated data with all necessary information to generate accuracy ratings, according to our model.\n\n\nCode\n# Calculate accuracy\nfinal_data &lt;- papers %&gt;% \n  mutate(\n    # make numeric helper variables using deviation coding\n    veracity_numeric = ifelse(veracity == \"true\", 0.5, -0.5),\n    condition_numeric = ifelse(condition == \"intervention\",  0.5, -0.5),\n    # calculate z-value accuracy\n    accuracy_z_value = \n      (beta_0 + SU_0 + EXP_0 + PAP_0) +\n      (beta_v + SU_v + EXP_v + PAP_v)*veracity_numeric +\n      (beta_c + EXP_c + PAP_c )*condition_numeric+\n      (beta_cv + EXP_cv + PAP_cv)*(condition_numeric*veracity_numeric) +\n      e_s,\n    # use the probit function (i.e. cumulative distribution function of the standard normal distribution)\n    # to optain probabilities from the z-values\n    accuracy_probability = pnorm(accuracy_z_value), \n    # generate a binary accuracy response by sampling from a bernoulli distribution\n    # (i.e. binomial distribution with one trial)\n    accuracy = rbinom(nrow(.), 1, accuracy_probability)\n  ) %&gt;% \n  # remove random effect variables\n  select(-matches(\"SU|EXP|PAP\", ignore.case = FALSE))\n\n\n\nFunction\n\n\nCode\n# calculate accuracy outcomes\ncalculate_accuracy &lt;- function(\n    data, \n    # fixed effects\n    beta_0, # intercept (- average response bias )\n    beta_v, # average sensitivity (d prime) \n    beta_c, # - delta response bias (i.e. - delta c)\n    beta_cv, # delta sensitivity (i.e. delta d prime)\n    ...\n    ){\n    \n    data &lt;- data %&gt;% \n    mutate(\n      # make numeric helper variables using deviation coding\n      veracity_numeric = ifelse(veracity == \"true\", 0.5, -0.5),\n      condition_numeric = ifelse(condition == \"intervention\",  0.5, -0.5),\n      # calculate z-value accuracy\n      accuracy_z_value = \n        (beta_0 + SU_0 + EXP_0 + PAP_0) +\n        (beta_v + SU_v + EXP_v + PAP_v)*veracity_numeric +\n        (beta_c + EXP_c + PAP_c )*condition_numeric+\n        (beta_cv + EXP_cv + PAP_cv)*(condition_numeric*veracity_numeric) +\n        e_s,\n      # use the probit function (i.e. cumulative distribution function of the standard normal distribution)\n      # to optain probabilities from the z-values\n      accuracy_probability = pnorm(accuracy_z_value), \n      # generate a binary accuracy response by sampling from a bernoulli distribution\n      # (i.e. binomial distribution with one trial)\n      accuracy = rbinom(nrow(.), 1, accuracy_probability)\n    ) %&gt;% \n    # remove random effect variables\n    select(-matches(\"SU|EXP|PAP\", ignore.case = FALSE))\n}\n\n# test\n# test &lt;- calculate_accuracy(papers)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#add-moderators",
    "href": "preregistration/simulation.html#add-moderators",
    "title": "Data simulation for preregistration",
    "section": "Add moderators",
    "text": "Add moderators\n\nWithin-experiment moderator\nThe most important within-participant moderator we will look at is political concordance. Let’s assume that half our our news item ratings were politically concordant and half not.\n\n\nCode\n# add some other variables for demonstration purpose\ndata &lt;- data %&gt;% \n  mutate(\n    # political concordance\n    political_concordance = ifelse(item_id %% 2 == 0, \"concordant\", \"discordant\"),\n    # add a deviation coded version for the political concordance variable\n    concordance_numeric = ifelse(political_concordance == \"discordant\", \n                                 -0.5, ifelse(!is.na(political_concordance), \n                                              0.5, \n                                              NA)\n    )\n  ) \n\n# check\n# data %&gt;% \n#   select(concordance_numeric, political_concordance)\n\n\nA second moderator that varies within experiments, but between participants, is age.\n\n\nCode\n# randomly assing intervention types\nparticipant_ages &lt;- data %&gt;%\n# Get unique combinations of unique_experiment_id and condition\n  distinct(unique_subject_id) %&gt;% \n  mutate(\n    # Sample from intervention types only when condition is not \"control\"\n    age = sample(seq(18:70), size = nrow(.), replace = TRUE)\n  )\n\n# add intervention types to data\ndata &lt;- left_join(data, participant_ages)\n\n\n\n\nBetween-experiment moderator\nThe most important between-experiment moderator we will look at is intervention type. To illustrate, let’s assume that all used interventions can be classified as one out of three broad categories of interventions (let’s call them “literacy tips”, “priming”, “warning labels”).\nSometimes, within the same experiment, researchers might have tested different interventions. Since we will likely have to deal with that case, we should simulate it here.\n\n\nCode\n# randomly assing intervention types\nintervention_types &lt;- data %&gt;%\n# Get unique combinations of unique_experiment_id and condition\n  distinct(unique_intervention_id, condition) %&gt;% \n  mutate(\n    # Sample from intervention types only when condition is not \"control\"\n    intervention_type = ifelse(condition != \"control\",\n                               sample(c(\"literacy tips\", \"priming\", \"warning labels\"), \n                                      size = sum(condition != \"control\"), \n                                      replace = TRUE),\n                               \"control\")\n  )\n\n# add intervention types to data\ndata &lt;- left_join(data, intervention_types)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#store-simulated-data",
    "href": "preregistration/simulation.html#store-simulated-data",
    "title": "Data simulation for preregistration",
    "section": "Store simulated data",
    "text": "Store simulated data\n\n\nCode\n# store individual level data\nfilename &lt;- \"../data/simulations/individual_level.csv\" \nwrite_csv(data, filename)",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#estimator",
    "href": "preregistration/simulation.html#estimator",
    "title": "Data simulation for preregistration",
    "section": "Estimator",
    "text": "Estimator\nWe start by defining the model we use to estimate our effects, and select the outcomes of interest. We will run a participant-level meta analysis using a two-stage approach: First, for each experiment, we calculate the above mixed model. The resulting estimates are our effect sizes. Second, we run a meta analysis on these effect sizes.\n\nDefine the model\nWe first write a function for calculating the model for each experiment.\n\n\nCode\n# calculate model function\ncalculate_SDT_model &lt;- function(data) {\n  \n  time &lt;- system.time({\n    model &lt;- glmer(accuracy ~ veracity_numeric + condition_numeric + \n                     veracity_numeric*condition_numeric +\n                     (1 + veracity_numeric | unique_subject_id),\n                   data = data, \n                   family = binomial(link = \"probit\")\n    )\n  })\n  \n  time &lt;- round(time[3]/60, digits = 2)\n  \n  # get a tidy version\n  model &lt;- tidy(model, conf.int = TRUE) %&gt;% \n    # add time\n    mutate(time_minutes = time)\n  \n  # give nicer names in SDT terminology to estimates (! and reverse estimates for response bias !)\n  model &lt;- model %&gt;% \n    mutate(\n      # reverse c and delta c estimates\n      estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\", \n                        -1*estimate, estimate),\n      term = case_when(term == \"(Intercept)\" ~ \"average response bias (c)\", \n                       term == \"veracity_numeric\" ~ \"average sensitivity (d')\", \n                       term == \"condition_numeric\" ~ \"delta c\",\n                       term == \"veracity_numeric:condition_numeric\" ~ \"delta d'\",\n                       TRUE ~ term\n      ),\n      sampling_variance = std.error^2\n    ) \n  \n  return(model)\n  \n}\n# test\n# mixed_model &lt;- calculate_SDT_model(data %&gt;% filter(unique_experiment_id == \"1_1\"))\n# mixed_model\n\n\n\n\nRun the loop\nWe then run a for loop, to calculate one model per experiment and store the results in a common data frame.\n\n\nCode\n# loop over experiments\nrun_SDT_model_loop &lt;- function(data){\n  \n  # make a vector with all unique experiment ids\n  experiments &lt;- data %&gt;% \n    distinct(unique_experiment_id) %&gt;% \n    # slice(1:3) %&gt;% # to test loop\n    pull()\n  \n  time &lt;- system.time({\n    \n    # run one model per experiment and store the results in a common data frame\n    results &lt;- experiments %&gt;%\n      map_dfr(function(x) {\n        \n        # restrict data to only the respective experiment\n        experiment &lt;- data %&gt;% filter(unique_experiment_id == x)\n        \n        # extract paper id\n        paper_id &lt;- unique(experiment$paper_id)\n        \n        # To keep track of progress\n        print(paste(\"calculating model for experiment \", x))\n        \n        model_experiment &lt;- calculate_SDT_model(experiment) %&gt;%\n          mutate(unique_experiment_id = x,\n                 paper_id = paper_id)\n        \n        return(model_experiment)\n      })\n  })\n  \n  print(paste(\"Elapsed time: \", round(time[3]/60, digits = 2), \" minutes\"))\n  \n  return(results)\n}\n\n\n# test\n# experiment_ids_to_test &lt;- data %&gt;% distinct(unique_experiment_id) %&gt;% slice(1:5) %&gt;% pull()\n# test &lt;- run_SDT_model_loop(data%&gt;% filter(unique_experiment_id %in% experiment_ids_to_test))\n# test\n\n\n\n\nRun the meta-analysis\n\n\nCode\n# Function to calculate meta models\ncalculate_meta_model &lt;- function(data, yi, vi, robust = TRUE) {\n  \n  # provide metafor compatible names\n  metafor_data &lt;- data %&gt;% \n    rename(yi = {{yi}}, \n           vi = {{vi}})\n  \n  # Multilevel random effect model for accuracy\n  model &lt;-  metafor::rma.mv(yi, vi, random = ~ 1 | paper_id / unique_experiment_id, \n                            data = metafor_data)\n  \n  return(model)\n  \n  if(robust == TRUE) {\n    # with robust standard errors clustered at the paper level \n    robust_model &lt;- robust(model, cluster = data$paper_id)\n    \n    return(robust_model)\n  }\n}\n# test \n\n# generate some model output\n# experiment_ids_to_test &lt;- data %&gt;% distinct(unique_experiment_id) %&gt;% slice(1:5) %&gt;% pull()\n# test_model_output &lt;- run_SDT_model_loop(data%&gt;% filter(unique_experiment_id %in% experiment_ids_to_test))\n# \n# # model for delta dprime\n# delta_dprime &lt;- calculate_meta_model(data = test_model_output  %&gt;% \n#                                        filter(term == \"delta d'\"), yi = estimate, \n#                                      vi = sampling_variance, robust = TRUE) %&gt;% \n#   tidy() %&gt;% \n#   mutate(term = ifelse(term == \"overall\", \"delta d'\", NA))\n# \n# # model for delta c\n# delta_c &lt;- calculate_meta_model(data = test_model_output  %&gt;% \n#                                   filter(term == \"delta c\"), yi = estimate, \n#                                 vi = sampling_variance, robust = TRUE) %&gt;% \n#   tidy() %&gt;% \n#   mutate(term = ifelse(term == \"overall\", \"delta c\", NA))\n\n\nWe combine all these analysis steps in a single estimate function.\n\n\nCode\n# final function for running all models \nget_meta_estimates &lt;- function(data){\n  \n  # generate outcome data for all experiments\n  outcome_data &lt;- run_SDT_model_loop(data)\n  \n  # calculate meta model for delta dprime\n  delta_dprime &lt;- calculate_meta_model(data = outcome_data  %&gt;% \n                                         filter(term == \"delta d'\"), \n                                       yi = estimate, \n                                       vi = sampling_variance, \n                                       robust = TRUE) %&gt;% \n    tidy() %&gt;% \n    mutate(term = ifelse(term == \"overall\", \"delta d'\", NA))\n  \n  # calculate meta model for delta c\n  delta_c &lt;- calculate_meta_model(data = outcome_data  %&gt;% \n                                    filter(term == \"delta c\"), \n                                  yi = estimate, \n                                  vi = sampling_variance, \n                                  robust = TRUE) %&gt;% \n    tidy() %&gt;% \n    mutate(term = ifelse(term == \"overall\", \"delta c\", NA))\n  \n  estimates &lt;- bind_rows(delta_dprime, delta_c)\n  \n  return(estimates)\n  \n}\n\n# test \n\n# reduce data to some experiments only\n# experiment_ids_to_test &lt;- data %&gt;% distinct(unique_experiment_id) %&gt;% slice(1:5) %&gt;% pull()\n# \n# result &lt;- get_meta_estimates(data %&gt;% filter(unique_experiment_id %in% experiment_ids_to_test))\n# result",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#run-iterations",
    "href": "preregistration/simulation.html#run-iterations",
    "title": "Data simulation for preregistration",
    "section": "Run iterations",
    "text": "Run iterations\nFor each combination of effect sizes, we run several iterations, so that we can get an impression of how well different meta-analytic samples recover the parameters. For each iteration, the function first generates a set of studies to run the meta-analysis on, and then runs the meta-analysis. The results will be stored in a single data frame, in which iterations are labeled.\n\n\nCode\n# repeat the process and store some information on it (e.g. time taken, number of iteration)\niterate &lt;- function(iterations, ...) {\n  \n  # create data frame with model results for generated samples\n  result &lt;- 1:iterations %&gt;%\n    purrr::map_df(function(x) {\n      \n      # Measure time for each iteration\n      iteration_time &lt;- system.time({\n        # generate data\n        participant_level_data &lt;- simulate_data(...)\n        \n        # run models on the data\n        estimates &lt;- get_meta_estimates(participant_level_data)\n      })\n      \n      # Add iteration number and time in minutes to estimates\n      estimates &lt;- estimates %&gt;%\n        mutate(iteration = x,\n               time_taken_minutes = iteration_time[3] / 60)  # Convert time to minutes\n      \n      # To keep track of progress\n      if (x %% 2 == 0) {print(paste(\"iteration number \", x))}\n      \n      return(estimates)\n    })\n  \n  return(result)\n}\n# test\n# note that even for just 10 meta-analyses, this takes quite a while, ~1min per iteration\n# test &lt;- do.call(iterate, c(parameters, list(iterations = 4)))\n# \n# ggplot(test %&gt;% filter(term == \"delta d'\"), aes(x = estimate)) +\n#   geom_histogram() +\n#   geom_vline(xintercept = parameters$beta_cv, linetype = \"dotted\", color = \"black\")\n# \n# ggplot(test %&gt;% filter(term == \"delta c\"), aes(x = estimate)) +\n#   geom_histogram() +\n#   geom_vline(xintercept = -parameters$beta_c, linetype = \"dotted\", color = \"black\")",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "preregistration/simulation.html#footnotes",
    "href": "preregistration/simulation.html#footnotes",
    "title": "Data simulation for preregistration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfor binary variables, this is the probability of choosing accurate as an answer.↩︎\nNote that the distributions here are assumed to be independent. Below, we specify that they are from multivariate distributions.↩︎",
    "crumbs": [
      "Preregistration",
      "Data simulation for preregistration"
    ]
  },
  {
    "objectID": "manuscript/manuscript.html",
    "href": "manuscript/manuscript.html",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "",
    "text": "This manuscript is not yet at the stage of a working paper\n\n\n\nIt currently shows highly preliminary results based on very few studies. These preliminary analyses have been performed for the project to be presented at the 15th Annual Conference of the European Political Science Association (EPSA), at Universidad Carlos III de Madrid, from June 26-28, 2025.\nAll analyses performed on the data are strictly in line with our preregistration, registered on the OSF on December 2, 2024. No additional analyses have been performed.\n\n\n\n\nCode\n# load plot theme\nsource(\"../R/plot_theme.R\") \n\n# load other functions\nsource(\"../R/custom_functions.R\")\n\n\n\n\nCode\n# load data\ndata &lt;- readRDS(\"../data/data.rds\")\n\n\n\nIntroduction\nIn recent years, many studies have tested interventions designed to help people detect online misinformation. However, the results are often not directly comparable, because researchers have used different modes of evaluating the effectiveness of these interventions (Guay et al. 2023). Moreover, the most popular outcome measure–a discernment score based on Likert-scale mean differences between true and false news–has recently been shown to be biased (Higham, Modirrousta-Galian, and Seabrooke 2024).\nThe aim of our paper is to re-analyze the effectiveness of individual-level interventions designed to reduce people’s susceptibility for believing in misinformation. Following a recent literature (Higham, Modirrousta-Galian, and Seabrooke 2024; Gawronski, Nahon, and Ng 2024; Modirrousta-Galian and Higham 2023; Batailler et al. 2019) we will use a Signal Detection Theory (SDT) framework. This allows us to evaluate two different effects of interventions: First, the effect on sensitivity, which is the ability of discriminating between true and false news. Second, the effect on response bias, which is the extent to which participants shift their general response criterion, i.e. the extent to which they become generally more/less skeptical in their accuracy ratings for all news (regardless of whether true or false).\nWe formulate two main research questions:\nRQ1: How do interventions against misinformation affect people’s ability to discriminate between true and false news (sensitivity, or “d’”, in a SDT framework)?\nRQ2: How do interventions against misinformation affect people’s skepticism towards news in general (i.e. response bias, or “c”, in a SDT framework)?\nWe also test some moderator effects, such as the type of interventions, the concordance of the news with people’s political identity, and age.\nTo answer our research questions we run an Individual Participant Data meta-analysis (IPD), using a two-stage approach: First, we extract individual participant data from relevant studies and run a Signal Detection Theory analysis separately for each experiment. Second, we run a meta-analysis on the outcomes of the experiments.\n\n\nResults\nTable 1 shows how instances of news ratings map onto SDT terminology. Our analysis measures the effects of misinformation interventions on two outcomes of Signal Detection Theory (SDT): \\(d'\\) (“d prime”, sensitivity), and \\(c\\) (response bias).\n\n\nCode\n# Data\ntable_data &lt;- tibble(\n  Stimulus = c(\"True news (target)\",\"False news (distractor)\"),\n  Accurate = c(\"Hit\", \"False alarm\"),\n  `Not Accurate` = c(\"Miss\", \"Correct rejection\")\n)\n\n# Set Stimulus as row names\n# rownames(table_data) &lt;- table_data$Stimulus\n# table_data$Stimulus &lt;- NULL\n\n# Create table using kable\nkable(table_data, \n      booktabs = TRUE) %&gt;%\n  kable_paper(full_width = FALSE) %&gt;%\n  add_header_above(c(\" \", \"Response\" = 2))\n# Data\ntable_data &lt;- tibble(\n  Stimulus = c(\"True news (target)\", \"False news (distractor)\"),\n  Accurate = c(\"Hit\", \"False alarm\"),\n  `Not Accurate` = c(\"Miss\", \"Correct rejection\"),\n  `SDT Metric` = c(\"Hit rate (HR) = Hits / (Hits + Misses)\", \n             \"False alarm rate (FAR) = False Alarms / (False Alarms + Correct Rejections)\")\n)\n\n# Create table using kable\nkable(table_data, booktabs = TRUE, escape = FALSE) %&gt;%\n  kable_paper(full_width = FALSE) %&gt;%\n  add_header_above(c(\" \", \"Participant response\" = 2, \" \"))\n\n\n\n\nTable 1: Accuracy ratings in Signal Detection Theory terms\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\nStimulus\nAccurate\nNot Accurate\n\n\n\n\nTrue news (target)\nHit\nMiss\n\n\nFalse news (distractor)\nFalse alarm\nCorrect rejection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant response\n\n\n\n\nStimulus\nAccurate\nNot Accurate\nSDT Metric\n\n\n\n\nTrue news (target)\nHit\nMiss\nHit rate (HR) = Hits / (Hits + Misses)\n\n\nFalse news (distractor)\nFalse alarm\nCorrect rejection\nFalse alarm rate (FAR) = False Alarms / (False Alarms + Correct Rejections)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  filter(condition == \"control\") |&gt; \n  group_by(veracity) |&gt; \n  summarise(\n    not_accurate = sum(accuracy == 0, na.rm=TRUE),\n    accurate = sum(accuracy == 1, na.rm=TRUE)) |&gt; \n  mutate(sdt_outcome = accurate/ (accurate + not_accurate))\n\n\n# A tibble: 2 × 4\n  veracity not_accurate accurate sdt_outcome\n  &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1 false           60536    23387       0.279\n2 true            27138    56458       0.675\n\n\nCode\ndata |&gt; \n  filter(condition == \"treatment\") |&gt; \n  group_by(veracity) |&gt; \n  summarise(\n    not_accurate = sum(accuracy == 0, na.rm=TRUE),\n    accurate = sum(accuracy == 1, na.rm=TRUE)) |&gt; \n  mutate(sdt_outcome = accurate/ (accurate + not_accurate))\n\n\n# A tibble: 2 × 4\n  veracity not_accurate accurate sdt_outcome\n  &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1 false           67329    21805       0.245\n2 true            24915    59820       0.706\n\n\n\n\nCode\n# Running this model takes some time. We therefor stored the results in a data frame that we can reload. \nfilename &lt;- \"../data/models/models_by_experiment.csv\" \n\n# run a loop with the sdt model separatel for each experiment\nrun_loop(data, filename)\n\n# read saved model results\nmodel_results &lt;- read_csv(filename)\n\n\n\n\nCode\n# make plot\nggplot(model_results, aes(x = estimate, fill = SDT_term)) +\n  geom_density(alpha = 0.5, adjust = 1.5) +\n  # colors \n  scale_fill_viridis_d(option = \"inferno\", begin = 0.1, end = 0.9) +\n  # labels and scales\n  labs(x = \"z-Score\", y = \"Density\") +\n  guides(fill = FALSE, color = FALSE) +\n  plot_theme +\n  theme(strip.text = element_text(size = 14)) +\n  facet_wrap(~SDT_term)\n\n\n\n\n\n\n\n\nFigure 1: Distributions of Signal Detection Theory outcomes across experiments. Note that these distributions are purely descriptive - effect sizes are not weighted by sample size of the respective experiment, as they are in the meta-analysis.\n\n\n\n\n\n\n\nCode\n# model for delta dprime\ndelta_dprime &lt;- calculate_meta_model(data = model_results |&gt; \n                                       filter(SDT_term == \"delta d'\"), \n                                     yi = SDT_estimate, \n                                     vi = sampling_variance, \n                                     robust = TRUE) |&gt; \n  tidy(conf.int=TRUE) |&gt; \n    mutate(term = ifelse(term == \"overall\", \"delta d'\", NA))\n\n# model for delta c\ndelta_c &lt;- calculate_meta_model(data = model_results |&gt;\n                                  filter(SDT_term == \"delta c\"), \n                                yi = SDT_estimate, \n                                vi = sampling_variance, \n                                robust = TRUE) |&gt; \n  tidy(conf.int=TRUE) |&gt; \n    mutate(term = ifelse(term == \"overall\", \"delta c\", NA))\n\nmeta_estimates &lt;- bind_rows(delta_dprime, delta_c)\n\n\n\n\nCode\n## make plot data\nforest_data &lt;- model_results |&gt; \n  filter(SDT_term %in% c(\"delta c\", \"delta d'\")) |&gt; \n  # Calculate weights (e.g., inverse of standard error)\n  mutate(weight = 1 / sqrt(sampling_variance)) |&gt; \n  group_by(SDT_term) |&gt; \n  arrange(desc(SDT_estimate)) |&gt; \n  mutate(position = 6+row_number()) |&gt; \n  ungroup()\n\n## model outcome\nforest_meta &lt;- meta_estimates |&gt; \n  mutate_if(is.numeric, round, digits = 2) |&gt; \n  mutate(\n    # rename term to be coherent with forest data\n    SDT_term = term,\n    # make label for plot\n    label = paste0(estimate, \" [\", conf.low, \", \", conf.high, \"]\"))\n\n## Plot using ggplot\nggplot(forest_data, aes(x = SDT_estimate, y = position, xmin = SDT_conf.low, xmax = SDT_conf.high)) +\n  geom_pointrange(size = 0.1) +\n  geom_pointrange(data = forest_meta, \n                  aes(x = estimate, y = 0, xmin = conf.low, xmax = conf.high), \n                  shape = 5,\n                  inherit.aes = FALSE) + \n  geom_text(data = forest_meta, \n            aes(x = estimate , y = 1, \n                label = label), \n            vjust = 0, hjust = \"center\", size = 3, inherit.aes = FALSE) + \n  scale_color_viridis_d(option = \"plasma\", name = \"Article\", \n                        begin = 0.5, end = 0.9) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Cohen's D\", y = \"Study\") +\n  plot_theme + \n  theme(legend.position = \"left\",\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank()) + \n  facet_wrap(~SDT_term, scales = \"free\")\n\n\n\n\n\n\n\n\nFigure 2: Forest plots for discernment and skepticism bias. The figure displays all effect sizes for both outcomes. Effects are weighed by their sample size. Effect sizes are calculated as z-scores. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the paper level.\n\n\n\n\n\n\n\nCode\n# Function to create SDT plot with baseline comparison\ncreate_sdt_plot &lt;- function(hit, miss, fa, cr, baseline_dprime = NULL, \n                            baseline_crit = NULL, show_response_regions = FALSE) {\n  # Calculate SDT parameters\n  hr &lt;- hit / (hit + miss)\n  fr &lt;- fa / (fa + cr)\n  discernment &lt;- hr - fr\n  zhr &lt;- qnorm(hr)\n  zfr &lt;- qnorm(fr)\n  crit &lt;- -(zhr + zfr) / 2 \n  dprime &lt;- zhr - zfr \n  maxl &lt;- dnorm(0)\n  ry &lt;- 0.01\n  \n  # Generate distributions\n  x_vals &lt;- seq(-4, 4, length.out = 1000)\n  densities_long &lt;- tibble(\n    x = x_vals,\n    `false` = dnorm(x_vals, mean = -dprime/2, sd = 1),\n    `true` = dnorm(x_vals, mean = dprime/2, sd = 1)\n  ) |&gt;\n    pivot_longer(cols = c(`false`, `true`), names_to = \"news\", values_to = \"density\") \n  \n  # Base plot\n  p &lt;- ggplot(densities_long, aes(x = x, y = density, fill = news)) +\n    geom_area(alpha = 0.33, position = \"identity\") +\n    scale_fill_viridis_d() +\n    \n    # Current criterion and labels\n    geom_vline(xintercept = crit, linewidth = 0.3, linetype = \"dashed\") +\n    annotate(\"text\", label = paste0(\"c = \", round(crit, 2)), x = crit + 0.1, y = maxl/2, vjust = -0.5) +\n\n    # Current d-prime\n    annotate(\"segment\", x = -dprime/2, xend = dprime/2, y = maxl, yend = maxl,\n             arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"both\")) +\n    annotate(\"text\", label = paste0(\"d' = \", round(dprime, 2)), x = 0, y = maxl, vjust = -0.5) +\n\n    # Discernment, HR, FR\n    annotate(\"text\", \n             label = paste0(\"discernment = \", round(discernment, 2), \"\\n\",\n                            \"HR = \", round(hr, 2), \"\\n\",\n                            \"FAR = \", round(fr, 2)),\n             x = 3, y = maxl/3, vjust = -0.5, size = 3)\n\n  # Response region annotations\n  if (show_response_regions) {\n    p &lt;- p +\n      annotate(\"segment\", x = crit + 0.2, xend = crit + 1.6, y = ry, yend = ry,\n               linewidth = 0.25, arrow = arrow(length = unit(4, \"pt\"), type = \"closed\")) +\n      annotate(\"text\", label = '\"Accurate\"', x = crit + 1, y = ry, vjust = -0.5) +\n      annotate(\"segment\", x = crit - 0.2, xend = crit - 1.6, y = ry, yend = ry,\n               linewidth = 0.4, arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"last\")) +\n      annotate(\"text\", label = '\"Not accurate\"', x = crit - 1, y = ry, vjust = -0.5)\n  }\n\n  # Styling\n  p &lt;- p +\n    scale_y_continuous(\"Density\", expand = expansion(c(0, 0.15))) +\n    labs(x = \"z-score\") +\n    coord_cartesian(ylim = c(0, maxl)) +\n    theme_minimal() +\n    theme(\n      axis.ticks.y = element_blank(),\n      axis.text.y = element_blank(),\n      legend.position = \"none\",\n      plot.title = element_blank()\n    )\n\n  # Add baseline overlays\n  if (!is.null(baseline_dprime)) {\n    p &lt;- p + \n      annotate(\"segment\", x = -baseline_dprime/2, xend = baseline_dprime/2, \n               y = maxl * 0.9, yend = maxl * 0.9,\n               color = \"gray50\", linetype = \"dashed\", linewidth = 0.3,\n               arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"both\"))\n  }\n\n  if (!is.null(baseline_crit)) {\n    p &lt;- p + \n      geom_vline(xintercept = baseline_crit, color = \"gray50\", \n                 linewidth = 0.1, linetype = \"dotted\") +\n      annotate(\"segment\", x = baseline_crit, xend = crit, \n               y = maxl * 0.5, yend = maxl * 0.5, \n               linetype = \"dashed\", color = \"gray50\",\n               arrow = arrow(length = unit(5, \"pt\"), type = \"closed\"))\n  }\n\n  return(p)\n}\n\n\n# Generate plots\nplots &lt;- list(\n  p1 = create_sdt_plot(60, 40, 40, 60, show_response_regions = TRUE),\n  p2 = create_sdt_plot(70, 30, 30, 70, \n                       baseline_dprime = qnorm(0.6) - qnorm(0.4)),\n  p3 = create_sdt_plot(50, 50, 10, 90,\n                       baseline_dprime = qnorm(0.6) - qnorm(0.4),\n                       baseline_crit = -(qnorm(0.6) + qnorm(0.4)) / 2)\n)\n\n# Combine and label\nfinal_plot &lt;- plots$p1 / (plots$p2 + plots$p3) + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"top\")\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethods\n\nBatailler, Cédric, Skylar M. Brannon, Paul Teas, and Bertram Gawronski. 2019. “A Signal Detection Approach to Understanding the Identification of Fake News,” September. https://osf.io/uc9me/.\n\n\nGawronski, Bertram, Lea S. Nahon, and Nyx L. Ng. 2024. “A Signal-Detection Framework for Misinformation Interventions.” Nature Human Behaviour, October, 1–3. https://doi.org/10.1038/s41562-024-02021-4.\n\n\nGuay, Brian, Adam J. Berinsky, Gordon Pennycook, and David Rand. 2023. “How to Think about Whether Misinformation Interventions Work.” Nature Human Behaviour 7 (8): 1231–33. https://doi.org/10.1038/s41562-023-01667-w.\n\n\nHigham, Philip A., Ariana Modirrousta-Galian, and Tina Seabrooke. 2024. “Mean Rating Difference Scores Are Poor Measures of Discernment: The Role of Response Criteria.” Current Opinion in Psychology 56 (April): 101785. https://doi.org/10.1016/j.copsyc.2023.101785.\n\n\nModirrousta-Galian, Ariana, and Philip A. Higham. 2023. “Gamified Inoculation Interventions Do Not Improve Discrimination Between True and Fake News: Reanalyzing Existing Research with Receiver Operating Characteristic Analysis.” Journal of Experimental Psychology: General."
  },
  {
    "objectID": "literature_search/literature_search.html",
    "href": "literature_search/literature_search.html",
    "title": "Literature Search",
    "section": "",
    "text": "Several recent meta-analyses and systematic reviews have tried to answer how well people distinguish between true and false news (Pfänder and Altay 2025), which variables are associated with that capacity (Sultan et al. 2024), and which interventions could help people improve detecting misinformation (Kozyreva et al. 2024; Sun et al. 2025). Instead of conducting a new systematic search from scratch, we base our review on the studies included in these reviews.\nFirst, we extract all references from studies included in the reviews, identify duplicates and merge all references into a de-duplicated data frame that we will use for the screening. We then have two screening stages: A first round of screening will be based on the abstract. All articles that pass this stage will be selected for full-text analysis.\nAll steps of this literature review are documented.\n\n\n\n\n\n\nReferences\n\nKozyreva, Anastasia, Philipp Lorenz-Spreen, Stefan M. Herzog, Ullrich K. H. Ecker, Stephan Lewandowsky, Ralph Hertwig, Ayesha Ali, et al. 2024. “Toolbox of Individual-Level Interventions Against Online Misinformation.” Nature Human Behaviour, May. https://doi.org/10.1038/s41562-024-01881-0.\n\n\nPfänder, Jan, and Sacha Altay. 2025. “Spotting False News and Doubting True News: A Systematic Review and Meta-Analysis of News Judgements.” Nature Human Behaviour, February, 1–12. https://doi.org/10.1038/s41562-024-02086-1.\n\n\nSultan, Mubashir, Alan N. Tump, Nina Ehmann, Philipp Lorenz-Spreen, Ralph Hertwig, Anton Gollwitzer, and Ralf H. J. M. Kurvers. 2024. “Susceptibility to Online Misinformation: A Systematic Meta-Analysis of Demographic and Psychological Factors.” Proceedings of the National Academy of Sciences 121 (47): e2409329121. https://doi.org/10.1073/pnas.2409329121.\n\n\nSun, Xiaojun, Xuqing Bai, Bizhong Chen, Gengfeng Niu, and Peipei Mao. 2025. “The Impact of Prebunking Interventions Against Misinformation on Discrimination Ability and Criterion: An IPD Network Meta-Analysis,” May. https://doi.org/10.21203/rs.3.rs-6660774/v1.",
    "crumbs": [
      "Literature Search"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html",
    "href": "literature_search/identify_studies.html",
    "title": "Identify studies",
    "section": "",
    "text": "We downloaded the data from https://osf.io/5r7q3. The data frame contains a column that identifies studies which tested an intervention. We reduce the data frame to only these studies.\nThis script identifies paper that tested an intervention from the control conditions news judgments meta-analysis.",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html#pfanderspottingfalsenews2025",
    "href": "literature_search/identify_studies.html#pfanderspottingfalsenews2025",
    "title": "Identify studies",
    "section": "",
    "text": "We downloaded the data from https://osf.io/5r7q3. The data frame contains a column that identifies studies which tested an intervention. We reduce the data frame to only these studies.\nThis script identifies paper that tested an intervention from the control conditions news judgments meta-analysis.",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html#sultansusceptibilityonlinemisinformation2024a",
    "href": "literature_search/identify_studies.html#sultansusceptibilityonlinemisinformation2024a",
    "title": "Identify studies",
    "section": "Sultan et al. (2024)",
    "text": "Sultan et al. (2024)\nWe downloaded a spreadsheet with the final studies and their titles from https://osf.io/zk9vy. Because the data comes in an annoying format, we extracted our reference list based on the papers appendix\n\n\nCode\nsultan &lt;- read_csv(\"review_papers/Sultan_2024.csv\") |&gt; \n  mutate(review = \"Sultan_2024\") |&gt; \n    rename(reference = `Full Reference`) |&gt; \n    group_by(review, Reference) %&gt;% \n    summarize(reference = unique(reference)) |&gt; \n  ungroup() |&gt; \n  select(review, reference)",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html#kozyrevatoolboxindividuallevelinterventions2024",
    "href": "literature_search/identify_studies.html#kozyrevatoolboxindividuallevelinterventions2024",
    "title": "Identify studies",
    "section": "Kozyreva et al. (2024)",
    "text": "Kozyreva et al. (2024)\nWe downloaded a spreadsheet with the final studies and their titles from https://osf.io/ejyh6/ (alternatively the data can also be downloaded here: https://github.com/askozyreva/toolbox/blob/main/data/toolbox_evidence.xlsx).\n\n\nCode\nkozyreva &lt;- read_excel(\"review_papers/Kozyreva_2024.xlsx\") |&gt; \n  mutate(review = \"Kozyreva_2024\") |&gt; \n  rename(reference = References_long) |&gt; \n  select(review, reference)",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html#sunimpactprebunkinginterventions2025",
    "href": "literature_search/identify_studies.html#sunimpactprebunkinginterventions2025",
    "title": "Identify studies",
    "section": "Sun et al. (2025)",
    "text": "Sun et al. (2025)\nWe downloaded the spreadsheet with the studies and their titles from https://osf.io/qz9fj/files/osfstorage?view_only=70cdfe5ad3104e6bb441e9cee25b82ee.\n\n\nCode\nsun &lt;- read_excel(\"review_papers/Sun_2025.xlsx\", skip = 1) |&gt;  # Skip first row (keep it as data)\n  mutate(review = \"Sun_2025\") |&gt; \n  rename(reference = Title) |&gt; \n  drop_na(reference) |&gt; \n  distinct(review, reference)",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "literature_search/identify_studies.html#combine-reviews",
    "href": "literature_search/identify_studies.html#combine-reviews",
    "title": "Identify studies",
    "section": "Combine reviews",
    "text": "Combine reviews\n\nAdd a title column\nFor three reviews (Pfänder and Altay 2025; Sultan et al. 2024; Kozyreva et al. 2024), we extract titles from longer reference entries.\n\n\nCode\n# Combine all datasets into one\ntitles_to_clean &lt;- bind_rows(pfander, sultan, kozyreva) |&gt; \n  # Extract text after year but before journal name/DOI\n  mutate(title = str_extract(reference, \"(?&lt;=\\\\)\\\\.\\\\s).*?(?=\\\\.\\\\s[A-Z])\"))  \n\n\nBecause the regex is not perfect due to different Title format, we export the data and do the missing entries by hand.\n\n\nCode\n#write_csv(all_reviews, \"review_papers/combined_data_titles.csv\")\n\n\nWe then read back the hand-edited file.\n\n\nCode\nby_hand_titles &lt;- read_delim(\"review_papers/combined_data_titles.csv\", \n                        delim = \";\")\n\n\nWe then add the review by Sun et al. (2025) back in.\n\n\nCode\nreviews &lt;- bind_rows(by_hand_titles, \n                     sun |&gt;\n                       mutate(title = reference))\n\n\n\n\nClean titles\nTo be able to match the titles we want to make sure to have them in a similar format\n\n\nCode\n# little helper function to make strings more compatible\nmake_strings_more_compatible &lt;- function(string) {\n  # Convert to lowercase\n  string &lt;- tolower(string)\n  \n  # Remove spaces and any non-alphabetic characters (keeping only letters)\n  string &lt;- gsub(\"[^a-z]\", \"\", string)  # Remove everything that's not a letter\n  \n  # Trim any extra whitespace (not strictly necessary now, but safe to keep)\n  string &lt;- trimws(string)\n  \n  return(string)\n}\n\nreviews &lt;- reviews %&gt;%\n  mutate(\n    compatible_title = make_strings_more_compatible(title)\n  )\n\n\n\n\nIdentify Duplicates\n\n\nCode\n# identify duplicates based on cleaned titles\nduplicates &lt;- reviews %&gt;%\n  group_by(compatible_title) %&gt;%\n  summarize(n_occurences = n(),\n            occurence = toString(unique(review)) ) |&gt; \n  filter(n_occurences &gt; 1) |&gt; \n  mutate(duplicate = TRUE) |&gt; \n  select(compatible_title, duplicate, occurence)\n\n\nNote that there is one apparent duplicate within the Kozyreva study. This is because the same study is coded for different types of interventions in the original data frame. Since this is not the kind of duplicate we are after, we remove it.\n\n\nCode\nduplicates &lt;- duplicates |&gt; \n  filter(compatible_title != \"combatingfakenewsonsocialmediawithsourceratingstheeffectsofuserandexpertreputationratings\")\n\n# identify duplicates in combined data\nreviews &lt;- left_join(reviews, duplicates)\n\n\n\n\nDe-duplicated data\nFinally, we can remove all the duplicates to obtain the spreadsheet for screening.\n\n\nCode\n# Remove duplicates and keep only one entry for each cleaned title\ndeduplicated_reviews &lt;- reviews %&gt;%\n  distinct(compatible_title, .keep_all = TRUE) |&gt;  # Keeps the first occurrence of each title\n  mutate(id = row_number())\n\nwrite_csv(deduplicated_reviews |&gt; \n            # remove unnecessary columns for coding\n            select(-c(compatible_title, title)), \n          \"review_papers/de-duplicated_references.csv\")\n\n\n\n\nCode\ndeduplicated_reviews |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreview\nreference\ntitle\ncompatible_title\nduplicate\noccurence\nid\n\n\n\n\nPfander_2025\nAltay, S., & Gilardi, F. (2023). People Are Skeptical of Headlines Labeled as AI-Generated, Even if True or Human-Made, Because They Assume Full AI Automation. OSF. https://doi.org/10.31234/osf.io/83k9r\nPeople Are Skeptical of Headlines Labeled as AI-Generated, Even if True or Human-Made, Because They Assume Full AI Automation\npeopleareskepticalofheadlineslabeledasaigeneratedeveniftrueorhumanmadebecausetheyassumefullaiautomation\nNA\nNA\n1\n\n\nPfander_2025\nBadrinathan, S. (2021). Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India. American Political Science Review, 115(4), 1325–1341. https://doi.org/10.1017/S0003055421000459\nEducative Interventions to Combat Misinformation: Evidence from a Field Experiment in India\neducativeinterventionstocombatmisinformationevidencefromafieldexperimentinindia\nTRUE\nPfander_2025, Kozyreva_2024\n2\n\n\nPfander_2025\nBago, B., Rosenzweig, L. R., Berinsky, A. J., & Rand, D. G. (2022). Emotion may predict susceptibility to fake news but emotion regulation does not seem to help. Cognition and Emotion, 1–15. https://doi.org/10.1080/02699931.2022.2090318\nEmotion may predict susceptibility to fake news but emotion regulation does not seem to help\nemotionmaypredictsusceptibilitytofakenewsbutemotionregulationdoesnotseemtohelp\nNA\nNA\n3\n\n\nPfander_2025\nBasol, M., Roozenbeek, J., Berriche, M., Uenal, F., McClanahan, W. P., & Linden, S. van der. (2021). Towards psychological herd immunity: Cross-cultural evidence for two prebunking interventions against COVID-19 misinformation. Big Data & Society, 8(1), 205395172110138. https://doi.org/10.1177/20539517211013868\nTowards psychological herd immunity: Cross-cultural evidence for two prebunking interventions against COVID-19 misinformation\ntowardspsychologicalherdimmunitycrossculturalevidencefortwoprebunkinginterventionsagainstcovidmisinformation\nTRUE\nPfander_2025, Kozyreva_2024\n4\n\n\nPfander_2025\nBrashier, N. M., Pennycook, G., Berinsky, A. J., & Rand, D. G. (2021). Timing matters when correcting fake news. Proceedings of the National Academy of Sciences, 118(5), e2020043118. https://doi.org/10.1073/pnas.2020043118\nTiming matters when correcting fake news\ntimingmatterswhencorrectingfakenews\nTRUE\nPfander_2025, Sultan_2024\n5\n\n\nPfander_2025\nClayton, K., Blair, S., Busam, J. A., Forstner, S., Glance, J., Green, G., Kawata, A., Kovvuri, A., Martin, J., Morgan, E., Sandhu, M., Sang, R., Scholz-Bright, R., Welch, A. T., Wolff, A. G., Zhou, A., & Nyhan, B. (2020). Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media. Political Behavior, 42(4), 1073–1095. https://doi.org/10.1007/s11109-019-09533-0\nReal Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media\nrealsolutionsforfakenewsmeasuringtheeffectivenessofgeneralwarningsandfactchecktagsinreducingbeliefinfalsestoriesonsocialmedia\nTRUE\nPfander_2025, Kozyreva_2024\n6\n\n\nPfander_2025\nDias, N., Pennycook, G., & Rand, D. G. (2020). Emphasizing publishers does not effectively reduce susceptibility to misinformation on social media. Harvard Kennedy School Misinformation Review. https://doi.org/10.37016/mr-2020-001\nEmphasizing publishers does not effectively reduce susceptibility to misinformation on social media\nemphasizingpublishersdoesnoteffectivelyreducesusceptibilitytomisinformationonsocialmedia\nNA\nNA\n7\n\n\nPfander_2025\nEun-Ju Lee & Jeong-woo Jang (2023): How Political Identity and Misinformation Priming Affect Truth Judgments and Sharing Intention of Partisan News, Digital Journalism, DOI: 10.1080/21670811.2022.2163413\nHow Political Identity and Misinformation Priming Affect Truth Judgments and Sharing Intention of Partisan News\nhowpoliticalidentityandmisinformationprimingaffecttruthjudgmentsandsharingintentionofpartisannews\nNA\nNA\n8\n\n\nPfander_2025\nGottlieb, J., Adida, C., & Moussa, R. (2022). Reducing Misinformation in a Polarized Context: Experimental Evidence from Côte d’Ivoire. OSF Preprints. https://doi.org/10.31219/osf.io/6x4wy\nReducing Misinformation in a Polarized Context: Experimental Evidence from Côte d’Ivoire\nreducingmisinformationinapolarizedcontextexperimentalevidencefromctedivoire\nNA\nNA\n9\n\n\nPfander_2025\nGuess, A. M., Lerner, M., Lyons, B., Montgomery, J. M., Nyhan, B., Reifler, J., & Sircar, N. (2020). A digital media literacy intervention increases discernment between mainstream and false news in the United States and India. Proceedings of the National Academy of Sciences, 117(27), 15536–15545. https://doi.org/10.1073/pnas.1920498117\nA digital media literacy intervention increases discernment between mainstream and false news in the United States and India\nadigitalmedialiteracyinterventionincreasesdiscernmentbetweenmainstreamandfalsenewsintheunitedstatesandindia\nTRUE\nPfander_2025, Kozyreva_2024, Sun_2025\n10\n\n\nPfander_2025\nGuess, A., McGregor, S., Pennycook, G., & Rand, D. (2024). Unbundling Digital Media Literacy Tips: Results from Two Experiments. OSF. https://doi.org/10.31234/osf.io/u34fp\nUnbundling Digital Media Literacy Tips: Results from Two Experiments\nunbundlingdigitalmedialiteracytipsresultsfromtwoexperiments\nNA\nNA\n11\n\n\nPfander_2025\nAltay, S., De Angelis, A., & Hoes, E. (2024). Media literacy tips promoting reliable news improve discernment and enhance trust in traditional media. Communications Psychology, 2(1), 1–9. https://doi.org/10.1038/s44271-024-00121-5\nMedia literacy tips promoting reliable news improve discernment and enhance trust in traditional media\nmedialiteracytipspromotingreliablenewsimprovediscernmentandenhancetrustintraditionalmedia\nNA\nNA\n12\n\n\nPfander_2025\nLutzke, L., Drummond, C., Slovic, P., & Árvai, J. (2019). Priming critical thinking: Simple interventions limit the influence of fake news about climate change on Facebook. Global Environmental Change, 58, 101964. https://doi.org/10.1016/j.gloenvcha.2019.101964\nPriming critical thinking: Simple interventions limit the influence of fake news about climate change on Facebook\nprimingcriticalthinkingsimpleinterventionslimittheinfluenceoffakenewsaboutclimatechangeonfacebook\nNA\nNA\n13\n\n\nPfander_2025\nLyons, B., Montgomery, J., & Reifler, J. (2023). Partisanship and older Americans’ engagement with dubious political news. OSF. https://doi.org/10.31219/osf.io/etb89\nPartisanship and older Americans’ engagement with dubious political news\npartisanshipandolderamericansengagementwithdubiouspoliticalnews\nNA\nNA\n14\n\n\nPfander_2025\nLyons, B., Modirrousta-Galian, A., Altay, S., & Salovich, N. A. (2024). Reduce blind spots to improve news discernment? Performance feedback reduces overconfidence but does not improve subsequent discernment. https://doi.org/10.31219/osf.io/kgfrb\nReduce blind spots to improve news discernment? Performance feedback reduces overconfidence but does not improve subsequent discernment\nreduceblindspotstoimprovenewsdiscernmentperformancefeedbackreducesoverconfidencebutdoesnotimprovesubsequentdiscernment\nNA\nNA\n15\n\n\nPfander_2025\nMartel, C., Pennycook, G., & Rand, D. G. (2020). Reliance on emotion promotes belief in fake news. Cognitive Research: Principles and Implications, 5(1), 47. https://doi.org/10.1186/s41235-020-00252-3\nReliance on emotion promotes belief in fake news\nrelianceonemotionpromotesbeliefinfakenews\nTRUE\nPfander_2025, Sultan_2024\n16\n\n\nPfander_2025\nModirrousta-Galian, A., Higham, P. A., & Seabrooke, T. (2023). Effects of inductive learning and gamification on news veracity discernment. Journal of Experimental Psychology: Applied, 29(3), 599–619. https://doi.org/10.1037/xap0000458\nEffects of inductive learning and gamification on news veracity discernment\neffectsofinductivelearningandgamificationonnewsveracitydiscernment\nTRUE\nPfander_2025, Sun_2025\n17\n\n\nPfander_2025\nMuda, R., Pennycook, G., Hamerski, D., & Białek, M. (2023). People are worse at detecting fake news in their foreign language. Journal of Experimental Psychology: Applied, 29(4), 712–724. https://doi.org/10.1037/xap0000475\nPeople are worse at detecting fake news in their foreign language\npeopleareworseatdetectingfakenewsintheirforeignlanguage\nNA\nNA\n18\n\n\nPfander_2025\nOrosz, G., Paskuj, B., Faragó, L., & Krekó, P. (2023). A prosocial fake news intervention with durable effects. Scientific Reports, 13(1), 3958. https://doi.org/10.1038/s41598-023-30867-7\nA prosocial fake news intervention with durable effects\naprosocialfakenewsinterventionwithdurableeffects\nTRUE\nPfander_2025, Sun_2025\n19\n\n\nPfander_2025\nPennycook, G., Cannon, T. D., & Rand, D. G. (2018). Prior exposure increases perceived accuracy of fake news. Journal of Experimental Psychology: General, 147(12), 1865–1880. https://doi.org/10.1037/xge0000465\nPrior exposure increases perceived accuracy of fake news\npriorexposureincreasesperceivedaccuracyoffakenews\nNA\nNA\n20\n\n\nPfander_2025\nPereira, F. B., Bueno, N. S., Nunes, F., & Pavão, N. (2023). Inoculation Reduces Misinformation: Experimental Evidence from Multidimensional Interventions in Brazil. Journal of Experimental Political Science, 1–12. https://doi.org/10.1017/XPS.2023.11\nInoculation Reduces Misinformation: Experimental Evidence from Multidimensional Interventions in Brazil\ninoculationreducesmisinformationexperimentalevidencefrommultidimensionalinterventionsinbrazil\nTRUE\nPfander_2025, Kozyreva_2024\n21\n\n\nPfander_2025\nRathje, S., Roozenbeek, J., Van Bavel, J.J. et al. Accuracy and social motivations shape judgements of (mis)information. Nat Hum Behav (2023). https://doi.org/10.1038/s41562-023-01540-w\nAccuracy and social motivations shape judgements of (mis)information\naccuracyandsocialmotivationsshapejudgementsofmisinformation\nTRUE\nPfander_2025, Sun_2025\n22\n\n\nPfander_2025\nRoss, B., Heisel, J., Jung, A.-K., & Stieglitz, S. (2018). Fake News on Social Media: The (In)Effectiveness of Warning Messages.\nFake News on Social Media: The (In)Effectiveness of Warning Messages\nfakenewsonsocialmediatheineffectivenessofwarningmessages\nNA\nNA\n23\n\n\nPfander_2025\nSmelter, T. J., & Calvillo, D. P. (2020). Pictures and repeated exposure increase perceived accuracy of news headlines. Applied Cognitive Psychology, 34(5), 1061–1071. https://doi.org/10.1002/acp.3684\nPictures and repeated exposure increase perceived accuracy of news headlines\npicturesandrepeatedexposureincreaseperceivedaccuracyofnewsheadlines\nTRUE\nPfander_2025, Sultan_2024\n24\n\n\nPfander_2025\nSultan, M., Tump, A. N., Geers, M., Lorenz-Spreen, P., Herzog, S. M., & Kurvers, R. H. J. M. (2022). Time pressure reduces misinformation discrimination ability but does not alter response bias. Scientific Reports, 12(1), 22416. https://doi.org/10.1038/s41598-022-26209-8\nTime pressure reduces misinformation discrimination ability but does not alter response bias\ntimepressurereducesmisinformationdiscriminationabilitybutdoesnotalterresponsebias\nTRUE\nPfander_2025, Sultan_2024\n25\n\n\nSultan_2024\nArechar et al. (2023). Understanding and combatting misinformation across 16 countries on six continents. Nat. Hum. Behav.\nUnderstanding and combatting misinformation across 16 countries on six continents\nunderstandingandcombattingmisinformationacrosscountriesonsixcontinents\nNA\nNA\n26\n\n\nSultan_2024\nBago, B., Rand, D. G., & Pennycook, G. (2020). Fake news, fast and slow: Deliberation reduces belief in false (but not true) news headlines. J. Exp. Psychol. Gen.\nFake news, fast and slow: Deliberation reduces belief in false (but not true) news headlines\nfakenewsfastandslowdeliberationreducesbeliefinfalsebutnottruenewsheadlines\nNA\nNA\n27\n\n\nSultan_2024\nBronstein, M. V., Pennycook, G., Bear, A., Rand, D. G., & Cannon, T. D. (2019). Belief in fake news is associated with delusionality, dogmatism, religious fundamentalism, and reduced analytic thinking. J. Appl. Res. Mem. Cogn.\nBelief in fake news is associated with delusionality, dogmatism, religious fundamentalism, and reduced analytic thinking\nbeliefinfakenewsisassociatedwithdelusionalitydogmatismreligiousfundamentalismandreducedanalyticthinking\nNA\nNA\n28\n\n\nSultan_2024\nBronstein, M. V., Pennycook, G., Buonomano, L., & Cannon, T. D. (2021). Belief in fake news, responsiveness to cognitive conflict, and analytic reasoning engagement. Think. Reason.\nBelief in fake news, responsiveness to cognitive conflict, and analytic reasoning engagement\nbeliefinfakenewsresponsivenesstocognitiveconflictandanalyticreasoningengagement\nNA\nNA\n29\n\n\nSultan_2024\nCalvillo, D. P., & Smelter, T. J. (2020). An initial accuracy focus reduces the effect of prior exposure on perceived accuracy of news headlines. Cogn. Res. Princ. Implic.\nAn initial accuracy focus reduces the effect of prior exposure on perceived accuracy of news headlines\naninitialaccuracyfocusreducestheeffectofpriorexposureonperceivedaccuracyofnewsheadlines\nNA\nNA\n30\n\n\nSultan_2024\nCalvillo, D. P., Ross, B. J., Garcia, R. J. B., Smelter, T. J., & Rutchick, A. M. (2020). Political ideology predicts perceptions of the threat of COVID-19 (and susceptibility to fake news about it). Soc. Psychol. Pers. Sci.\nPolitical ideology predicts perceptions of the threat of COVID-19 (and susceptibility to fake news about it)\npoliticalideologypredictsperceptionsofthethreatofcovidandsusceptibilitytofakenewsaboutit\nNA\nNA\n31\n\n\nSultan_2024\nCalvillo, D. P., Garcia, R. J. B., Bertrand, K., & Mayers, T. A. (2021). Personality factors and self-reported political news consumption predict susceptibility to political fake news. Pers. Individ. Dif.\nPersonality factors and self-reported political news consumption predict susceptibility to political fake news\npersonalityfactorsandselfreportedpoliticalnewsconsumptionpredictsusceptibilitytopoliticalfakenews\nNA\nNA\n32\n\n\nSultan_2024\nCalvillo, D. P., Rutchick, A. M., & Garcia, R. J. B. (2021). Individual differences in belief in fake news about election fraud after the 2020 U.S. election. Behav. Sci.\nIndividual differences in belief in fake news about election fraud after the 2020 U.S. election\nindividualdifferencesinbeliefinfakenewsaboutelectionfraudaftertheuselection\nNA\nNA\n33\n\n\nSultan_2024\nCarnahan, D., Bergan, D. E., & Lee, S. (2021). Do corrective effects last? Results from a longitudinal experiment on beliefs toward immigration in the U.S. Polit. Behav.\nDo corrective effects last? Results from a longitudinal experiment on beliefs toward immigration in the U.S\ndocorrectiveeffectslastresultsfromalongitudinalexperimentonbeliefstowardimmigrationintheus\nNA\nNA\n34\n\n\nSultan_2024\nEpstein, Z., Sirlin, N., Arechar, A., Pennycook, G., & Rand, D. (2023). The social media context interferes with truth discernment. Sci. Adv.\nThe social media context interferes with truth discernment\nthesocialmediacontextinterfereswithtruthdiscernment\nNA\nNA\n35\n\n\nSultan_2024\nGarrett, R. K., & Bond, R. M. (2021). Conservatives’ susceptibility to political misperceptions. Sci. Adv.\nConservatives’ susceptibility to political misperceptions\nconservativessusceptibilitytopoliticalmisperceptions\nNA\nNA\n36\n\n\nSultan_2024\nLongoni, C., Fradkin, A., Cian, L., & Pennycook, G. (2022). News from generative artificial intelligence is believed less. 2022 ACM Conf. Fairness, Accountability, and Transparency.\nNews from generative artificial intelligence is believed less. 2022 ACM Conf\nnewsfromgenerativeartificialintelligenceisbelievedlessacmconf\nNA\nNA\n37\n\n\nSultan_2024\nNewton, C., Feeney, J., & Pennycook, G. (2023). On the disposition to think analytically: Four distinct intuitive-analytic thinking styles. Pers. Soc. Psychol. Bull.\nOn the disposition to think analytically: Four distinct intuitive-analytic thinking styles\nonthedispositiontothinkanalyticallyfourdistinctintuitiveanalyticthinkingstyles\nNA\nNA\n38\n\n\nSultan_2024\nPennycook, G., McPhetres, J., Zhang, Y., Lu, J. G., & Rand, D. G. (2020). Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention. Psychol. Sci.\nFighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention\nfightingcovidmisinformationonsocialmediaexperimentalevidenceforascalableaccuracynudgeintervention\nTRUE\nSultan_2024, Kozyreva_2024\n39\n\n\nSultan_2024\nPennycook, G., Epstein, Z., Mosleh, M., Arechar, A. A., Eckles, D., & Rand, D. G. (2021). Shifting attention to accuracy can reduce misinformation online. Nature.\nShifting attention to accuracy can reduce misinformation online\nshiftingattentiontoaccuracycanreducemisinformationonline\nTRUE\nSultan_2024, Kozyreva_2024\n40\n\n\nSultan_2024\nRoozenbeek, J., Maertens, R., Herzog, S. M., Geers, M., Kurvers, R., Sultan, M., & van der Linden, S. (2022). Susceptibility to misinformation is consistent across question framings and response modes and better explained by myside bias and partisanship than analytical thinking. Judgm. Decis. Mak.\nSusceptibility to misinformation is consistent across question framings and response modes and better explained by myside bias and partisanship than analytical thinking\nsusceptibilitytomisinformationisconsistentacrossquestionframingsandresponsemodesandbetterexplainedbymysidebiasandpartisanshipthananalyticalthinking\nNA\nNA\n41\n\n\nSultan_2024\nRoss, R. M., Rand, D. G., & Pennycook, G. (2021). Beyond ‘fake news’: Analytic thinking and the detection of false and hyperpartisan news headlines. Judgm. Decis. Mak.\nBeyond ‘fake news’: Analytic thinking and the detection of false and hyperpartisan news headlines\nbeyondfakenewsanalyticthinkingandthedetectionoffalseandhyperpartisannewsheadlines\nNA\nNA\n42\n\n\nKozyreva_2024\nEpstein, Z, Berinsky, A. J., Cole, R., Gully, A., Pennycook, G., & Rand, D. G. (2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School Misinformation Review, 2(3).\nDeveloping an accuracy-prompt toolkit to reduce COVID-19 misinformation online\ndevelopinganaccuracyprompttoolkittoreducecovidmisinformationonline\nNA\nNA\n43\n\n\nKozyreva_2024\nRoozenbeek, J., Freeman, A. L. J., & van der Linden, S. (2021). How accurate are accuracy-nudge interventions? A preregistered direct replication of Pennycook et al. (2020). Psychological Science, 32(7), 1169–1178.\nHow accurate are accuracy-nudge interventions? A preregistered direct replication of Pennycook et al. (2020)\nhowaccurateareaccuracynudgeinterventionsapreregistereddirectreplicationofpennycooketal\nNA\nNA\n44\n\n\nKozyreva_2024\nPennycook, G., & Rand, D.G (2022). Accuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation. Nature Communications, 13(1), Article 2333.\nAccuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation\naccuracypromptsareareplicableandgeneralizableapproachforreducingthespreadofmisinformation\nNA\nNA\n45\n\n\nKozyreva_2024\nArechar, A. A., Allen, J. N. L., Berinsky, A. J., Cole, R., Epstein, Z., Garimella, K., Gully, A., Lu, J. G., Moss, R. M., Stagnaro, M. N., Zhang, Y., Pennycook, G., & Rand, D. G. (2023). Understanding and combating misinformation across 16 countries on six continents. Nature Human Behaviour.\nUnderstanding and combating misinformation across 16 countries on six continents\nunderstandingandcombatingmisinformationacrosscountriesonsixcontinents\nNA\nNA\n46\n\n\nKozyreva_2024\nOffer-Westort, M., Rosenzweig, L. R., & Athey, S. (2023).\n\n\n\n\n\n\n\nBattling the coronavirus ‘infodemic’ among social media users in Kenya and Nigeria\nBattling the coronavirus ‘infodemic’ among social media users in Kenya and Nigeria\nbattlingthecoronavirusinfodemicamongsocialmediausersinkenyaandnigeria\nNA\nNA\n47\n\n\n\nKozyreva_2024\nSwire, B., Ecker, U. K. H., & Lewandowsky, S. (2017). The role of familiarity in correcting inaccurate information. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(12), 1948–1961.\nThe role of familiarity in correcting inaccurate information\ntheroleoffamiliarityincorrectinginaccurateinformation\nNA\nNA\n48\n\n\nKozyreva_2024\nHuang, H. (2017). A war of (mis)information: The political effects of rumors and rumor rebuttals in an authoritarian country. British Journal of Political Science, 47(2), 283-312.\nA war of (mis)information: The political effects of rumors and rumor rebuttals in an authoritarian country\nawarofmisinformationthepoliticaleffectsofrumorsandrumorrebuttalsinanauthoritariancountry\nNA\nNA\n49\n\n\nKozyreva_2024\nGesser-Edelsburg, A., Diamant, A., Hijazi, R., & Mesch, G. S. (2018). Correcting misinformation by health organizations during measles outbreaks: A controlled experiment. PloS one, 13(12), e0209505.\nCorrecting misinformation by health organizations during measles outbreaks: A controlled experiment\ncorrectingmisinformationbyhealthorganizationsduringmeaslesoutbreaksacontrolledexperiment\nNA\nNA\n50\n\n\nKozyreva_2024\nPaynter, J., Luskin-Saxby, S., Keen, D., Fordyce, K., Frost, G., Imms, C., Miller, S., Trembath, D., Tucker, M., & Ecker, U. K. H. (2019). Evaluation of a template for countering misinformation—Real-world autism treatment myth debunking. PLOS ONE, 14(1), Article e0210746.\nEvaluation of a template for countering misinformation—Real-world autism treatment myth debunking\nevaluationofatemplateforcounteringmisinformationrealworldautismtreatmentmythdebunking\nNA\nNA\n51\n\n\nKozyreva_2024\nSchmid, P., & Betsch, C. (2019). Effective strategies for rebutting science denialism in public discussions. Nature Human Behaviour, 3(9), 931–939.\nEffective strategies for rebutting science denialism in public discussions\neffectivestrategiesforrebuttingsciencedenialisminpublicdiscussions\nNA\nNA\n52\n\n\nKozyreva_2024\nSchmid, P., Schwarzer, M., & Betsch, C. (2020). Weight-of-evidence strategies to mitigate the influence of messages of science denialism in public discussions. Journal of Cognition, 3(1), Article 36.\nWeight-of-evidence strategies to mitigate the influence of messages of science denialism in public discussions\nweightofevidencestrategiestomitigatetheinfluenceofmessagesofsciencedenialisminpublicdiscussions\nNA\nNA\n53\n\n\nKozyreva_2024\nEcker, U. K. H., Butler, L. H., & Hamby, A. (2020). You don’t have to tell a story! A registered report testing the effectiveness of narrative versus non-narrative misinformation corrections. Cognitive Research: Principles and Implications, 5(1), Article 64.\nYou don’t have to tell a story! A registered report testing the effectiveness of narrative versus non-narrative misinformation corrections\nyoudonthavetotellastoryaregisteredreporttestingtheeffectivenessofnarrativeversusnonnarrativemisinformationcorrections\nNA\nNA\n54\n\n\nKozyreva_2024\nBowles, J., Larreguy, H., & Liu, S. (2020). Countering misinformation via WhatsApp: Preliminary evidence from the COVID-19 pandemic in Zimbabwe. PLOS One, 15(10): e0240005.\nCountering misinformation via WhatsApp: Preliminary evidence from the COVID-19 pandemic in Zimbabwe\ncounteringmisinformationviawhatsapppreliminaryevidencefromthecovidpandemicinzimbabwe\nNA\nNA\n55\n\n\nKozyreva_2024\nPorter, E. & Wood, T. J. (2021). The Global effectiveness of fact-checking: Evidence from simultaneous experiments in Argentina, Nigeria, South Africa, and the United Kingdom. Proceedings of National Academy of Sciences 118(37), e2104235118.\nThe Global effectiveness of fact-checking: Evidence from simultaneous experiments in Argentina, Nigeria, South Africa, and the United Kingdom\ntheglobaleffectivenessoffactcheckingevidencefromsimultaneousexperimentsinargentinanigeriasouthafricaandtheunitedkingdom\nNA\nNA\n56\n\n\nKozyreva_2024\nWinters, M., Oppenheim, B., Sengeh, P., Jalloh, M. B., Webber, N., Pratt, S. A., … & Nordenstedt, H. (2021). Debunking highly prevalent health misinformation using audio dramas delivered by WhatsApp: evidence from a randomised controlled trial in Sierra Leone. BMJ global health, 6(11), e006954.\nDebunking highly prevalent health misinformation using audio dramas delivered by WhatsApp: evidence from a randomised controlled trial in Sierra Leone\ndebunkinghighlyprevalenthealthmisinformationusingaudiodramasdeliveredbywhatsappevidencefromarandomisedcontrolledtrialinsierraleone\nNA\nNA\n57\n\n\nKozyreva_2024\nTay, L. Q., Hurlstone, M. J., Kurz, T., & Ecker, U. K. H. (2022). A comparison of prebunking and debunking interventions for implied versus explicit misinformation. British Journal of Psychology, 113(3), 591–607.\nA comparison of prebunking and debunking interventions for implied versus explicit misinformation\nacomparisonofprebunkinganddebunkinginterventionsforimpliedversusexplicitmisinformation\nNA\nNA\n58\n\n\nKozyreva_2024\nSchmid, P., & Betsch, C. (2022). Benefits and Pitfalls of Debunking Interventions to Counter mRNA Vaccination Misinformation During the COVID-19 Pandemic. Science Communication. 44(5):531–58.\nBenefits and Pitfalls of Debunking Interventions to Counter mRNA Vaccination Misinformation During the COVID-19 Pandemic\nbenefitsandpitfallsofdebunkinginterventionstocountermrnavaccinationmisinformationduringthecovidpandemic\nNA\nNA\n59\n\n\nKozyreva_2024\nBauer, F, & Wilson, K. L. (2022). Reactions to China-linked fake news: Experimental evidence from Taiwan. The China Quarterly 249, 21-46.\nReactions to China-linked fake news: Experimental evidence from Taiwan\nreactionstochinalinkedfakenewsexperimentalevidencefromtaiwan\nNA\nNA\n60\n\n\nKozyreva_2024\nSchroeder, N. L., & Kucera, A. C. (2022). Refutation text facilitates learning: A meta-analysis of between-subjects experiments. Educational Psychology Review, 34(2), 957-987.\nRefutation text facilitates learning: A meta-analysis of between-subjects experiments\nrefutationtextfacilitateslearningametaanalysisofbetweensubjectsexperiments\nNA\nNA\n61\n\n\nKozyreva_2024\nYu, W., Shen, F., & Min, C. (2022). Correcting science misinformation in an authoritarian country: An experiment from China. Telematics and Informatics 66, 101749.\nCorrecting science misinformation in an authoritarian country: An experiment from China\ncorrectingsciencemisinformationinanauthoritariancountryanexperimentfromchina\nNA\nNA\n62\n\n\nKozyreva_2024\nBatista Pereira, F., Bueno, N. S., Nunes, F., & Pavão, N. (2022). Fake news, fact checking, and partisanship: the resilience of rumors in the 2018 Brazilian elections. The Journal of Politics, 84(4), 2188-2201.\nFake news, fact checking, and partisanship: the resilience of rumors in the 2018 Brazilian elections\nfakenewsfactcheckingandpartisanshiptheresilienceofrumorsinthebrazilianelections\nNA\nNA\n63\n\n\nKozyreva_2024\nBadrinathan, S., & Chauchard, S. (2023). I Don’t Think That’s True, Bro!” Social Corrections of Misinformation in India. The International Journal of Press/Politics, 0(0).\nI Don’t Think That’s True, Bro!” Social Corrections of Misinformation in India\nidontthinkthatstruebrosocialcorrectionsofmisinformationinindia\nNA\nNA\n64\n\n\nKozyreva_2024\nPorter, E., Velez, Y., & Wood, T. J. (2023). Correcting COVID-19 vaccine misinformation in 10 countries. Royal Society Open Science, 10(3), 221097.\nCorrecting COVID-19 vaccine misinformation in 10 countries\ncorrectingcovidvaccinemisinformationincountries\nNA\nNA\n65\n\n\nKozyreva_2024\nArmand, A., Augsburg, B., Bancalari, A., & Kameshwara, K. K. (2022). Social proximity and misinformation: Experimental evidence from a mobile phone-based campaign in India. CEPR Discussion Paper No. DP16492 Institute for Fiscal Studies.\nSocial proximity and misinformation: Experimental evidence from a mobile phone-based campaign in India\nsocialproximityandmisinformationexperimentalevidencefromamobilephonebasedcampaigninindia\nNA\nNA\n66\n\n\nKozyreva_2024\nHirshleifer, S., Naseem, M., Raza, A.A., & Rezaee, A. (2023). The spread of (mis)information: A social media experiment in Pakistan. UC Berkeley: Center for Effective Global Action.\nThe spread of (mis)information: A social media experiment in Pakistan\nthespreadofmisinformationasocialmediaexperimentinpakistan\nNA\nNA\n67\n\n\nKozyreva_2024\nBruns, H., Dessart, F. J., Krawczyk, M. W., Lewandowsky, S., Pantazi, M., Pennycook, G., Schmid, P. & Smillie, L. (2023). The role of (trust in) the source of prebunks and debunks of misinformation. Evidence from online experiments in four EU countries. OSF. (Preprint, not peer-reviewed)\nThe role of (trust in) the source of prebunks and debunks of misinformation\ntheroleoftrustinthesourceofprebunksanddebunksofmisinformation\nNA\nNA\n68\n\n\nKozyreva_2024\nFazio, L. K. (2020). Pausing to consider why a headline is true or false can help reduce the sharing of false news.\n\n\n\n\n\n\n\nHarvard Kennedy School (HKS) Misinformation Review.\nPausing to consider why a headline is true or false can help reduce the sharing of false news\npausingtoconsiderwhyaheadlineistrueorfalsecanhelpreducethesharingoffalsenews\nNA\nNA\n69\n\n\n\nKozyreva_2024\nPillai, R. M. & Fazio, L. K. (2023). Explaining why headlines are true or false reduces intentions to share false information. Collabra: Psychology.\nExplaining why headlines are true or false reduces intentions to share false information\nexplainingwhyheadlinesaretrueorfalsereducesintentionstosharefalseinformation\nNA\nNA\n70\n\n\nKozyreva_2024\nvan der Linden, S., Leiserowitz, A., Rosenthal, S., & Maibach, E. (2017). Inoculating the public against misinformation about climate change. Global Challenges, 1(2), Article 1600008.\nInoculating the public against misinformation about climate change\ninoculatingthepublicagainstmisinformationaboutclimatechange\nNA\nNA\n71\n\n\nKozyreva_2024\nCook J., Lewandowsky, S., & Ecker, U. K. H. (2017) Neutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence. PLOS ONE, 12(5), Article e0175799.\nNeutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence\nneutralizingmisinformationthroughinoculationexposingmisleadingargumentationtechniquesreducestheirinfluence\nNA\nNA\n72\n\n\nKozyreva_2024\nRoozenbeek, J., & van der Linden, S. (2019). Fake news game confers psychological resistance against online misinformation. Palgrave Communications, 5(1),Article 65.\nFake news game confers psychological resistance against online misinformation\nfakenewsgameconferspsychologicalresistanceagainstonlinemisinformation\nNA\nNA\n73\n\n\nKozyreva_2024\nRoozenbeek, J., & van der Linden, S. (2020). Breaking Harmony Square: A game that “inoculates” against political misinformation. Harvard Kennedy School (HKS) Misinformation Review.\nBreaking Harmony Square: A game that “inoculates” against political misinformation\nbreakingharmonysquareagamethatinoculatesagainstpoliticalmisinformation\nNA\nNA\n74\n\n\nKozyreva_2024\nMaertens, R., Roozenbeek, J., Basol, M., & van der Linden, S. (2021). Long-term effectiveness of inoculation against misinformation: Three longitudinal experiments. Journal of Experimental Psychology: Applied, 27(1), 1–16.\nLong-term effectiveness of inoculation against misinformation: Three longitudinal experiments\nlongtermeffectivenessofinoculationagainstmisinformationthreelongitudinalexperiments\nNA\nNA\n75\n\n\nKozyreva_2024\nRoozenbeek, J., van der Linden, S., Goldberg, B., Rathje, S., & Lewandowsky, S. (2022). Psychological inoculation improves resilience against misinformation on social media. Science Advances, 8(34), Article eabo6254.\nPsychological inoculation improves resilience against misinformation on social media\npsychologicalinoculationimprovesresilienceagainstmisinformationonsocialmedia\nNA\nNA\n76\n\n\nKozyreva_2024\nLu, C., Hu, B., Li, Q., Bi, C., & Ju, X. D. (2023). Psychological Inoculation for Credibility Assessment, Sharing Intention, and Discernment of Misinformation: Systematic Review and Meta-Analysis. Journal of Medical Internet Research, 25, e49255.\nPsychological Inoculation for Credibility Assessment, Sharing Intention, and Discernment of Misinformation: Systematic Review and Meta-Analysis\npsychologicalinoculationforcredibilityassessmentsharingintentionanddiscernmentofmisinformationsystematicreviewandmetaanalysis\nNA\nNA\n77\n\n\nKozyreva_2024\nWong, C. M. L., & Wu, Y. (2023). Limits to inoculating against the risk of fake news: a replication study in Singapore during COVID-19. Journal of Risk Research, 1-16.\nLimits to inoculating against the risk of fake news: a replication study in Singapore during COVID-19\nlimitstoinoculatingagainsttheriskoffakenewsareplicationstudyinsingaporeduringcovid\nNA\nNA\n78\n\n\nKozyreva_2024\nArmand, A., Fracchia, M., & Vicente, P. C. (2023). Let’s call! using the phone to increase acceptance of covid-19 vaccines (No. wp2113). Health Eonomics, 33, 1.\nLet’s call! using the phone to increase acceptance of covid-19 vaccines (No. wp2113)\nletscallusingthephonetoincreaseacceptanceofcovidvaccinesnowp\nNA\nNA\n79\n\n\nKozyreva_2024\nGarg, N. & Yadav, M., (2022). Learning to Resist Misinformation: A Field Experiment in India. (Preprint, not peer-reviewed)\nLearning to Resist Misinformation: A Field Experiment in India.\nlearningtoresistmisinformationafieldexperimentinindia\nNA\nNA\n80\n\n\nKozyreva_2024\nBowles, J., Croke, K., Larreguy, H., Liu, S., & Marshall, J. (2023). Sustaining Exposure to Fact-checks: Misinformation Discernment, Media Consumption, and its Political Implications.\nSustaining Exposure to Fact-checks: Misinformation Discernment, Media Consumption, and its Political Implications.\nsustainingexposuretofactchecksmisinformationdiscernmentmediaconsumptionanditspoliticalimplications\nNA\nNA\n81\n\n\nKozyreva_2024\nAthey, S. C., Matias , C., Koutout, K., Kristine & Li, Z. (2023). Emotion- versus Reasoning-Based Drivers of Misinformation Sharing: A Field Experiment Using Text Message Courses in Kenya. Stanford University Graduate School of Business Research Paper No. 4489759. SSRN (Preprint, not peer-reviewed)\nEmotion- versus Reasoning-Based Drivers of Misinformation Sharing: A Field Experiment Using Text Message Courses in Kenya\nemotionversusreasoningbaseddriversofmisinformationsharingafieldexperimentusingtextmessagecoursesinkenya\nNA\nNA\n82\n\n\nKozyreva_2024\nWineburg, S., & McGrew, S. (2019). Lateral reading and the nature of expertise: Reading less and learning more when evaluating digital information. Teachers College Record, 121(11), 1–40.\nLateral reading and the nature of expertise: Reading less and learning more when evaluating digital information\nlateralreadingandthenatureofexpertisereadinglessandlearningmorewhenevaluatingdigitalinformation\nNA\nNA\n83\n\n\nKozyreva_2024\nMcGrew, S., Smith, M., Breakstone, J., Ortega, T., Wineburg, S. (2019). Improving university students’ web savvy: An intervention study. British Journal of Educational Psychology, 89(3), 485–500.\nImproving university students’ web savvy: An intervention study\nimprovinguniversitystudentswebsavvyaninterventionstudy\nNA\nNA\n84\n\n\nKozyreva_2024\nMcGrew, S. (2020). Learning to evaluate: An intervention in civic online reasoning. Computers & Education, 145, Article 103711.\nLearning to evaluate: An intervention in civic online reasoning\nlearningtoevaluateaninterventioninciviconlinereasoning\nNA\nNA\n85\n\n\nKozyreva_2024\nDonovan, A. M., & Rapp, D. N. (2020). Look it up: Online search reduces the problematic effects of exposures to inaccuracies. Memory & Cognition, 48(7), 1128–1145.\nLook it up: Online search reduces the problematic effects of exposures to inaccuracies\nlookituponlinesearchreducestheproblematiceffectsofexposurestoinaccuracies\nNA\nNA\n86\n\n\nKozyreva_2024\nBrodsky, J. E., Brooks, P. J., Scimeca, D., Todorova, R., Galati, P., Batson, M., Grosso, R., Matthews, M., Miller, V., & Caulfield, M. (2021). Improving college students’ fact checking strategies through lateral reading instruction in a general education civics course. Cognitive Research: Principles and Implications, 6(1), Article 23.\nImproving college students’ fact checking strategies through lateral reading instruction in a general education civics course\nimprovingcollegestudentsfactcheckingstrategiesthroughlateralreadinginstructioninageneraleducationcivicscourse\nNA\nNA\n87\n\n\nKozyreva_2024\nBreakstone, J., Smith, M., Connors, P., Ortega, T., Kerr, D., & Wineburg, S. (2021). Lateral reading: College students learn to critically evaluate internet sources in an online course. The Harvard Kennedy School Misinformation Review, 2(1).\nLateral reading: College students learn to critically evaluate internet sources in an online course\nlateralreadingcollegestudentslearntocriticallyevaluateinternetsourcesinanonlinecourse\nNA\nNA\n88\n\n\nKozyreva_2024\nYang, S., Lee, J. W., Kim, H. J., Kang, M., Chong, E., & Kim, E. M. (2021). Can an online educational game contribute to developing information literate citizens? Computers & Education, 161, 104057.\nCan an online educational game contribute to developing information literate citizens?\ncananonlineeducationalgamecontributetodevelopinginformationliteratecitizens\nNA\nNA\n89\n\n\nKozyreva_2024\nKobayashi, T., Taka, F., & Suzuki, T. (2021). Can “Googling” correct misbelief? Cognitive and affective consequences of online search. PLoS ONE, 16(9), e0256575.\nCan “Googling” correct misbelief? Cognitive and affective consequences of online search\ncangooglingcorrectmisbeliefcognitiveandaffectiveconsequencesofonlinesearch\nNA\nNA\n90\n\n\nKozyreva_2024\nWineburg, S., Breakstone, J., McGrew, S., Smith, M. D., & Ortega, T. (2022). Lateral reading on the open Internet: A district-wide field study in high school government classes. Journal of Educational Psychology, 114(5), 893–909.\nLateral reading on the open Internet: A district-wide field study in high school government classes\nlateralreadingontheopeninternetadistrictwidefieldstudyinhighschoolgovernmentclasses\nNA\nNA\n91\n\n\nKozyreva_2024\nPanizza, F., Ronzani, P., Martini, C., Mattavelli, S., Morisseau, T., & Motterlini, M. (2022). Lateral reading and monetary incentives to spot disinformation about science. Scientific Reports, 12(1), Article 5678.\nLateral reading and monetary incentives to spot disinformation about science\nlateralreadingandmonetaryincentivestospotdisinformationaboutscience\nNA\nNA\n92\n\n\nKozyreva_2024\nMcGrew, S., & Breakstone, J. (2023). Civic Online Reasoning across the curriculum: Developing and testing the efficacy of digital literacy lessons. AERA Open, 9.\nCivic Online Reasoning across the curriculum: Developing and testing the efficacy of digital literacy lessons\nciviconlinereasoningacrossthecurriculumdevelopingandtestingtheefficacyofdigitalliteracylessons\nNA\nNA\n93\n\n\nKozyreva_2024\nFendt, M., Nistor, N., Scheibenzuber, C., & Artmann, B. (2023). Sourcing against misinformation: Effects of a scalable lateral reading training based on cognitive apprenticeship. Computers in Human Behavior, 146, 107820.\nSourcing against misinformation: Effects of a scalable lateral reading training based on cognitive apprenticeship\nsourcingagainstmisinformationeffectsofascalablelateralreadingtrainingbasedoncognitiveapprenticeship\nTRUE\nKozyreva_2024, Sun_2025\n94\n\n\nKozyreva_2024\nBarzilai. S., Mor-Hagani S., Abed F., Tal-Savir D., Goldik N., Talmon I, Davidow O. (2023). Misinformation Is Contagious: Middle school students learn how to evaluate and share information responsibly through a digital game. Computers & Education, 202, 104832.\nMisinformation Is Contagious: Middle school students learn how to evaluate and share information responsibly through a digital game\nmisinformationiscontagiousmiddleschoolstudentslearnhowtoevaluateandshareinformationresponsiblythroughadigitalgame\nNA\nNA\n95\n\n\nKozyreva_2024\nApuke, O. D., & Gever, C. V. (2023). A quasi experiment on how the field of librarianship can help in combating fake news. The Journal of Academic Librarianship, 49(1), 102616\nA quasi experiment on how the field of librarianship can help in combating fake news\naquasiexperimentonhowthefieldoflibrarianshipcanhelpincombatingfakenews\nNA\nNA\n96\n\n\nKozyreva_2024\nResnick, P., Alfayez, A., Im, J., & Gilbert, E. (2023). Searching for or reviewing evidence improves crowdworkers’ misinformation judgments and reduces partisan bias. Collective Intelligence, 2(2).\nSearching for or reviewing evidence improves crowdworkers’ misinformation judgments and reduces partisan bias\nsearchingfororreviewingevidenceimprovescrowdworkersmisinformationjudgmentsandreducespartisanbias\nNA\nNA\n97\n\n\nKozyreva_2024\nAli, A., & Qazi, I. A. (2023). Countering misinformation on social media through educational interventions: Evidence from a randomized experiment in Pakistan. Journal of Development Economics, 169,103108.\nCountering misinformation on social media through educational interventions: Evidence from a randomized experiment in Pakistan\ncounteringmisinformationonsocialmediathrougheducationalinterventionsevidencefromarandomizedexperimentinpakistan\nNA\nNA\n98\n\n\nKozyreva_2024\nQian, S., Shen, C., & Zhang, J. (2023). Fighting cheapfakes: using a digital media literacy intervention to motivate reverse search of out-of-context visual misinformation. Journal of Computer-Mediated Communication, 28(1), zmac024.\nFighting cheapfakes: using a digital media literacy intervention to motivate reverse search of out-of-context visual misinformation\nfightingcheapfakesusingadigitalmedialiteracyinterventiontomotivatereversesearchofoutofcontextvisualmisinformation\nNA\nNA\n99\n\n\nKozyreva_2024\nKim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating Fake News on Social Media with Source Ratings: The Effects of User and Expert Reputation Ratings, Journal of Management Information Systems, 36:3, 931-968.\nCombating Fake News on Social Media with Source Ratings: The Effects of User and Expert Reputation Ratings\ncombatingfakenewsonsocialmediawithsourceratingstheeffectsofuserandexpertreputationratings\nNA\nNA\n100\n\n\nKozyreva_2024\nAslett, K., Guess, A. M., Bonneau, R., Nagler, J., & Tucker, J. A. (2022). News credibility labels have limited average effects on news diet quality and fail to reduce misperceptions. Science advances, 8(18), eabl3844.\nNews credibility labels have limited average effects on news diet quality and fail to reduce misperceptions\nnewscredibilitylabelshavelimitedaverageeffectsonnewsdietqualityandfailtoreducemisperceptions\nNA\nNA\n101\n\n\nKozyreva_2024\nShahid, F., Mare, S., & Vashistha, A., (2022). Examining Source Effects on Perceptions of Fake News in Rural India. Proc. ACM Hum.-Comput. Interact. 6, CSCW1, Article 89(April).\nExamining Source Effects on Perceptions of Fake News in Rural India\nexaminingsourceeffectsonperceptionsoffakenewsinruralindia\nNA\nNA\n102\n\n\nKozyreva_2024\nCeladin, T., Capraro, V. ., Pennycook, G., & Rand, D. G. (2023). Displaying News Source Trustworthiness Ratings Reduces Sharing Intentions for False News Posts. Journal of Online Trust and Safety, 1(5).\nDisplaying News Source Trustworthiness Ratings Reduces Sharing Intentions for False News Posts\ndisplayingnewssourcetrustworthinessratingsreducessharingintentionsforfalsenewsposts\nNA\nNA\n103\n\n\nKozyreva_2024\nCookson, D., Jolley, D., Dempsey, R. C., & Povey, R. (2021). A social norms approach intervention to address misperceptions of anti-vaccine conspiracy beliefs amongst UK parents. PLOS ONE, 16(11), Article e0258985.\nA social norms approach intervention to address misperceptions of anti-vaccine conspiracy beliefs amongst UK parents\nasocialnormsapproachinterventiontoaddressmisperceptionsofantivaccineconspiracybeliefsamongstukparents\nNA\nNA\n104\n\n\nKozyreva_2024\nAndı, S. & Akesson, J. (2021). Nudging away false news: Evidence from a social norms experiment. Digital Journalism, 9(1), 106–125.\nNudging away false news: Evidence from a social norms experiment\nnudgingawayfalsenewsevidencefromasocialnormsexperiment\nNA\nNA\n105\n\n\nKozyreva_2024\nEcker, U. K. H., Sanderson, J. A., McIlhiney, P., Rowsell, J. J., Quekett, H. L., Brown, G. D. A., & Lewandowsky, S. (2022). Combining refutations and social norms increases belief change. Quarterly Journal of Experimental Psychology.\nCombining refutations and social norms increases belief change\ncombiningrefutationsandsocialnormsincreasesbeliefchange\nNA\nNA\n106\n\n\nKozyreva_2024\nPrike, T., Butler, L., & Ecker, U. K. H. (2023). Source-credibility information and social norms improve truth discernment and reduce engagement with misinformation online. Scientific Reports, 14, 6900\nSource-credibility information and social norms improve truth discernment and reduce engagement with misinformation online\nsourcecredibilityinformationandsocialnormsimprovetruthdiscernmentandreduceengagementwithmisinformationonline\nNA\nNA\n107\n\n\nKozyreva_2024\nEcker, U. K. H., Lewandowsky, S., & Tang, D. T. W. (2010). Explicit warnings reduce but do not eliminate the continued influence of misinformation. Memory & Cognition, 38(8), 1087–1100.\nExplicit warnings reduce but do not eliminate the continued influence of misinformation\nexplicitwarningsreducebutdonoteliminatethecontinuedinfluenceofmisinformation\nNA\nNA\n108\n\n\nKozyreva_2024\nMena, P. (2019). Cleaning Up Social Media: The Effect of Warning Labels on Likelihood of Sharing False News on Facebook. Policy & Internet, 12(2): 165-183.\nCleaning Up Social Media: The Effect of Warning Labels on Likelihood of Sharing False News on Facebook\ncleaningupsocialmediatheeffectofwarninglabelsonlikelihoodofsharingfalsenewsonfacebook\nNA\nNA\n109\n\n\nKozyreva_2024\nPennycook, G., Bear, A., Collins, E. T., & Rand, D. G. (2020). The implied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings. Management Science, 66(11), 4944–4957.\nThe implied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings\ntheimpliedtrutheffectattachingwarningstoasubsetoffakenewsheadlinesincreasesperceivedaccuracyofheadlineswithoutwarnings\nNA\nNA\n110\n\n\nKozyreva_2024\nEcker, U. K. H., O’Reilly, Z., Reid, J. S., & Chang, E. P. (2020). The effectiveness of short-format refutational fact-checks. The British Journal of Psychology, 111, 36-54.\nThe effectiveness of short-format refutational fact-checks\ntheeffectivenessofshortformatrefutationalfactchecks\nNA\nNA\n111\n\n\nKozyreva_2024\nGrady, R. H., Ditto, P. H., & Loftus, E. F. (2021). Nevertheless, partisanship persisted: Fake news warnings help briefly, but bias returns with time. Cognitive Research: Principles and Implications, 6(1), Article 52.\nNevertheless, partisanship persisted: Fake news warnings help briefly, but bias returns with time\nneverthelesspartisanshippersistedfakenewswarningshelpbrieflybutbiasreturnswithtime\nNA\nNA\n112\n\n\nKozyreva_2024\nLee, J., Kim, J. W., & Yun Lee, H. (2023). Unlocking Conspiracy Belief Systems: How Fact-Checking Label on Twitter Counters Conspiratorial MMR Vaccine Misinformation. Health\n\n\n\n\n\n\n\nCommunication, 38(9), 1780-1792.\nUnlocking Conspiracy Belief Systems: How Fact-Checking Label on Twitter Counters Conspiratorial MMR Vaccine Misinformation\nunlockingconspiracybeliefsystemshowfactcheckinglabelontwittercountersconspiratorialmmrvaccinemisinformation\nNA\nNA\n113\n\n\n\nKozyreva_2024\nKoch, T. K., Frischlich, L., & Lermer, E. (2023). Effects of fact‐checking warning labels and social endorsement cues on climate change fake news credibility and engagement on social media. Journal of Applied Social Psychology, 53(6): 495-507.\nEffects of fact‐checking warning labels and social endorsement cues on climate change fake news credibility and engagement on social media\neffectsoffactcheckingwarninglabelsandsocialendorsementcuesonclimatechangefakenewscredibilityandengagementonsocialmedia\nNA\nNA\n114\n\n\nKozyreva_2024\nMoon, W.-K., Chung, M., & Jones-Jang, S. Mo. (2022). How Can We Fight Partisan Biases in the COVID-19 Pandemic? AI Source Labels on Fact-checking Messages Reduce Motivated Reasoning.\n\n\n\n\n\n\n\nMass Communication and Society, 26(4): 646-670.\nHow Can We Fight Partisan Biases in the COVID-19 Pandemic? AI Source Labels on Fact-checking Messages Reduce Motivated Reasoning\nhowcanwefightpartisanbiasesinthecovidpandemicaisourcelabelsonfactcheckingmessagesreducemotivatedreasoning\nNA\nNA\n115\n\n\n\nSun_2025\nDisentangling Item and Testing Effects in Inoculation Research on Online Misinformation: Solomon Revisited\nDisentangling Item and Testing Effects in Inoculation Research on Online Misinformation: Solomon Revisited\ndisentanglingitemandtestingeffectsininoculationresearchononlinemisinformationsolomonrevisited\nNA\nNA\n116\n\n\nSun_2025\nThe Spot the Troll Quiz game increases accuracy in discerning between real and i/uthentic social media accounts\nThe Spot the Troll Quiz game increases accuracy in discerning between real and i/uthentic social media accounts\nthespotthetrollquizgameincreasesaccuracyindiscerningbetweenrealandiuthenticsocialmediaaccounts\nNA\nNA\n117\n\n\nSun_2025\nPsychological inoculation strategies to fight climate disinformation across 12 countries\nPsychological inoculation strategies to fight climate disinformation across 12 countries\npsychologicalinoculationstrategiestofightclimatedisinformationacrosscountries\nNA\nNA\n118\n\n\nSun_2025\nMixed News about the Bad News Game\nMixed News about the Bad News Game\nmixednewsaboutthebadnewsgame\nNA\nNA\n119\n\n\nSun_2025\nA Health Media Literacy Intervention Increases Skepticism of Both I/ccurate and Accurate Cancer News Among U.S. Adults\nA Health Media Literacy Intervention Increases Skepticism of Both I/ccurate and Accurate Cancer News Among U.S. Adults\nahealthmedialiteracyinterventionincreasesskepticismofbothiccurateandaccuratecancernewsamongusadults\nNA\nNA\n120\n\n\nSun_2025\nProminent misinformation interventions reduce misperceptions but increase scepticism\nProminent misinformation interventions reduce misperceptions but increase scepticism\nprominentmisinformationinterventionsreducemisperceptionsbutincreasescepticism\nNA\nNA\n121\n\n\nSun_2025\nStrategies to combat misinformation: Enduring effects of a 15-minute online intervention on critical-thinking adolescents\nStrategies to combat misinformation: Enduring effects of a 15-minute online intervention on critical-thinking adolescents\nstrategiestocombatmisinformationenduringeffectsofaminuteonlineinterventiononcriticalthinkingadolescents\nNA\nNA\n122\n\n\nSun_2025\nFeedback exercises boost discernment of misinformation for gamified inoculation interventions\nFeedback exercises boost discernment of misinformation for gamified inoculation interventions\nfeedbackexercisesboostdiscernmentofmisinformationforgamifiedinoculationinterventions\nNA\nNA\n123\n\n\nSun_2025\nCritical thinking and misinformation vulnerability: experimental evidence from Colombia\nCritical thinking and misinformation vulnerability: experimental evidence from Colombia\ncriticalthinkingandmisinformationvulnerabilityexperimentalevidencefromcolombia\nNA\nNA\n124\n\n\nSun_2025\nIs the COVID-19 bad news game good news? Testing whether creating and dissemi/ting fake news about vaccines in a computer game reduces people’s belief in anti-vaccine arguments\nIs the COVID-19 bad news game good news? Testing whether creating and dissemi/ting fake news about vaccines in a computer game reduces people’s belief in anti-vaccine arguments\nisthecovidbadnewsgamegoodnewstestingwhethercreatinganddissemitingfakenewsaboutvaccinesinacomputergamereducespeoplesbeliefinantivaccinearguments\nNA\nNA\n125",
    "crumbs": [
      "Literature Search",
      "Identify studies"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html",
    "href": "preregistration/preregistration.html",
    "title": "Pre-data collection registration",
    "section": "",
    "text": "Code\n# load plot theme\nsource(\"../R/plot_theme.R\") \n\n# load other functions\nsource(\"../R/custom_functions.R\")",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#datasampling",
    "href": "preregistration/preregistration.html#datasampling",
    "title": "Pre-data collection registration",
    "section": "Data/Sampling",
    "text": "Data/Sampling\n\nSystematic Literature review\nWe will base ourselves on the systematic literature review of a recent meta-analysis on people’s accuracy judgments of true and false news (Pfänder and Altay 2024). Of all the studies extracted from this review, we will reduce our sample to those that tested an intervention to augment news discernment.\nSince the field is rapidly growing, we will also try to add papers that have been published since the systematic review. We will code for all papers whether they were part of the original systematic review of Pfänder and Altay (2024) or identified afterwards, so that the selection process is transparent. In a second pre-registration–after data collection but before analysis–we will publish a full list of studies, detailing whether they were part of the systematic review or added afterwards by us because we came across them.\nWe have already stored individual level data for several studies used in Pfänder and Altay (2024). We have yet to collect data for the remaining studies. For the studies for which we already have stored individual-level data, we have not yet cleaned it according to the requirements of the study proposed here. We have not performed any of the here suggested analysis on the collected data. We have simulated data to be clear on the data structure our analysis requires and to test how well our model performs in recovering parameters. All analyses presented in this pre-registration use this simulated data.\n\n\nSelecting Interventions\nSometimes, within the same experiment, researchers test different interventions, by comparing it to a single control group. For a meta-analysis, this is a problem, sometimes referred to as double-counting. Dependencies between effect sizes based on the same comparison group cannot be accounted for by a meta-analytic model (Harrer et al. 2021). As a consequence, we will have to make a selection of for experiments that test several interventions at the same time: Following options recommended in the cochrane manual we use the following rule of thumb: If the interventions are conceptually similar enough, pool the interventions together. If they are too dissimilar, pick the most pertinent one.\nIt is hard to predict in advance how relevant these decisions will be. However, we will make them on a purely theoretical basis: Once we have collected our data, we will make an overview of all interventions, and make a decision in case of multiple intervention arms within the same experiment. We will preregister these decisions as part of the second pre-registration. Only then will we run the analysis.\nWe have tried to mimic the situation of multiple intervention groups but only one control group in our simulated data. Some experiments have several interventions, and they are sometimes conceptually similar, sometimes different. Table 1 shows the experimental structure of the first two simulated papers.\n\n\nCode\ndata %&gt;% \n  filter(paper_id %in% c(1:2)) %&gt;% \n  distinct(paper_id, experiment_id, intervention_id, intervention_type) %&gt;% \n  kable(caption = \"Experimental structure of the first two simulated papers\",\n        booktabs = TRUE) %&gt;% \n    kable_styling(font_size = 10) \n\n\n\n\nTable 1: Experimental structure of the first two simulated papers\n\n\n\n\n\n\npaper_id\nexperiment_id\nintervention_id\nintervention_type\n\n\n\n\n1\n1\nNA\ncontrol\n\n\n1\n1\n1\npriming\n\n\n1\n2\nNA\ncontrol\n\n\n1\n2\n1\nwarning labels\n\n\n2\n1\nNA\ncontrol\n\n\n2\n1\n1\npriming\n\n\n2\n1\n2\nwarning labels\n\n\n2\n2\nNA\ncontrol\n\n\n2\n2\n1\nwarning labels\n\n\n\n\n\n\n\n\n\n\nIn Paper 1, there are two experiments (experiment_id = ‘1’ and experiment_id = ‘2’). Both experiments only have two arms, respectively: the control group and an intervention group (priming for Experiment 1; warning labels for Experiment 2). In these cases, since there is only one intervention, there is no decision to make as to which intervention to keep. By contrast, the first experiment of Paper 2 has three arms, among with two different intervention types (‘priming’ and ‘warning labels’). In such a case, we would need to make a decision on which intervention type to keep. This decision will probably depend on which intervention type occurred how many times etc.\nIn our data, we will first detect all experiments with multiple interventions (Table 2).\n\n\nCode\nconflict &lt;- data %&gt;% \n  distinct(paper_id, experiment_id, unique_experiment_id, intervention_id, intervention_type) %&gt;% \n  group_by(unique_experiment_id) %&gt;% \n  # substact 1 because of the control condition\n  summarize(n_different_intervention_types = n_distinct(intervention_type) - 1, \n            # display different intervention types\n            types = paste(unique(intervention_type), collapse = \", \")\n            ) %&gt;% \n  # label all experiments with more than one intervention type\n  mutate(conflict = ifelse(n_different_intervention_types &gt; 1, TRUE, FALSE)) %&gt;% \n  filter(conflict == TRUE)\n\nconflict %&gt;% \n  kable(caption = \"Studies with mutliple intervention arms in the simulated data\", \n        booktabs = TRUE) %&gt;% \n    kable_styling(font_size = 10) \n\n\n\n\nTable 2: Studies with mutliple intervention arms in the simulated data\n\n\n\n\n\n\nunique_experiment_id\nn_different_intervention_types\ntypes\nconflict\n\n\n\n\n10_2\n2\ncontrol, priming, literacy tips\nTRUE\n\n\n10_3\n2\ncontrol, priming, warning labels\nTRUE\n\n\n2_1\n2\ncontrol, priming, warning labels\nTRUE\n\n\n3_1\n2\ncontrol, warning labels, literacy tips\nTRUE\n\n\n4_1\n2\ncontrol, literacy tips, priming\nTRUE\n\n\n5_1\n2\ncontrol, warning labels, priming\nTRUE\n\n\n6_1\n2\ncontrol, literacy tips, priming\nTRUE\n\n\n6_2\n2\ncontrol, warning labels, literacy tips\nTRUE\n\n\n7_1\n2\ncontrol, literacy tips, priming\nTRUE\n\n\n7_3\n2\ncontrol, priming, literacy tips\nTRUE\n\n\n8_1\n2\ncontrol, warning labels, priming\nTRUE\n\n\n9_1\n2\ncontrol, warning labels, priming\nTRUE\n\n\n9_2\n2\ncontrol, literacy tips, priming\nTRUE\n\n\n\n\n\n\n\n\n\n\nWe will then have to make specific exclusion decisions for each experiment (Table 3).\n\n\nCode\nconflict &lt;- conflict %&gt;% \n  # list all intervention types to be excluded for a given experiment\n  mutate(exclude =   case_when(unique_experiment_id == \"1_1\" ~ \"priming, warning labels\",\n                               unique_experiment_id == \"10_1\" ~ \"priming\",\n                               unique_experiment_id == \"10_3\" ~ \"warning labels\",\n                               # continue this list for all studies (we don't do this here because lazy)\n                               TRUE ~ NA_character_\n  )\n  ) %&gt;% \n  select(unique_experiment_id, exclude)\n\nconflict %&gt;% \n  kable(caption = \"Example of exclusion decisions for some of the experiments with multiple intervention arms\", \n        booktabs = TRUE) %&gt;% \n    kable_styling(font_size = 10) \n\n\n\n\nTable 3: Example of exclusion decisions for some of the experiments with multiple intervention arms\n\n\n\n\n\n\nunique_experiment_id\nexclude\n\n\n\n\n10_2\nNA\n\n\n10_3\nwarning labels\n\n\n2_1\nNA\n\n\n3_1\nNA\n\n\n4_1\nNA\n\n\n5_1\nNA\n\n\n6_1\nNA\n\n\n6_2\nNA\n\n\n7_1\nNA\n\n\n7_3\nNA\n\n\n8_1\nNA\n\n\n9_1\nNA\n\n\n9_2\nNA\n\n\n\n\n\n\n\n\n\n\nWe then filter our data to only keep the intervention conditions we decided upon (Table 4).\n\n\n\nTable 4\n\n\n\nCode\ndata &lt;- data %&gt;% \n  left_join(conflict) %&gt;% \n  # Use str_detect to check if intervention_type is found in exclude string\n  filter(is.na(exclude) | !str_detect(exclude, intervention_type))\n\n# check\n# data %&gt;%\n#   distinct(paper_id, experiment_id, unique_experiment_id, intervention_id, intervention_type, condition) %&gt;%\n#   filter(unique_experiment_id %in% c(\"10_1\", \"2_1\", \"5_1\"))\n\n\n\n\nOnce all decisions regarding the intervention selections have been made, we can rely our ‘condition’ variable for all models below, which takes only the values of either ‘control’ or ‘intervention’.",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#analysis-plan",
    "href": "preregistration/preregistration.html#analysis-plan",
    "title": "Pre-data collection registration",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nOutcomes\nWe want to measure the effects of misinformation interventions on two outcomes of Signal Detection Theory (SDT): \\(d'\\) (“d prime”, sensitivity), and \\(c\\) (response bias). Table 5 shows how instances of news ratings map onto SDT terminology.\n\n\nCode\n# Data\ntable_data &lt;- tibble(\n  Stimulus = c(\"True news (target)\",\"False news (distractor)\"),\n  Accurate = c(\"Hit\", \"False alarm\"),\n  `Not Accurate` = c(\"Miss\", \"Correct rejection\")\n)\n\n# Set Stimulus as row names\n# rownames(table_data) &lt;- table_data$Stimulus\n# table_data$Stimulus &lt;- NULL\n\n# Create table using kable\nkable(table_data, \n      caption = \"Accuracy ratings in Signal Detection Theory terms\", \n              booktabs = TRUE) %&gt;%\n  kable_paper(full_width = FALSE) %&gt;%\n  add_header_above(c(\" \", \"Response\" = 2))\n\n\n\n\nTable 5: Accuracy ratings in Signal Detection Theory terms\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\nStimulus\nAccurate\nNot Accurate\n\n\n\n\nTrue news (target)\nHit\nMiss\n\n\nFalse news (distractor)\nFalse alarm\nCorrect rejection\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# make SDT variables\nsdt_data &lt;- data %&gt;% \n  # code ratings in terms of SDT language\n  mutate(sdt_outcome = case_when(accuracy == 1 & veracity == \"true\" ~ \"hit\", \n                                 accuracy == 1 & veracity == \"fake\" ~ \"false_alarm\",\n                                 accuracy == 0 & veracity == \"true\" ~ \"miss\",\n                                 accuracy == 0 & veracity == \"fake\" ~ \"correct_rejection\",\n                                 .default =  NA)\n         )\n\n\nThe sensitivity, \\(d’\\), measures people’s capacity to discriminate between true and false news. It is defined as the difference of the standardized hit and false alarm rates\n\\[\nd' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)\n\\]\nwhere \\(HR\\) refers to hit rate or the proportion of true news trials that participants classified–correctly–as “accurate” (\\(HR = \\frac{N_{Hits}}{N_{Hits} + N_{Misses}}\\)), \\(FAR\\) refers to false alarm rate or the proportion of false news trials that participants classified–incorrectly–as “accurate” (\\(HR = \\frac{N_{False Alarms}}{N_{FalseAlarms} + N_{Correct Rejections}}\\)). \\(\\Phi\\) is the cumulative normal density function, and is used to convert z scores into probabilities. Its inverse, \\(\\Phi^{-1}\\), converts a proportion (such as a hit rate or false alarm rate) into a z score. Below, we refer to standardized hit and false alarm rates as zHR and zFAR, respectively. Due to this transformation, a proportion of .5 is converted to a z score of 0 (reflecting responses at chance). Proportions greater than .5 produce positive z scores, and proportions smaller than .5 negative ones. A positive \\(d'\\) score indicates that people rate true news as more accurate than false news.\nThe response bias, \\(c\\) can be conceived of as an overall tendency to rate items as accurate. It is defined as\n\\[\nc = -\\frac{1}{2}(\\text{zHR} + \\text{zFAR})\n\\]\nNote that \\(c\\) = 0 when \\(zFAR = -zHR\\). Because \\(-zHR = z(1-HR)\\) is the (z-transformed) share of misses (true news rated as “not accurate”), \\(c\\) = 0 when the false alarm rate is equal to the rate of misses (Macmillan & Creelman (2004). In other words, if people–mistakenly–rate true news as not accurate (the rate of misses) as much as they–mistakenly–rate false false news as accurate (the rate of false alarms), then there is no response bias. A positive \\(c\\) score occurs when the miss rate (rating true news news as false) is larger than the false alarm rate (rating false news as true). In other words, a positive \\(c\\) score indicates that people were more skeptical towards true news than they were gullible towards false news. We refer to this as an overall tendency towards skepticism.\nOur outcomes of interest are the intervention effects on \\(d'\\) (“d prime”, sensitivity) and \\(c\\) (response bias). Since these outcomes are about the difference between the control and the treatment group, we call them “Delta”. Specifically, they are defined as:\n\\[\n\\Delta d' = d'_{treatment} - d'_{control}\n\\]\nand\n\\[\n\\Delta c = c_{treatment} - c_{control}.\n\\]\nA positive \\(\\Delta d'\\) score indicates that the intervention increased participants’ ability to discriminate between true and false news compared to the control group. A positive \\(\\Delta c\\) score indicates that the intervention led to a greater tendency towards skepticism, meaning participants were more likely to rate items as “not accurate” regardless of veracity, compared to the control group.\n\n\nVariables\nTable 6 contains a list of variables we will collect. We might not be able to collect all variables for all studies. For political concordance, e.g., this will certainly not be the case.\n\n\nCode\n# Create the codebook data frame\ncodebook &lt;- data.frame(\n  Variable_Name = c(\n    \"paper_id\", \"experiment_id\", \"subject_id\", \"country\", \"year\", \n    \"veracity\", \"condition\", \"intervention_label\", \"intervention_description\", \n    \"intervention_selection\",\n    \"accuracy_raw\", \"scale\", \"originally_identified_treatment_effect\", \n    \"concordance\", \"age\", \"age_range\", \"identified_via\", \"id\", \n    \"unique_experiment_id\", \"accuracy\"\n  ),\n  Description = c(\n    \"Identifier for each paper\",\n    \"Identifier for each experiment within a paper\",\n    \"Identifier of individual participants within an experiment\",\n    \"The country of the sample\",\n    \"Ideally year of data collection, otherwise year of publication\",\n    \"Identifying false and true news items\",\n    \"Treatment vs. control\",\n    \"A label for what the intervention consisted of\",\n    \"A detailed description of the intervention\",\n    \"If multiple interventions tested within a single experiment (and related to a single control group), reasoning as to which intervention to select\",\n    \"Participants' accuracy ratings on the scale used in the original study\",\n    \"The scale used in the original study\",\n    \"Whether the authors identified a significant treatment effect (`FALSE` if no, `TRUE` if yes)\",\n    \"Political concordance of news items (concordant or discordant)\",\n    \"Participant age. In some cases, participant age will not be exact, but within a binned category. In this case, we will take the mid-point of this category for the age variable\",\n    \"Binned age, if only this is provided by the study.\",\n    \"Indicates if a paper was identified by the systematic review or added after\",\n    \"Unique participant ID (merged `paper_id`, `experiment_id`, `subject_id`)\",\n    \"Unique experiment ID (merged `paper_id` and `experiment_id`)\",\n    \"Binary version of `accuracy_raw`; unchanged if originally binary\"\n  )\n)\n\n# Generate the styled table with kableExtra\nkable(codebook, \n      caption = \"Codebook for variables to collect\",\n      col.names = c(\"Variable Name\", \"Description\"),\n      booktabs = TRUE,\n      longtable = TRUE,) %&gt;%\n  kable_styling(latex_options = \"repeat_header\",\n                font_size = 10) %&gt;% \n  column_spec(1, bold = TRUE) %&gt;%  # Bold the first column\n  column_spec(2, width = \"25em\") %&gt;%  # Set width for the description column\n  row_spec(0, bold = TRUE)  # Bold the header row\n\n\n\n\nTable 6: Codebook for variables to collect\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\npaper_id\nIdentifier for each paper\n\n\nexperiment_id\nIdentifier for each experiment within a paper\n\n\nsubject_id\nIdentifier of individual participants within an experiment\n\n\ncountry\nThe country of the sample\n\n\nyear\nIdeally year of data collection, otherwise year of publication\n\n\nveracity\nIdentifying false and true news items\n\n\ncondition\nTreatment vs. control\n\n\nintervention_label\nA label for what the intervention consisted of\n\n\nintervention_description\nA detailed description of the intervention\n\n\nintervention_selection\nIf multiple interventions tested within a single experiment (and related to a single control group), reasoning as to which intervention to select\n\n\naccuracy_raw\nParticipants' accuracy ratings on the scale used in the original study\n\n\nscale\nThe scale used in the original study\n\n\noriginally_identified_treatment_effect\nWhether the authors identified a significant treatment effect (`FALSE` if no, `TRUE` if yes)\n\n\nconcordance\nPolitical concordance of news items (concordant or discordant)\n\n\nage\nParticipant age. In some cases, participant age will not be exact, but within a binned category. In this case, we will take the mid-point of this category for the age variable\n\n\nage_range\nBinned age, if only this is provided by the study.\n\n\nidentified_via\nIndicates if a paper was identified by the systematic review or added after\n\n\nid\nUnique participant ID (merged `paper_id`, `experiment_id`, `subject_id`)\n\n\nunique_experiment_id\nUnique experiment ID (merged `paper_id` and `experiment_id`)\n\n\naccuracy\nBinary version of `accuracy_raw`; unchanged if originally binary\n\n\n\n\n\n\n\n\n\n\nBuilding a comparable accuracy measure that we can analyze in the SDT framework outlined here, requires collapsing scales. The studies that we will look at use a variety of scales. Some use a binary scale, which is the one that is the most straightforward to use in a SDT framework (signal vs. no signal) and thus also the one we have simulated. However, a lot of studies use Likert-type scales. For these studies, we will collapse the original scores into a dichotomized version with answers of either ‘not accurate’ (0) or ‘accurate’ (1). For example, a study might have used a 4-point scale going from 1, not accurate at all, to 4, completely accurate. In this case, we will code responses of 1 and 2 as not accurate (0) and 3 and 4 as accurate (1). For scales, with a mid-point (example 3 on a 5-point scale), we will code midpoint answers as ‘NA’, meaning they will be excluded from the analysis.\n\n\nMain Analysis\nWe will run a participant-level meta analysis using a two-stage approach: First, for each experiment, we calculate a generalized linear mixed model (glmm) that yields SDT outcomes. The resulting estimates are our effect sizes. Second, we run a meta analysis on these effect sizes. In Appendix Section 6.1 to this registration, we explain step by step how to go from a by-hand SDT analysis to a glmm SDT analysis. In the following, we will outline the analysis steps we will perform.\n\nStage 1: experiment-level analysis\nFor each experiment, we run a separate participant-level SDT analysis. Because participants provide several ratings, we use mixed-models with random participant effects that account for the resulting dependency in the data points. We use random effects both for the intercept and the effect of veracity. We do not use random effects for condition or the interaction of condition and veracity, as condition is typically manipulated between participants. We can obtain SDT outcomes directly from a generalized linear mixed model (glmm) when using a probit link function (see Appendix Section 6.1). Table 7 shows the model output for our simulated Experiment 1, from paper 1.\n\n\nCode\n# calculate model function\ncalculate_model &lt;- function(data) {\n  \n  time &lt;- system.time({\n    model &lt;- glmer(accuracy ~ veracity_numeric + condition_numeric + \n                     veracity_numeric*condition_numeric +\n                     (1 + veracity_numeric | unique_subject_id),\n                   data = data, \n                   family = binomial(link = \"probit\")\n    )\n  })\n  \n  time &lt;- round(time[3]/60, digits = 2)\n  \n  # get a tidy version\n  model &lt;- tidy(model, conf.int = TRUE) %&gt;% \n    # add time\n    mutate(time_minutes = time)\n  \n  return(model)\n  \n}\n# test\n# mixed_model &lt;- calculate_model(data %&gt;% filter(unique_experiment_id == \"1_1\"))\n# mixed_model\n\n\nWe calculate one model per experiment and store the results in a common data frame.\n\n\nCode\n# Running this model takes some time. We therefor stored the results in a data frame that we can reload. \nfilename &lt;- \"../data/simulations/models_by_experiment.csv\" \n\nrun_loop &lt;- function(data, filename){\n  \n  # only execute the following if the file does NOT exist\n  if (!file.exists(filename)) {\n    \n    # make a vector with all unique experiment ids\n    experiments &lt;- data %&gt;% \n      distinct(unique_experiment_id) %&gt;% \n      # slice(1:3) %&gt;% # to test loop\n      pull()\n    \n    time &lt;- system.time({\n      \n      # run one model per experiment and store the results in a common data frame\n      results &lt;- experiments %&gt;%\n        map_dfr(function(x) {\n          \n          # restrict data to only the respective experiment\n          experiment &lt;- data %&gt;% filter(unique_experiment_id == x)\n          \n          # extract paper id\n          paper_id &lt;- unique(experiment$paper_id)\n          \n          # To keep track of progress\n          print(paste(\"calculating model for experiment \", x))\n          \n          model_experiment &lt;- calculate_model(experiment) %&gt;%\n            mutate(unique_experiment_id = x,\n                   paper_id = paper_id)\n          \n          return(model_experiment)\n        })\n    })\n    \n    write_csv(results, filename)\n    \n    print(paste(\"Elapsed time: \", round(time[3]/60, digits = 2), \" minutes\"))\n  }\n  \n}\n\n# execute function\nrun_loop(data, filename)\n\n# read saved model results\nmodel_results &lt;- read_csv(filename)\n\n\nWe code our binary independent variables, veracity and condition, using deviation coding (i.e. for veracity, false = -0.5, true = 0.5; for condition, control = -0.5, treatment = 0.5). Using deviation coding, the main effect model output terms translate to SDT outcomes as follows:\n\n(Intercept) = average -\\(c\\), pooled across all conditions\nveracity_numeric = average \\(d'\\), pooled across all conditions\ncondition_numeric = -\\(\\Delta c\\), i.e. the change in -response bias between control and treatment\nveracity_numeric:condition_numeric = \\(\\Delta d'\\), i.e. the change in sensitivity between control and treatment\n\n\n\nCode\n# We clean up the model output by naming our estimates of interest in terms of SDT outcomes, and reversing the coefficients for response bias c, and delta response bias c.\n\nmodel_results &lt;- model_results %&gt;% \n  filter(effect == \"fixed\") %&gt;% \n  mutate(\n    # make sdt outcomes\n    SDT_term = case_when(\n    term == \"(Intercept)\" ~ \"average c\",\n    term == \"veracity_numeric\" ~ \"average d'\",\n    term == \"condition_numeric\" ~ \"delta c\",\n    term == \"veracity_numeric:condition_numeric\" ~ \"delta d'\",\n    TRUE ~ \"Other\"\n  ), \n  # reverse c and delta c estimates\n  SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\", \n                        -1*estimate, estimate),\n    sampling_variance = std.error^2\n  ) \n\n\n\n\nCode\nmodel_results %&gt;% \n  filter(unique_experiment_id == \"1_1\") %&gt;% \n  select(-starts_with(\"conf\")) %&gt;% \n  rounded_numbers() %&gt;% \n  select(-c(effect, group)) %&gt;% \n  kable(\n    caption = \"Results of a generalize linear mixed model (glmm)\",\n    booktabs = TRUE) %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 7: Results of a generalize linear mixed model (glmm)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ntime_minutes\nunique_experiment_id\npaper_id\nSDT_term\nSDT_estimate\nsampling_variance\n\n\n\n\n(Intercept)\n-0.514\n0.032\n-16.175\n0.000\n0.01\n1_1\n1\naverage c\n0.514\n0.001\n\n\nveracity_numeric\n0.786\n0.062\n12.711\n0.000\n0.01\n1_1\n1\naverage d'\n0.786\n0.004\n\n\ncondition_numeric\n-0.119\n0.063\n-1.892\n0.059\n0.01\n1_1\n1\ndelta c\n0.119\n0.004\n\n\nveracity_numeric:condition_numeric\n-0.285\n0.123\n-2.312\n0.021\n0.01\n1_1\n1\ndelta d'\n-0.285\n0.015\n\n\n\n\n\n\n\n\n\n\nAs shown in Figure 1, we can then plot the distributions of our outcome estimates of interest, across experiments.\n\n\nCode\n# make plot\nggplot(model_results, aes(x = estimate, fill = SDT_term)) +\n  geom_density(alpha = 0.5, adjust = 1.5) +\n  # colors \n  scale_fill_viridis_d(option = \"inferno\", begin = 0.1, end = 0.9) +\n  # labels and scales\n  labs(x = \"z-Score\", y = \"Density\") +\n  guides(fill = FALSE, color = FALSE) +\n  plot_theme +\n  theme(strip.text = element_text(size = 14)) +\n  facet_wrap(~SDT_term)\n\n\n\n\n\n\n\n\nFigure 1: Distributions of Signal Detection Theory outcomes across experiments. Note that these distributions are purely descriptive - effect sizes are not weighted by sample size of the respective experiment, as they are in the meta-analysis.\n\n\n\n\n\n\n\nStage 2: Meta-analysis\nNext, we run a meta analysis on these effect size estimates at the experiment-level. In our models for the meta analysis, each effect size is weighted by the inverse of its standard error, thereby giving more weight to experiments with larger sample sizes. We will use random effects models, which assume that there is not only one true effect size but a distribution of true effect sizes. We will use a multi-level meta-analytic model, with random effects at the publication and the experiment level. This approach allows us to account for the hierarchical structure of our data, in which (at least some) papers (level three) contribute several effect sizes from different experiments (level two)1. However, the multi-level models do not account for dependencies in sampling error. When one paper contributes several effect sizes, one might expect their respective sampling errors to be correlated. To account for dependency in sampling errors, we compute cluster-robust standard errors, confidence intervals, and statistical tests for all meta-analytic estimates. For our simulated data, Table 8 shows the results of the meta-analytic models and Figure 2 provides an overview of effect sizes for each experiment as a forest plot.\n\n\nCode\n# Function to calculate meta models\ncalculate_models &lt;- function(data, yi, vi, robust = TRUE) {\n  \n  # provide metafor compatible names\n  metafor_data &lt;- data %&gt;% \n    rename(yi = {{yi}}, \n           vi = {{vi}})\n  \n  # Multilevel random effect model for accuracy\n  model &lt;-  metafor::rma.mv(yi, vi, random = ~ 1 | paper_id / unique_experiment_id, \n                            data = metafor_data)\n  \n  return(model)\n  \n  if(robust == TRUE) {\n    # with robust standard errors clustered at the paper level \n    robust_model &lt;- robust(model, cluster = data$paper_id)\n    \n    return(robust_model)\n  }\n}\n\n\n\n\nCode\n# model for delta dprime\ndelta_dprime &lt;- calculate_models(data = model_results %&gt;% \n                                   filter(SDT_term == \"delta d'\"), yi = SDT_estimate, \n                                 vi = sampling_variance, robust = TRUE)\n\n# model for delta c\ndelta_c &lt;- calculate_models(data = model_results %&gt;% \n                                   filter(SDT_term == \"delta c\"), yi = SDT_estimate, \n                                 vi = sampling_variance, robust = TRUE)\n\n\nmodelsummary::modelsummary(list(\"Delta d'\" = delta_dprime, \n                                \"Delta c\" = delta_c\n                                ), \n                           title = \"Results of Meta-analysis\",\n                           stars = TRUE, \n                           output = \"kableExtra\"\n                           )\n\n\n\n\nTable 8: Results of Meta-analysis\n\n\n\n\n\n\n\nDelta d'\nDelta c\n\n\n\n\noverall\n0.100*\n0.078*\n\n\n\n(0.043)\n(0.032)\n\n\nNum.Obs.\n21\n21\n\n\nAIC\n−8.1\n−20.4\n\n\nBIC\n−4.9\n−17.3\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Set up a 1x2 layout for the two plots\npar(mfrow = c(1, 2))\n\n# Calculate weights (e.g., inverse of standard error)\nmodel_results &lt;- model_results %&gt;%\n  mutate(weight = 1 / sqrt(sampling_variance))\n\n# Create plot for d'\nforest.rma(delta_dprime,\n           xlim = c(-1, 2),          # Adjust horizontal plot region limits\n           at = c(-1, -0.5, 0, 1, 2), \n           order = \"obs\",            # Order by size of yi\n           slab = unique_experiment_id, \n           annotate = TRUE,          # Study labels and annotations\n           efac = c(0, 1),           # Remove vertical bars at end of CIs\n           pch = 15,                 # Change point symbol to filled squares\n           col = \"gray40\",           # Change color of points/CIs\n           cex.lab = 0.8, \n           cex.axis = 0.8,           # Increase size of x-axis title/labels\n           lty = c(\"solid\", \"dotted\", \"blank\"),  # Remove horizontal line at top of plot\n           mlab = \"BLASt\", \n           ylim = c(-2, n_distinct(model_results$unique_experiment_id)), \n           addfit = FALSE, \n           xlab = \"Delta d'\") \naddpoly(delta_dprime, mlab = \" \", cex = 1, row = -2) \nabline(h = 0)\n\n# Create plot for c\nforest.rma(delta_c,\n           xlim = c(-1, 2),          # Adjust horizontal plot region limits\n           at = c(-1, -0.5, 0, 1, 2), \n           order = \"obs\",            # Order by size of yi\n           slab = unique_experiment_id, \n           annotate = TRUE,          # Study labels and annotations\n           efac = c(0, 1),           # Remove vertical bars at end of CIs\n           pch = 15,                 # Change point symbol to filled squares\n           col = \"gray40\",           # Change color of points/CIs\n           cex.lab = 0.8, \n           cex.axis = 0.8,           # Increase size of x-axis title/labels\n           lty = c(\"solid\", \"dotted\", \"blank\"),  # Remove horizontal line at top of plot\n           mlab = \"BLASt\", \n           ylim = c(-2, n_distinct(model_results$unique_experiment_id)), \n           addfit = FALSE, \n           xlab = \"Delta c\") \naddpoly(delta_c, mlab = \" \", cex = 1, row = -2) \nabline(h = 0)\n\n# Reset the layout to the default (1x1)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nFigure 2: Forest plots for delta d’ and delta c. The figure displays all effect sizes for both outcomes. Effects are weighed by their sample size. Effect sizes are z-values. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the sample level.\n\n\n\n\n\n\n\n\nModerator analysis\nWe distinguish between two broad kinds of moderator variables: those that vary within-experiments (e.g. political concordance) and those that vary between experiments (e.g. intervention type). We will use different approaches for these two kinds.\n\na. within-experiment variables\nThere are two moderator variables that vary within-experiments that we will test: age and political concordance. Age varies within experiments, but between participants (each participant can only have one age). Political concordance varies not only within experiments, and also within participants–in the typical experiment design, each participant is shown both concordant and discordant items. In the follwing, we will focus on political concordance, which requires slightly more complex modelling, but provide the models for age in the respective code chunks2.\nWe have two options: The first is integrating political concordance in our baseline model. While this would be the straightforward thing to do, it involves a three-way interaction with many random effects and will likely bring up convergence issues. The second option is to run our baseline model separately for concordant and discordant items, and then proceed as for between-experiment moderators. We will try the first option first, and only revert to the second if we encounter serious convergence issues.\n\ni. Integrating Concordance in baseline model\nIn the first option, we add political concordance to our Stage 1 model, such that we have a three-way interaction between veracity, condition and political concordance.\n\n\nCode\n# calculate model function for political concordance\ncalculate_concordance_model &lt;- function(data) {\n  \n  time &lt;- system.time({\n    model &lt;- glmer(accuracy ~ veracity_numeric + condition_numeric + concordance_numeric + \n                     veracity_numeric*condition_numeric*concordance_numeric +\n                     (1 + veracity_numeric + veracity_numeric:concordance_numeric | \n                        unique_subject_id),\n                   data = data, \n                   family = binomial(link = \"probit\")\n    )\n  })\n  \n  time &lt;- round(time[3]/60, digits = 2)\n  \n  # get a tidy version\n  model &lt;- tidy(model, conf.int = TRUE) %&gt;% \n    # add time\n    mutate(time_minutes = time)\n  \n  return(model)\n  \n}\n\n# calculate model function for age\ncalculate_age_model &lt;- function(data) {\n  \n  time &lt;- system.time({\n    model &lt;- glmer(accuracy ~ veracity_numeric + condition_numeric + age + \n                     veracity_numeric*condition_numeric*age +\n                     (1 + veracity_numeric  | \n                        unique_subject_id),\n                   data = data, \n                   family = binomial(link = \"probit\")\n    )\n  })\n  \n  time &lt;- round(time[3]/60, digits = 2)\n  \n  # get a tidy version\n  model &lt;- tidy(model, conf.int = TRUE) %&gt;% \n    # add time\n    mutate(time_minutes = time)\n  \n  return(model)\n  \n}\n# age_model &lt;- calculate_age_model(data %&gt;% filter(unique_experiment_id == \"1_1\"))\n# age_model %&gt;% print(n = 14)\n\n\nTable 9 illustrates the model output for the first experiment (Experiment 1 of Paper 1) in our simulated data.\n\n\nCode\nconcordance_model &lt;- calculate_concordance_model(data %&gt;% filter(unique_experiment_id == \"1_1\"))\nconcordance_model %&gt;% \n  rounded_numbers() %&gt;% \n  select(-c(effect, group)) %&gt;% \n  kable(booktabs = TRUE, \n        caption = \"Results of the generalized linear mixed model (glmm) on the first experiment (Experiment 1 of Paper 1) in our simulated data.\") %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 9: Results of the generalized linear mixed model (glmm) on the first experiment (Experiment 1 of Paper 1) in our simulated data.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ntime_minutes\n\n\n\n\n(Intercept)\n-0.510\n0.033\n-15.511\n0.000\n-0.574\n-0.445\n0.05\n\n\nveracity_numeric\n0.808\n0.063\n12.740\n0.000\n0.684\n0.933\n0.05\n\n\ncondition_numeric\n-0.112\n0.065\n-1.723\n0.085\n-0.239\n0.015\n0.05\n\n\nconcordance_numeric\n-0.095\n0.063\n-1.504\n0.133\n-0.219\n0.029\n0.05\n\n\nveracity_numeric:condition_numeric\n-0.287\n0.126\n-2.283\n0.022\n-0.534\n-0.041\n0.05\n\n\nveracity_numeric:concordance_numeric\n-0.181\n0.129\n-1.400\n0.161\n-0.435\n0.072\n0.05\n\n\ncondition_numeric:concordance_numeric\n-0.009\n0.125\n-0.074\n0.941\n-0.255\n0.236\n0.05\n\n\nveracity_numeric:condition_numeric:concordance_numeric\n-0.197\n0.255\n-0.770\n0.441\n-0.697\n0.304\n0.05\n\n\nsd__(Intercept)\n0.119\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\ncor__(Intercept).veracity_numeric\n1.000\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\ncor__(Intercept).veracity_numeric:concordance_numeric\n1.000\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\nsd__veracity_numeric\n0.085\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\ncor__veracity_numeric.veracity_numeric:concordance_numeric\n1.000\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\nsd__veracity_numeric:concordance_numeric\n0.342\nNA\nNA\nNA\nNA\nNA\n0.05\n\n\n\n\n\n\n\n\n\n\nGiven that all our binary variables in the data are deviation-coded (-0.5 vs. 0.5) outputs are to be interpreted as follows:\n\n(Intercept) = average -\\(c\\), pooled across all conditions\nveracity_numeric = average \\(d'\\), pooled across all conditions\ncondition_numeric = -\\(\\Delta c_{\\text{condition}}\\), i.e. the change in -response bias between control and treatment, pooled across concordant and discordant items\nconcordance_numeric = -\\(\\Delta c_{\\text{concordance}}\\), i.e. the change in -response bias between concordant and discordant items, pooled across control and treatment\nveracity_numeric:condition_numeric = \\(\\Delta d'_{\\text{condition}}\\), i.e. the change in sensitivity between control and treatment, pooled across concordant and discordant items\nveracity_numeric:concordance_numeric = \\(\\Delta d'_{\\text{concordance}}\\), i.e. the change in sensitivity between concordant and discordant items, pooled across control and treatment\ncondition_numeric:concordance_numeric = Effect of concordance on -\\(\\Delta c_{\\text{condition}}\\),\nveracity_numeric:condition_numeric:concordance_numeric = Effect of concordance on \\(\\Delta d'_{\\text{condition}}\\)\n\nSince interpretation can get a bit complex in three-way interactions here is a good ressource, we demonstrate below that the model estimates correspond to their respective SDT outcomes.\nTo do so, we first run the same model as before, on the same data as before (Experiment 1 of Paper 1), but removing the random effects, so that our model estimates will correspond to the estimates from a by-hand Signal Detection Theory analysis (Table 10).\n\n\nCode\n# same model as above, but without random effects \nglm_model &lt;- glm(accuracy ~ veracity_numeric + condition_numeric + concordance_numeric + \n                     veracity_numeric*condition_numeric*concordance_numeric,\n                   data = data %&gt;% filter(unique_experiment_id == \"1_1\"), \n                   family = binomial(link = \"probit\"))\n\n# give nicer names to estimates\nglm_model &lt;- glm_model %&gt;% \n  tidy() %&gt;% \n  mutate(\n    # reverse c and delta c estimates\n    SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\" | term == \"concordance_numeric\" | term == \"condition_numeric:concordance_numeric\" , \n                      -1*estimate, estimate),\n    SDT_term = case_when(term == \"(Intercept)\" ~ \"average response bias (c)\", \n                     term == \"veracity_numeric\" ~ \"average sensitivity (d')\", \n                     term == \"condition_numeric\" ~ \"delta c (condition)\",\n                     term == \"concordance_numeric\" ~ \"delta c (concordance)\",\n                     term == \"veracity_numeric:condition_numeric\" ~ \"delta d' (condition)\",\n                     term == \"veracity_numeric:concordance_numeric\" ~ \"delta d' (concordance)\",\n                     term == \"condition_numeric:concordance_numeric\" ~ \"effect of concordance on delta c (condition)\",\n                     term == \"veracity_numeric:condition_numeric:concordance_numeric\" ~ \"effect of concordance on delta d' (condition)\",\n    )\n  ) \n\n\nglm_model %&gt;% \n  rounded_numbers() %&gt;% \n  kable(booktabs = TRUE, \n        caption = \"Results of the generalized linear model (glm), i.e. without random effects, on the first experiment (Experiment 1 of Paper 1) in our simulated data.\") %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 10: Results of the generalized linear model (glm), i.e. without random effects, on the first experiment (Experiment 1 of Paper 1) in our simulated data.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nSDT_estimate\nSDT_term\n\n\n\n\n(Intercept)\n-0.504\n0.031\n-16.190\n0.000\n0.504\naverage response bias (c)\n\n\nveracity_numeric\n0.804\n0.062\n12.917\n0.000\n0.804\naverage sensitivity (d')\n\n\ncondition_numeric\n-0.109\n0.062\n-1.750\n0.080\n0.109\ndelta c (condition)\n\n\nconcordance_numeric\n-0.099\n0.062\n-1.591\n0.112\n0.099\ndelta c (concordance)\n\n\nveracity_numeric:condition_numeric\n-0.282\n0.125\n-2.263\n0.024\n-0.282\ndelta d' (condition)\n\n\nveracity_numeric:concordance_numeric\n-0.159\n0.125\n-1.278\n0.201\n-0.159\ndelta d' (concordance)\n\n\ncondition_numeric:concordance_numeric\n-0.005\n0.125\n-0.037\n0.970\n0.005\neffect of concordance on delta c (condition)\n\n\nveracity_numeric:condition_numeric:concordance_numeric\n-0.187\n0.249\n-0.750\n0.453\n-0.187\neffect of concordance on delta d' (condition)\n\n\n\n\n\n\n\n\n\n\nWe can calculate the Signal Detection Theory outcomes by hand as from the summary data shown in Table 11.\n\n\nCode\n# calculate SDT outcomes per condition\nsdt_outcomes &lt;- sdt_data %&gt;%\n  filter(unique_experiment_id == \"1_1\") %&gt;% \n  group_by(sdt_outcome, condition, political_concordance) %&gt;%\n  count() %&gt;% \n  pivot_wider(names_from = sdt_outcome, \n              values_from = n) %&gt;% \n  mutate(\n    z_hit_rate = qnorm(hit / (hit + miss)),\n    z_false_alarm_rate = qnorm(false_alarm / (false_alarm + correct_rejection)),\n    dprime = z_hit_rate - z_false_alarm_rate,\n    c = -1 * (z_hit_rate + z_false_alarm_rate) / 2\n  ) %&gt;% \n  ungroup() \n\nsdt_outcomes %&gt;% \n  kable(booktabs = TRUE, \n        caption = \"Summary data grouped by experimental condition for (Experiment 1 of Paper 1) in our simulated data.\") %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 11: Summary data grouped by experimental condition for (Experiment 1 of Paper 1) in our simulated data.\n\n\n\n\n\n\ncondition\npolitical_concordance\ncorrect_rejection\nfalse_alarm\nhit\nmiss\nz_hit_rate\nz_false_alarm_rate\ndprime\nc\n\n\n\n\ncontrol\nconcordant\n166\n34\n145\n155\n-0.0417893\n-0.9541653\n0.9123760\n0.4979773\n\n\ncontrol\ndiscordant\n244\n56\n107\n93\n0.0878448\n-0.8902469\n0.9780917\n0.4012010\n\n\nintervention\nconcordant\n162\n38\n110\n190\n-0.3406948\n-0.8778963\n0.5372015\n0.6092956\n\n\nintervention\ndiscordant\n245\n55\n91\n109\n-0.1130385\n-0.9027348\n0.7896963\n0.5078867\n\n\n\n\n\n\n\n\n\n\nBased on this data, we can calculate the average response bias ((Intercept)) and sensitivity (veracity_numeric), pooled across all conditions; our treatment effects delta_dprime (veracity_numeric:condition_numeric in the model output) and delta_c (condition_numeric in the model output) from before, i.e. ignoring/pooling across political concordance; the differences in response bias (concordance_numeric in the model output) and sensitivity (veracity_numeric:concordance_numeric in the model output) between concordant and discordant items, pooling across control and treatment groups; the moderator effect of convergence, i.e. how much stronger/weaker the effects on response bias (condition_numeric:concordance_numeric in the model output) and sensitivity (veracity_numeric:condition_numeric:concordance_numeric in the model output) are for concordant items, compared to discordant ones. Table Table 12 shows the results of these by-hand calculations. Comparing the results from the by-hand calculation (Table 12) and the glm (Table 10), we note that the results are the same.\n\n\nCode\n# average dprime and c\nSDT_pooled_averages &lt;- sdt_outcomes %&gt;% \n  summarize(across(c(dprime, c), ~mean(.x, na.rm = TRUE), .names = \"average_{.col}\")) \n\n# treatment effect (i.e. main outcomes)\nintervention_effects &lt;- sdt_outcomes %&gt;%\n  select(political_concordance, condition, dprime, c) %&gt;% \n  pivot_wider(\n    names_from = condition, \n    values_from = c(dprime, c)\n  ) %&gt;% \n  mutate(delta_dprime = dprime_intervention - dprime_control, \n         delta_c = c_intervention - c_control) %&gt;% \n  # pool across concordance\n  summarize(across(starts_with(\"delta\"), ~mean(.x, na.rm = TRUE), .names = \"{.col}\")) \n\n# differences in SDT outcomes by convergence\ndifferences_SDT_by_convergence &lt;- sdt_outcomes %&gt;%\n  select(political_concordance, condition, dprime, c) %&gt;% \n  pivot_wider(\n    names_from = political_concordance, \n    values_from = c(dprime, c)\n  ) %&gt;% \n  mutate(delta_dprime = dprime_concordant - dprime_discordant, \n         delta_c = c_concordant - c_discordant) %&gt;% \n  # pool across condition\n  summarize(across(starts_with(\"delta\"), ~mean(.x, na.rm = TRUE), .names = \"{.col}_concordance\")) \n\n# moderator effects\nmoderator_effects &lt;- sdt_outcomes %&gt;%\n  select(political_concordance, condition, dprime, c) %&gt;% \n  pivot_wider(\n    names_from = condition, \n    values_from = c(dprime, c)\n  ) %&gt;% \n  mutate(delta_dprime = dprime_intervention - dprime_control, \n         delta_c = c_intervention - c_control) %&gt;% \n  select(political_concordance, starts_with(\"delta\")) %&gt;% \n  pivot_wider(\n    names_from = political_concordance, \n    values_from = starts_with(\"delta\")\n  ) %&gt;% \n  mutate(moderator_effect_dprime = delta_dprime_concordant - delta_dprime_discordant, \n         moderator_effect_c = delta_c_concordant - delta_c_discordant) %&gt;% \n  select(starts_with(\"moderator\"))\n\n\n\n\nCode\n# make an overview table \nSDT_outcomes_overview &lt;- bind_cols(SDT_pooled_averages, intervention_effects, \n                                   differences_SDT_by_convergence, \n                                   moderator_effects) %&gt;% \n  pivot_longer(\n    cols = everything(), \n    names_to = \"Outcome\",\n    values_to = \"Value\"\n  ) %&gt;% \n  rounded_numbers()\n\nSDT_outcomes_overview %&gt;% \n  kable(booktabs = TRUE, \n        caption = \"Outcomes from by-hand SDT calculation.\") %&gt;%\n  kable_styling(font_size = 10,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 12: Outcomes from by-hand SDT calculation.\n\n\n\n\n\n\nOutcome\nValue\n\n\n\n\naverage_dprime\n0.804\n\n\naverage_c\n0.504\n\n\ndelta_dprime\n-0.282\n\n\ndelta_c\n0.109\n\n\ndelta_dprime_concordance\n-0.159\n\n\ndelta_c_concordance\n0.099\n\n\nmoderator_effect_dprime\n-0.187\n\n\nmoderator_effect_c\n0.005\n\n\n\n\n\n\n\n\n\n\nJust as we do for our main analysis, we estimate the model for each experiment separately and store the results in a common data frame.\n\n\nCode\n# Since the loop takes some time, we stored the results in a data frame that we can reload. \nfilename &lt;- \"../data/simulations/concordance_models.csv\" \n\nrun_loop_concordance &lt;- function(data, filename){\n  \n  # make a vector with all unique experiment ids\n  experiments &lt;- data %&gt;% \n    distinct(unique_experiment_id) %&gt;% \n    #slice(1:10) %&gt;% # reduce number of experiments here, to avoid long computation times\n    pull()\n  \n  # only execute the following if the file does NOT exist\n  if (!file.exists(filename)) {\n    \n    time &lt;- system.time({\n      \n      # run one model per experiment and store the results in a common data frame\n      results &lt;- experiments %&gt;%\n        map_dfr(function(x) {\n          \n          # restrict data to only the respective experiment\n          experiment &lt;- data %&gt;% filter(unique_experiment_id == x)\n          \n          # extract paper id\n          paper_id &lt;- unique(experiment$paper_id)\n          \n          # To keep track of progress\n          print(paste(\"calculating model for experiment \", x))\n          \n          model_experiment &lt;- calculate_concordance_model(experiment) %&gt;%\n            mutate(unique_experiment_id = x,\n                   paper_id = paper_id)\n          \n          return(model_experiment)\n        })\n    })\n    \n    write_csv(results, filename)\n    \n    print(paste(\"Elapsed time: \", round(time[3]/60, digits = 2), \" minutes\"))\n  }\n  \n}\n\n# execute function\nrun_loop_concordance(data, filename)\n# read saved model results\nconcordance_model_results &lt;- read_csv(filename)\n\n\n\n\nCode\n# give nicer names to estimates\nconcordance_model_results &lt;- concordance_model_results %&gt;% \n  filter(effect == \"fixed\") %&gt;% \n  mutate(\n    # reverse c and delta c estimates\n    SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\" | term == \"concordance_numeric\" | term == \"condition_numeric:concordance_numeric\", \n                      -1*estimate, estimate),\n    SDT_term = case_when(term == \"(Intercept)\" ~ \"average response bias (c)\", \n                     term == \"veracity_numeric\" ~ \"average sensitivity (d')\", \n                     term == \"condition_numeric\" ~ \"delta c (condition)\",\n                     term == \"concordance_numeric\" ~ \"delta c (concordance)\",\n                     term == \"veracity_numeric:condition_numeric\" ~ \"delta d' (condition)\",\n                     term == \"veracity_numeric:concordance_numeric\" ~ \"delta d' (concordance)\",\n                     term == \"condition_numeric:concordance_numeric\" ~ \"effect of concordance on delta c (condition)\",\n                     term == \"veracity_numeric:condition_numeric:concordance_numeric\" ~ \"effect of concordance on delta d' (condition)\",\n    ),\n    sampling_variance = std.error^2\n  ) \n\n\nWe then run the same meta-analytic model as for the main analysis, but on the moderator effect estimates. Table 13 shows the results of these models.\n\n\nCode\n# model for delta dprime\nconcordance_delta_dprime &lt;- calculate_models(data = concordance_model_results %&gt;% \n                                   filter(SDT_term == \"effect of concordance on delta d' (condition)\"), \n                                 yi = SDT_estimate, \n                                 vi = sampling_variance, robust = TRUE)\n\n# model for delta c\nconcordance_delta_c &lt;- calculate_models(data = concordance_model_results %&gt;% \n                                   filter(SDT_term == \"effect of concordance on delta c (condition)\"), \n                                 yi = SDT_estimate, \n                                 vi = sampling_variance, robust = TRUE)\n\n\nmodelsummary::modelsummary(list(\"Delta d'\" = concordance_delta_dprime, \n                                \"Delta c\" = concordance_delta_c\n                                ), \n                           title = \"Moderator analysis for politicial concordance based on an individual-level estimates\",\n                           stars = TRUE,\n                           output = \"kableExtra\",\n                           coef_rename = c(\"overall\" = \"Effect of political concordance\")\n                           ) \n\n\n\n\nTable 13: Moderator analysis for politicial concordance based on an individual-level estimates\n\n\n\n\n\n\n\nDelta d'\nDelta c\n\n\n\n\nEffect of political concordance\n−0.002\n0.009\n\n\n\n(0.055)\n(0.029)\n\n\nNum.Obs.\n21\n21\n\n\nAIC\n5.1\n−20.4\n\n\nBIC\n8.2\n−17.2\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nii. Running separate baseline models for concordance\nIf we encounter serious convergence issues by integrating the moderator variable in the individual-level model, we will use an alternative strategy. It consists in calculating separate models for concordant and discordant items, and then running a meta-regressions.\n\n\nCode\n# make two data frames for the two conditions\ndata_concordant &lt;- data %&gt;% filter(political_concordance == \"concordant\")\ndata_discordant &lt;- data %&gt;% filter(political_concordance == \"discordant\")\n\nrun_loop(data = data_concordant, filename = \"../data/simulations/concordant_data.csv\")\nrun_loop(data = data_discordant, filename = \"../data/simulations/discordant_data.csv\")\n\n# read saved model results\nconcordant_results &lt;- read_csv(\"../data/simulations/concordant_data.csv\")\ndiscordant_results &lt;- read_csv(\"../data/simulations/discordant_data.csv\")\n\nresults &lt;- bind_rows(concordant_results %&gt;% \n                       mutate(political_concordance = \"concordant\"), \n                     discordant_results %&gt;% \n                       mutate(political_concordance = \"discordant\")\n                     )\n\n\nThis procedure is basically the same as for between-experiment variables (see next section) and run a meta-regression, but there is a slight difference in the meta-regression model specifications: In the case of political concordance, our outcome data frame on which we run the meta-analysis contains two observations per experiment–one for discordant, the other for concordant items. We want to account for this dependency structure with a slightly different random effects structure, where observations are nested in experiments.\nTable 14 shows the outcome of this meta-regression based on separate baseline estimates for concordant and discordant news items. In our simulated data–where no true moderator effect was modeled–these estimates are larger than the once we obtain from the intregrated individual-level model (Table 13), but reassuringly they are not significant in either case.\n\n\nCode\n# add an observation identifier\nresults &lt;- results %&gt;% \n  mutate(observation_id = 1:nrow(.))\n\n\n\n\nCode\n# give nicer names to estimates\nresults &lt;- results %&gt;% \n  filter(effect == \"fixed\") %&gt;% \n  mutate(\n    # reverse c and delta c estimates\n    SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\", \n                      -1*estimate, estimate),\n    SDT_term = case_when(term == \"(Intercept)\" ~ \"average response bias (c)\", \n                     term == \"veracity_numeric\" ~ \"average sensitivity (d')\", \n                     term == \"condition_numeric\" ~ \"delta c\",\n                     term == \"veracity_numeric:condition_numeric\" ~ \"delta d'\",\n    ),\n    sampling_variance = std.error^2\n  ) \n\n\n\n\nCode\n# Function to calculate meta models for the concordance variable\nmeta_regression_concordance &lt;- function(data, yi, vi, moderator, robust = TRUE) {\n  \n  # provide metafor compatible names\n  metafor_data &lt;- data %&gt;% \n    rename(yi = {{yi}}, \n           vi = {{vi}}, \n           moderator = {{moderator}})\n  \n  # Multilevel random effect model for accuracy\n  model &lt;-  metafor::rma.mv(yi, vi, \n                            mods = ~moderator,\n                            random = ~ 1 | unique_experiment_id / observation_id, \n                            data = metafor_data)\n  \n  return(model)\n  \n  if(robust == TRUE) {\n    # with robust standard errors clustered at the paper level \n    robust_model &lt;- robust(model, cluster = data$paper_id)\n    \n    return(robust_model)\n  }\n}\n\n\n\n\nCode\nconcordance_delta_dprime &lt;- meta_regression_concordance(data = results %&gt;% \n                                                              filter(SDT_term == \"delta d'\"), \n                                                            yi = SDT_estimate, \n                                                            vi = sampling_variance, \n                                                            moderator = political_concordance,\n                                                            robust = TRUE)\n\nconcordance_delta_c &lt;- meta_regression_concordance(data = results %&gt;% \n                                                              filter(SDT_term == \"delta c\"), \n                                                            yi = SDT_estimate, \n                                                            vi = sampling_variance, \n                                                            moderator = political_concordance,\n                                                            robust = TRUE)\n\nmodelsummary::modelsummary(list(\"Delta d'\" = concordance_delta_dprime, \n                                \"Delta c\" = concordance_delta_c\n                                ), \n                           stars = TRUE,\n                           title = \"Moderator analysis for political concordance based on a meta-regression\",\n                           output = \"kableExtra\"\n                           )\n\n\n\n\nTable 14: Moderator analysis for political concordance based on a meta-regression\n\n\n\n\n\n\n\nDelta d'\nDelta c\n\n\n\n\nintercept\n0.522***\n0.485***\n\n\n\n(0.038)\n(0.025)\n\n\nmoderatordiscordant\n0.011\n0.021\n\n\n\n(0.029)\n(0.015)\n\n\nNum.Obs.\n44\n44\n\n\nAIC\n−37.5\n−96.7\n\n\nBIC\n−30.4\n−89.6\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb. Between-experiment variables\nThe main between-experiment variable we will look at here is interventions type. In our simulated data, we made up three intervention types (“literacy tips”, “priming”, “warning labels”). We run a meta-regression, in which we add intervention type as a covariate to the meta-analytic model from the main analysis. The results of this analaysis in our simulated data–where no true moderator effect was modeled–can be found in Table 15.\n\n\nCode\n# we add the intervention types to the effect sizes data frame with the SDT outcomes\n\n# get interventions of all experiments\ndata_intervention_types &lt;- data %&gt;% \n  group_by(unique_experiment_id) %&gt;% \n  # Get all experiments\n  reframe(intervention_type = unique(intervention_type)) \n\n# add intervention types to data\nmoderator_data &lt;- left_join(model_results, data_intervention_types)\n\n\n\n\nCode\n# Function to calculate meta models\nmeta_regression &lt;- function(data, yi, vi, moderator, robust = TRUE) {\n  \n  # provide metafor compatible names\n  metafor_data &lt;- data %&gt;% \n    rename(yi = {{yi}}, \n           vi = {{vi}}, \n           moderator = {{moderator}})\n  \n  # Multilevel random effect model for accuracy\n  model &lt;-  metafor::rma.mv(yi, vi, \n                            mods = ~moderator,\n                            random = ~ 1 | paper_id / unique_experiment_id, \n                            data = metafor_data)\n  \n  return(model)\n  \n  if(robust == TRUE) {\n    # with robust standard errors clustered at the paper level \n    robust_model &lt;- robust(model, cluster = data$paper_id)\n    \n    return(robust_model)\n  }\n}\n\n\n\n\nCode\n# meta-regression for delta dprime\ninterventiontype_delta_dprime &lt;- meta_regression(data = moderator_data %&gt;% \n                                  filter(SDT_term == \"delta d'\"), \n                                yi = estimate, \n                                vi = sampling_variance, \n                                moderator = intervention_type,\n                                robust = TRUE)\n\n# meta-regression for c\ninterventiontype_delta_c &lt;- meta_regression(data = moderator_data %&gt;% \n                                  filter(SDT_term == \"delta c\"), \n                                yi = estimate, \n                                vi = sampling_variance, \n                                moderator = intervention_type,\n                                robust = TRUE)\n\n\n\n\nCode\nmodelsummary::modelsummary(list(\"Delta d'\" = interventiontype_delta_dprime, \n                                \"Delta c\" = interventiontype_delta_c\n                                ), \n                           stars = TRUE,\n                           output = \"kableExtra\",\n                           title = \"Moderator analysis for intervention type\"\n                           )\n\n\n\n\nTable 15: Moderator analysis for intervention type\n\n\n\n\n\n\n\nDelta d'\nDelta c\n\n\n\n\nintercept\n0.101*\n−0.078*\n\n\n\n(0.046)\n(0.032)\n\n\nmoderatorliteracy tips\n0.001\n0.001\n\n\n\n(0.042)\n(0.021)\n\n\nmoderatorpriming\n0.000\n0.000\n\n\n\n(0.005)\n(0.005)\n\n\nmoderatorwarning labels\n−0.008\n−0.006\n\n\n\n(0.046)\n(0.023)\n\n\nNum.Obs.\n52\n52\n\n\nAIC\n−76.2\n−135.3\n\n\nBIC\n−64.5\n−123.6\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#sensitivity-analysis",
    "href": "preregistration/preregistration.html#sensitivity-analysis",
    "title": "Pre-data collection registration",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\n\nCode\nsensitivity_data &lt;- read_csv(\"../data/simulations/sensitivity_analysis.csv\")\n\n\nSince we are running a meta-analysis based on a systematic review, we cannot control the final sample size. To have rough estimate on the statistical power we anticipate our study to have, we ran a sensitivity analysis based on a simulation. For the simulation, we–conservatively–assumed that the meta-analysis sample will consist of 10 papers. We assumed that each paper has between 1 and 4 experiments, and each experiment can have between two and four experimental arms (one of which is always the control condition). For each experimental arm, we assumed a sample size of 100 participants. The number of experiments per paper and arms per experiments was chosen randomly for each study. We further assumed that participants always saw 5 true and 5 false news. For details about other parameter assumptions, see the parameter list specified above. Although our final sample of papers will probably have properties quite different from what we assumed here, we believe these assumptions are rather conservative.\nIn our simulations, we varied the values (small = 0.2, medium = 0.5, large = 0.8) for the true effect sizes for d’ and c in the data. For each combination of the two effect sizes, we ran 100 iterations, i.e. 100 times we generated a different sample of 10 papers, and ran our meta-analysis on that sample (900 different meta-analyses in total). The aim of the sensitivity analysis consists in checking for how many of these 100 meta-analyses per combination we find a significant effect. The share of analyses that detect the true effect is the statistical power.\nAs shown in Figure 3 for d’ and in Figure 4 for c, even for very small effect sizes (0.2), we find statistical power greater than 90%, given our assumptions. For d’, the value of c does not appear to affect the statistical power. For c, a low d’ (0.2) appears to yield slightly lower statistical power than a medium (0.5) or large (0.8) d.\n\n\nCode\n# specify significance\nalpha &lt;- 0.05\n\nplot_data &lt;- sensitivity_data %&gt;% \n  mutate(significant = ifelse(p.value &lt; alpha, TRUE, FALSE)) %&gt;% \n  group_by(parameter_delta_d_prime, parameter_delta_c) %&gt;% \n  summarise(power = mean(significant)) \n\n\n\n\nCode\nggplot(plot_data, \n       aes(x = parameter_delta_d_prime, y = power, color = as.factor(parameter_delta_c))) +\n  geom_point(size = 1.5, alpha = 1) +\n  geom_line(size = 1, alpha = 0.3) + \n  # add a horizontal line at 90%, our power_threshold\n  geom_hline(aes(yintercept = .9), linetype = 'dashed') + \n  # Prettify!\n  theme_minimal() + \n  scale_colour_viridis_d(option = \"plasma\", begin = 0.4, end = 0.7) + \n  scale_y_continuous(labels = scales::percent) + \n  labs(x = 'True effect size', y = 'Power', \n       title = \"Power Curve for Delta d'\")\n\n\n\n\n\n\n\n\nFigure 3: Results of sensitivity analysis for Delta d’. The plot shows the power curve, i.e. the share of statistically significant effects across 100 simulated meta-analyses for each pair of values of d’ (x-axis) and c (color legend).\n\n\n\n\n\n\n\nCode\n# plot results\nggplot(plot_data, \n       aes(x = parameter_delta_c, y = power, color = as.factor(parameter_delta_d_prime))) +\n  geom_point(size = 1.5, alpha = 0.5) +\n  geom_line(size = 1, alpha = 0.3) + \n  # add a horizontal line at 90%, our power_threshold\n  geom_hline(aes(yintercept = .9), linetype = 'dashed') + \n  # Prettify!\n  theme_minimal() + \n  scale_colour_viridis_d(option = \"plasma\") + \n  scale_y_continuous(labels = scales::percent) + \n  labs(x = 'True effect size', y = 'Power', \n       title = \"Power Curve for Delta c\")\n\n\n\n\n\n\n\n\nFigure 4: Results of sensitivity analysis for Delta c. The plot shows the power curve, i.e. the share of statistically significant effects across 100 simulated meta-analyses for each pair of values of c (x-axis) and d’ (color legend).",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#parameter-recovery",
    "href": "preregistration/preregistration.html#parameter-recovery",
    "title": "Pre-data collection registration",
    "section": "Parameter Recovery",
    "text": "Parameter Recovery\nInstead of only checking whether our models find a significant effect or not, we also descriptively check how well our model recovers the data generating parameters across the different samples.\nAs shown in Figure @ref(fig:sensitivity-parameter-dprime) for d’ and Figure @ref(fig:sensitivity-parameter-c) for c, we find that the distributions of meta-analytic estimates across the 100 samples per pair of effect sizes are centered around the true data generating parameter when the effect size is small (0.2). With an increasing true effect size, however, the estiamte distributions tend to be shifted to the left of the parameter, which suggests that our models consistently underestimate the true effect for larger effect sizes.\nOverall, our simulation suggests that (i) given conservative sample size assumptions, we will have large enough statistical power to detect even small effects, and (ii) that our model might slightly underestimate larger true effect sizes, which makes it a conservative estimator.\n\n\nCode\n# custom function for grid labels\ncustom_labeller &lt;- labeller(\n  parameter_delta_c = function(x) paste(\"Delta c:\", x),\n  parameter_delta_d_prime = function(x) paste(\"Delta d':\", x)\n)\n\n# plot Delta d' estimates\nggplot(sensitivity_data %&gt;% filter(term == \"delta d'\"), aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = parameter_delta_d_prime), linetype = \"dotted\", color = \"black\") +\n  labs(x = \"Estimate for Delta d'\") + \n  facet_grid(rows = vars(parameter_delta_c), cols = vars(parameter_delta_d_prime), \n    labeller = custom_labeller)\n\n\n\n\n\n\n\n\nFigure 5: Distributions of Delta d’ across simulated meta-analyses. The plot shows the distribution of meta-analytic estimates, for each combination of Delta d’ and Delta c values.\n\n\n\n\n\n\n\nCode\nggplot(sensitivity_data %&gt;% filter(term == \"delta c\"), aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = parameter_delta_c), linetype = \"dotted\", color = \"black\") +\n  labs(x = \"Estimate for Delta c\") + \n  facet_grid(cols = vars(parameter_delta_c), rows = vars(parameter_delta_d_prime), \n    labeller = custom_labeller)\n\n\n\n\n\n\n\n\nFigure 6: Distributions of Delta c across simulated meta-analyses. The plot shows the distribution of meta-analytic estimates, for each combination of Delta c and Delta d’ values.",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#sec-step-by-step",
    "href": "preregistration/preregistration.html#sec-step-by-step",
    "title": "Pre-data collection registration",
    "section": "From basic Signal Detection Theory (SDT) to mixed models step-by-step",
    "text": "From basic Signal Detection Theory (SDT) to mixed models step-by-step\nIn this appendix, we explain step-by-step how to go from a by-hand to a generalized linear mixed model (glmm) Signal Detection Theory (SDT) analysis.\n\nBasic Signal Detection Theory\nAfter having classified instances of news ratings according to SDT terminology (Table 5), we can manually calculate SDT outcomes. Table 16 shows by-hand calculated SDT outcomes for the first experiment of our simulated meta-analysis sample.\n\n\nCode\n# Pick a single experiment\ndata_experiment_1 &lt;- sdt_data %&gt;% \n  filter(unique_experiment_id == \"1_1\")\n\n# calculate SDT outcomes per condition\nsdt_outcomes &lt;- data_experiment_1 %&gt;% \n  group_by(sdt_outcome, condition) %&gt;%\n  count() %&gt;% \n  pivot_wider(names_from = sdt_outcome, \n              values_from = n) %&gt;% \n  mutate(\n    z_hit_rate = qnorm(hit / (hit + miss)),\n    z_false_alarm_rate = qnorm(false_alarm / (false_alarm + correct_rejection)),\n    dprime = z_hit_rate - z_false_alarm_rate,\n    c = -1 * (z_hit_rate + z_false_alarm_rate) / 2\n  ) %&gt;% \n  ungroup()\n\n\n\n\nCode\nsdt_outcomes %&gt;% \n  rounded_numbers() %&gt;% \n  kable(\n    caption = \"SDT outcomes calculated by-hand for Experiment 1 of simulated data.\",\n    booktabs = TRUE) %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 16: SDT outcomes calculated by-hand for Experiment 1 of simulated data.\n\n\n\n\n\n\ncondition\ncorrect_rejection\nfalse_alarm\nhit\nmiss\nz_hit_rate\nz_false_alarm_rate\ndprime\nc\n\n\n\n\ncontrol\n410\n90\n252\n248\n0.010\n-0.915\n0.925\n0.453\n\n\nintervention\n407\n93\n201\n299\n-0.248\n-0.893\n0.645\n0.570\n\n\n\n\n\n\n\n\n\n\nOur treatment effects are the differences between the control and treatment group. We therefor call them delta_dprime and delta_c here (see Table 17).\n\n\nCode\ntreatment_effects &lt;- sdt_outcomes %&gt;%\n  select(condition, dprime, c) %&gt;% \n  pivot_wider(\n    names_from = condition, \n    values_from = c(dprime, c)\n  ) %&gt;% \n  mutate(delta_dprime = dprime_intervention - dprime_control, \n         delta_c = c_intervention - c_control) %&gt;%\n  select(starts_with(\"delta\"))\n\n\n\n\nCode\ntreatment_effects %&gt;% \n  rounded_numbers() %&gt;% \n  kable(\n    caption = \"SDT treatment effects calculated by-hand for Experiment 1 of simulated data.\",\n    booktabs = TRUE) %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 17: SDT treatment effects calculated by-hand for Experiment 1 of simulated data.\n\n\n\n\n\n\ndelta_dprime\ndelta_c\n\n\n\n\n-0.281\n0.118\n\n\n\n\n\n\n\n\n\n\n\nb. SDT in Generalized Mixed Model (glm)\nTo obtain test statistics for these outcomes, we can do the equivalent analysis in a generalized linear model (glm), using a probit link function. We use deviation coding for our veracity (fake = -0.5, true = 0.5) and condition (-0.5 = control, 0.5 = intervention) variables. Table 18 shows the results of the glm.\n\n\nCode\n# run model\nmodel_glm &lt;- glm(accuracy ~ veracity_numeric*condition_numeric, data = data_experiment_1, family = binomial(link = \"probit\"))\n\n# Tidy the model and add the SDT_term column\nmodel_results &lt;- tidy(model_glm, conf.int = TRUE) %&gt;%\n  mutate(SDT_term = case_when(\n    term == \"(Intercept)\" ~ \"Average c (pooled across all conditions)\",\n    term == \"veracity_numeric\" ~ \"Average d' (pooled across all conditions)\",\n    term == \"condition_numeric\" ~ \"Delta c (change in response bias between control and treatment)\",\n    term == \"veracity_numeric:condition_numeric\" ~ \" Delta d' (change in sensitivity between control and treatment)\",\n    TRUE ~ \"Other\"\n  ), \n  # reverse c and delta c estimates\n  SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\", \n                        -1*estimate, estimate)\n  )\n\n\n\n\nCode\n# Create a table with kable\nmodel_results %&gt;%\n  select(term, estimate, p.value, SDT_estimate, SDT_term) %&gt;%\n  kable(\n    caption = \"Summary of glm results for Experiment 1 of simulated data\",\n    col.names = c(\"Term\", \"Estimate\", \"p-value\", \"SDT Estiamte\", \"SDT Term\"),\n    digits = 3,\n    booktabs = TRUE) %&gt;%\n  kable_styling(\n    full_width = FALSE, \n    font_size = 10, \n    latex_options = c(\"scale_down\")\n  ) %&gt;%\n  add_footnote(\n    \"The model coefficients have the interpretations in terms of SDT as presented here in this table because we use deviation coding for our veracity (fake = -0.5, true = 0.5) and condition (-0.5 = control, 0.5 = intervention) variables.\",\n    notation = \"none\"\n  )\n\n\n\n\nTable 18: Summary of glm results for Experiment 1 of simulated data\n\n\n\n\n\n\nTerm\nEstimate\np-value\nSDT Estiamte\nSDT Term\n\n\n\n\n(Intercept)\n-0.512\n0.000\n0.512\nAverage c (pooled across all conditions)\n\n\nveracity_numeric\n0.785\n0.000\n0.785\nAverage d' (pooled across all conditions)\n\n\ncondition_numeric\n-0.118\n0.053\n0.118\nDelta c (change in response bias between control and treatment)\n\n\nveracity_numeric:condition_numeric\n-0.281\n0.021\n-0.281\nDelta d' (change in sensitivity between control and treatment)\n\n\n\n The model coefficients have the interpretations in terms of SDT as presented here in this table because we use deviation coding for our veracity (fake = -0.5, true = 0.5) and condition (-0.5 = control, 0.5 = intervention) variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc. SDT in mixed models\nHowever, this analysis is naive, because it treats all observations (instances of news ratings) as independent. Yet, participants give several ratings, and news ratings from the same participant are not independent of each other.\n\ni. Participant averages\nOne simple way to account for this dependency is to compute participant-level outcomes, and using these averages as observations. This way, each participant only contributes one data point. Figure @ref(fig:individual-level-plot) shows the distributions of participants’ averages for Experiment 1 of the simulated data. To obtain estimates for our treatment effects \\(\\Delta c\\) and \\(\\Delta d'\\), we can then run a linear regression with condition as a predictor (or do a t-test). The results of these regressions are shown in Table 19.\n\n\nCode\n# calculate SDT outcomes per participant\nsdt_participants &lt;- data_experiment_1 %&gt;% \n  drop_na(sdt_outcome) %&gt;% \n  group_by(unique_subject_id, sdt_outcome) %&gt;%\n  count() %&gt;%\n  ungroup() %&gt;% \n  # Note that currently, not all outcomes appear for participants (e.g. if a participant had only hits and false alarms, correct rejections and misses will not appear). This is a problem later, because when we compute hit and miss rates, the categories that are not appearing will be coded as NA, messing up the calculations. To avoid this, we use the complete() function and ensure that outcomes which do not occur are coded as 0. \n  complete(\n    unique_subject_id,\n    sdt_outcome, \n    fill = list(n = 0)\n  ) %&gt;% \n  # since we want the condition variable in our data, we code it back into there\n  left_join(\n    sdt_data %&gt;% select(unique_subject_id, condition) %&gt;% distinct()\n  ) %&gt;%  \n  pivot_wider(names_from = sdt_outcome, \n              values_from = n) %&gt;% \n  # At this point we need to correct for cases when hit rate or false alarm rate take the values of 0 (case in which qnorm(0) = -Inf) or 1 (case in which qnorm(1) = Inf). We follow Batailler in applying log-linear rule correction (Hautus, 1995)\n  mutate(\n    hit = hit + 0.5,\n    miss = miss + 0.5,\n    correct_rejection = correct_rejection + 0.5,\n    false_alarm = false_alarm + 0.5,\n  ) %&gt;% \n  # We can then compute sdt outcomes for each participant\n  mutate(\n    z_hit_rate = qnorm(hit / (hit + miss)),\n    z_false_alarm_rate = qnorm(false_alarm / (false_alarm + correct_rejection)),\n    dprime = z_hit_rate - z_false_alarm_rate,\n    c = -1 * (z_hit_rate + z_false_alarm_rate) / 2\n  ) %&gt;% \n  ungroup()\n\n\n\n\nCode\n# plot\n\n# Main plot data: shape data to long format\nplot_data &lt;- sdt_participants %&gt;% \n  pivot_longer(c(dprime, c),\n               names_to = \"outcome\", \n               values_to = \"value\") %&gt;% \n  # make nicer names\n  mutate(outcome = ifelse(outcome == \"dprime\", \"D' (sensitivity)\", \n                          \"C (response bias)\"))\n\n# summary data for labels\n# table \nsummary_data &lt;- plot_data %&gt;% \n  drop_na(value) %&gt;% \n  mutate(valence = ifelse(value &gt; 0, \"positive\", \n                          ifelse(value == 0, \"neutral\", \n                                 \"negative\")\n                          )\n         ) %&gt;% \n  group_by(valence, outcome) %&gt;% \n  summarize(n_subj = n_distinct(unique_subject_id)) %&gt;% \n    pivot_wider(names_from = outcome, \n              values_from = n_subj) %&gt;% \n  # relative frequency\n  ungroup() %&gt;% \n  mutate(\n    rel_dprime = `D' (sensitivity)` / sum(`D' (sensitivity)`),\n    rel_c =  `C (response bias)` / sum(`C (response bias)`)\n    ) %&gt;% \n  pivot_longer(c(rel_dprime, rel_c), \n               names_to = \"outcome\", \n               values_to = \"value\") %&gt;% \n  mutate(outcome = ifelse(outcome == \"rel_dprime\", \"D' (sensitivity)\", \n                          \"C (response bias)\"), \n         label = paste0(round(value, digits = 4)*100, \" %\"),\n         x_position = case_when(valence == \"negative\" ~ -1,\n                                valence == \"neutral\" ~ 0,\n                                valence == \"positive\" ~ 1), \n         y_position = 1.5)\n\n# make plot\nindividual_level_plot &lt;- ggplot(plot_data, aes(x = value, fill = outcome, color = outcome)) +\n  geom_density(alpha = 0.5, adjust = 1.5)+\n  # add line at 0\n  geom_vline(xintercept = 0, \n             linewidth = 0.5, linetype = \"24\", color = \"grey\") +\n  # scale\n  # scale_x_continuous(breaks = seq(from = -1, to = 1, by = 0.2)) +\n  # add labels for share of participants\n  geom_label(inherit.aes = FALSE, data = summary_data,\n             aes(x = x_position, y = y_position, \n                 label = label),\n             alpha = 0.6,\n             color = \"grey50\", size = 3, show.legend = FALSE) +\n  # colors \n  scale_color_viridis_d(option = \"turbo\", begin = 0.25, end = 1)+\n  scale_fill_viridis_d(option = \"turbo\", begin = 0.25, end = 1) +\n  # labels and scales\n  labs(x = \"Z-scores\", y = \"Density\") +\n  guides(fill = FALSE, color = FALSE) +\n  plot_theme +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_blank(),\n        strip.text = element_text(size = 14)) +\n  facet_wrap(~outcome)\n\n#individual_level_plot\n\n# Save the plot to a file\nggsave(\"individual_level_plot.png\", individual_level_plot, width = 8, height = 6)\n\n# In the RMarkdown file\nknitr::include_graphics(\"individual_level_plot.png\")\n\n\n\n\n\n\n\n\nFigure 7: Distribution of participant-level averages for Experiment 1 of the simulated data. The percentage labels (from left to right) represent the share of participants with a negative score, a score of exactly 0, and a positive score, for both measures respectively. Note that when calculating by-participant averages, we follow Batailler et al. (2022) in applying log-linear rule correction. This is particularly relevant for cases when the hit rate or the false alarm rate take the values of 0 (case in which qnorm(0) = -Inf) or 1 (case in which qnorm(1) = Inf).\n\n\n\n\n\n\n\nCode\nmodel_dprime &lt;- lm(dprime ~ condition, data = sdt_participants)\nmodel_c &lt;- lm(c ~ condition, data = sdt_participants)\n\nmodelsummary::modelsummary(list(\"d'\" = model_dprime, \n                                \"c\" = model_c\n                                ), \n                           stars = TRUE,\n                           output = \"kableExtra\",\n                           title = \"SDT outcomes based on a regression on participant-level averages\",\n                           coef_rename = c(\"conditionintervention\" = \"Treatment Effect\"),\n                           )\n\n\n\n\nTable 19: SDT outcomes based on a regression on participant-level averages\n\n\n\n\n\n\n\nd'\nc\n\n\n\n\n(Intercept)\n0.814***\n0.402***\n\n\n\n(0.068)\n(0.038)\n\n\nTreatment Effect\n−0.263**\n0.101+\n\n\n\n(0.096)\n(0.054)\n\n\nNum.Obs.\n200\n200\n\n\nR2\n0.036\n0.017\n\n\nR2 Adj.\n0.031\n0.012\n\n\nAIC\n417.5\n186.1\n\n\nBIC\n427.4\n196.0\n\n\nLog.Lik.\n−205.768\n−90.061\n\n\nRMSE\n0.68\n0.38\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy comparing the results of the regression based on participant-level averages (Table 19), to the results of the glm at the rating-level and which glosses over participant dependencies (Table 18), we can see that the estimates for our outcomes \\(\\Delta c\\) and \\(\\Delta d'\\) are slightly different. However, we can account even better for our data structure, and estimate both within and between participant variation separately by using a generalized linear mixed model (glmm).\n\n\n\nii. Mixed model SDT\nUsing a glm with probit link function as above, we can additionally specify random effects. The result is a generalized linear mixed model (glmm). Adding random effects for participants allows us to model the dependency of data points from the same participant, thereby account for these difference, while not loosing data points as in the participant-averages approach discussed above.\nTable 20 shows that, for our simulated experiment 1, the estimates of the glmm are close to, but slightly different from, the initial gml without random effects (Table 18).\n\n\nCode\n# Sometimes these models take time, so we check that time\ntime &lt;- system.time({\n  mixed_model &lt;- glmer(accuracy ~ veracity_numeric + condition_numeric + \n                         veracity_numeric*condition_numeric +\n                         (1 + veracity_numeric | unique_subject_id),\n                       data = data_experiment_1, \n                       family = binomial(link = \"probit\"))\n})\n\n#print(paste(\"Elapsed time: \", round(time[3]/60, digits = 2), \" minutes\"))\n\n# get a tidy version\nmixed_model &lt;- tidy(mixed_model, conf.int = TRUE)\n\n\n\n\nCode\n# show results\nmixed_model &lt;- mixed_model %&gt;%\n  mutate(SDT_term = case_when(\n    term == \"(Intercept)\" ~ \"Average c (pooled across all conditions)\",\n    term == \"veracity_numeric\" ~ \"Average d' (pooled across all conditions)\",\n    term == \"condition_numeric\" ~ \"Delta c (change in -response bias between control and treatment)\",\n    term == \"veracity_numeric:condition_numeric\" ~ \" Delta d' (change in sensitivity between control and treatment)\",\n    TRUE ~ \"Other\"\n  ), \n  # reverse c and delta c estimates\n  SDT_estimate = ifelse(term == \"(Intercept)\" | term == \"condition_numeric\", \n                        -1*estimate, estimate)\n  )\n\nmixed_model %&gt;% \n  select(-starts_with(\"conf\")) %&gt;% \n  rounded_numbers() %&gt;% \n  select(-c(effect, group)) %&gt;% \n  kable(\n    caption = \"Results of a generalize linear mixed model (glmm)\",\n    booktabs = TRUE) %&gt;%\n  kable_styling(font_size = 8,  # Set a smaller font size\n                latex_options = c(\"scale_down\")) # Scale down the table\n\n\n\n\nTable 20: Results of a generalize linear mixed model (glmm)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nSDT_term\nSDT_estimate\n\n\n\n\n(Intercept)\n-0.514\n0.032\n-16.175\n0.000\nAverage c (pooled across all conditions)\n0.514\n\n\nveracity_numeric\n0.786\n0.062\n12.711\n0.000\nAverage d' (pooled across all conditions)\n0.786\n\n\ncondition_numeric\n-0.119\n0.063\n-1.892\n0.059\nDelta c (change in -response bias between control and treatment)\n0.119\n\n\nveracity_numeric:condition_numeric\n-0.285\n0.123\n-2.312\n0.021\nDelta d' (change in sensitivity between control and treatment)\n-0.285\n\n\nsd__(Intercept)\n0.109\nNA\nNA\nNA\nOther\n0.109\n\n\ncor__(Intercept).veracity_numeric\n1.000\nNA\nNA\nNA\nOther\n1.000\n\n\nsd__veracity_numeric\n0.090\nNA\nNA\nNA\nOther\n0.090",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "preregistration/preregistration.html#footnotes",
    "href": "preregistration/preregistration.html#footnotes",
    "title": "Pre-data collection registration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLevel 1 being the the participant level, i.e. the sampling variation of the original studies, see Harrer et al. (2021).↩︎\nvisible only in the .html version↩︎",
    "crumbs": [
      "Preregistration"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi!",
    "section": "",
    "text": "This is the project website for our paper: “How effective are interventions designed to help people detect misinformation?”"
  },
  {
    "objectID": "presentations/presentations.html",
    "href": "presentations/presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "How effective are interventions designed to help people detect misinformation?\n\n\n\n\n\nIntroducing the project at the 15th Annual Conference of the European Political Science Association, Universidad Carlos III de Madrid, Spain.\n\n\n\n\n\nJun 27, 2025\n\n\nJan Pfänder & Sacha Altay\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/list_studies.html",
    "href": "data/list_studies.html",
    "title": "Cleaning Scripts",
    "section": "",
    "text": "Note\n\n\n\nNote that we only list the first author in this overview.\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\nAuthor\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nAltay, Sacha\n\n\n2024\n\n\nPeople Are Skeptical of Headlines Labeled as AI-Generated, Even If True or Human-Made, Because They Assume Full AI Automation.\n\n\n\n\nBadrinathan, Sumitra\n\n\n2021\n\n\nEducative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.\n\n\n\n\nBago, Bence\n\n\n2022\n\n\nEmotion May Predict Susceptibility to Fake News but Emotion Regulation Does Not Seem to Help.\n\n\n\n\nBrashier, Nadia M.\n\n\n2021\n\n\nTiming Matters When Correcting Fake News.\n\n\n\n\nClayton, Katherine\n\n\n2020\n\n\nReal Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.\n\n\n\n\nDias, Nicholas\n\n\n2020\n\n\nEmphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media.\n\n\n\n\nGuess, Andrew M.\n\n\n2020\n\n\nA Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Data",
      "Cleaning Scripts"
    ]
  },
  {
    "objectID": "data/papers/badrinathan_2021/badrinathan_2021.html",
    "href": "data/papers/badrinathan_2021/badrinathan_2021.html",
    "title": "Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.",
    "section": "",
    "text": "Badrinathan, Sumitra. 2021. “Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.” American Political Science Review 115 (4): 1325–41. https://doi.org/10.1017/S0003055421000459."
  },
  {
    "objectID": "data/papers/badrinathan_2021/badrinathan_2021.html#reference",
    "href": "data/papers/badrinathan_2021/badrinathan_2021.html#reference",
    "title": "Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.",
    "section": "",
    "text": "Badrinathan, Sumitra. 2021. “Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.” American Political Science Review 115 (4): 1325–41. https://doi.org/10.1017/S0003055421000459."
  },
  {
    "objectID": "data/papers/badrinathan_2021/badrinathan_2021.html#intervention",
    "href": "data/papers/badrinathan_2021/badrinathan_2021.html#intervention",
    "title": "Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'The study tested the effect of a literacy intervention with two slightly different variants. At first, all participants in the treatment group received the following : \"Pedagogical intervention: Next, respondents went through a learning module to help inoculate against misinformation. This included an hour-long discussion on encouraging people to verify information along with concrete tools to do so.\" The difference between the two treatment groups was in some of the materials they were presented with: \"Both treatment groups received the pedagogical intervention. However, one group received corrections to four pro-BJP false stories and the other received corrections to four anti-BJP false stories. Besides differences in the stories that were fact-checked, the tips on the flyer remained the same for both treatment groups. The author pooled the conditions in their study as well as in the data we have at hand. We therefor assign a single label (`intervention_label` = \"literacy\") for the treatment.',\n    intervention_selection = NA,\n    intervention_selection_description = 'The author pooled both treatment groups together, after not having found differences in the treatment effect between the two. We follow the author, mostly because both treatment conditions seem sufficiently similar.',\n    # the author measured detection of misinformation, not discernment  \n    originally_identified_treatment_effect = NA,\n    control_format = \"picture\")\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\nintervention_selection_description\n\n\n\n\nThe study tested the effect of a literacy intervention with two slightly different variants. At first, all participants in the treatment group received the following : \"Pedagogical intervention: Next, respondents went through a learning module to help inoculate against misinformation. This included an hour-long discussion on encouraging people to verify information along with concrete tools to do so.\" The difference between the two treatment groups was in some of the materials they were presented with: \"Both treatment groups received the pedagogical intervention. However, one group received corrections to four pro-BJP false stories and the other received corrections to four anti-BJP false stories. Besides differences in the stories that were fact-checked, the tips on the flyer remained the same for both treatment groups. The author pooled the conditions in their study as well as in the data we have at hand. We therefor assign a single label (`intervention_label` = \"literacy\") for the treatment.\nThe author pooled both treatment groups together, after not having found differences in the treatment effect between the two. We follow the author, mostly because both treatment conditions seem sufficiently similar.\n\n\n\n\n\n\n\n\nNotes\nFrom an e-mail exchange with the author, we know that true news were selected from “mainstream as well as fact checking sites” and that news were presented in format of “headline + image in some cases”.\nThe author measured detection of misinformation, not discernment. The outcome was only based on the misinformation items.\n\n“Receiving this hour-long media literacy intervention did not significantly increase respondents’ ability to identify misinformation on average.”"
  },
  {
    "objectID": "data/papers/badrinathan_2021/badrinathan_2021.html#data-cleaning",
    "href": "data/papers/badrinathan_2021/badrinathan_2021.html#data-cleaning",
    "title": "Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nRead data.\n\n\nCode\nd &lt;- read_csv(\"badrinathan_2021.csv\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 1224 Columns: 339\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (22): po_name, vill_tow_name, near_place, enu_name, sup_name, q12_oth,...\ndbl  (278): Serial.No., DeviceId.x, po_code, Newpo_code, po_pri_nu, enu_code...\nlgl   (37): spd_bn, spd_res_name, spd_vill, spd_emu_info, gp_inf_cons, disp_...\ndttm   (2): StartTime.x, StartTime.y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\naccuracy_raw, scale, veracity, Conditions (intervention_label, condition)\nFrom the cleaning document of the author, we can deduce which variables correspond to false and which to true news items.\nThe false news items were:\ndv1: CCTV : ‘cctv’ dv2: no terror attacks : ‘attacks’ dv3 : pulwama fake photos : ‘pulwama’ dv4 : ganga fake photos : ‘ganga’ dv5 : fake plastic finger : ‘plastic’ dv6 : soldier : ‘soldier’ dv7 : gomutra : ‘gomutra’ dv8 : rally : ‘rally’ dv9 : child kidnap : ‘kidnap_dv’ dv10 : 2000 note : ‘note’ dv11 : patel statue : ‘patel’ dv12 : flag on statue of liberty : ‘flag’ dv13 : evm hacking : ‘evm’\nThe true news items were:\ndv14: man ki baat: ‘true1’ dv15: pulwama : ‘true1’\nNote that variables are coded 1 for correct responses (i.e. 1 corresponds to ‘not accurate’ for false news, and to ‘accurate’ for true news).\nThe data set does not distinguish between the two slightly different treatment groups, as the author pooled them for her analysis.\n\n\nCode\n# bring data to long format and recode variables\nlong_d &lt;- d %&gt;% \n  pivot_longer(c(attacks, pulwama, ganga, plastic, soldier, gomutra, rally, \n                 kidnap_dv, note, patel, flag, evm, true1, true2), \n               names_to = \"item\",\n               values_to = \"ratings\") %&gt;% \n  # make an binary 'veracity' variable identifying true and fake\n  mutate(veracity = ifelse(grepl('true', item), 'true', 'false'), \n         # make condition a factor\n         condition = recode_factor(treatment, `0` = \"control\", `1` = \"treatment\"), \n         # add an intervention label\n         intervention_label = ifelse(condition == \"treatment\", \"literacy\", NA),\n         # recode accuracy responses for fake news\n         # so that 1 = rated as accurate (just as is measured for true news)\n         accuracy_raw = ifelse(veracity == 'false', \n                                  ifelse(ratings == 1, 0, 1), \n                                  ratings), \n         scale = \"binary\"\n         )\n\n# check\n# long_d %&gt;% \n#   group_by(item, veracity, treatment, condition) %&gt;% \n#   summarize(n = n(), \n#             mean_rating = mean(ratings, na.rm=TRUE),\n#             mean_accuracy= mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nnews_id, news_selection\nIn the previous section, we have already created a news identifier item. Here we just rename this identifier.\n\n\nCode\nlong_d &lt;- long_d |&gt; \n  mutate(news_id = item, \n         news_selection = \"researchers\")\n\n\n\n\nConcordance (concordance, partisan_identity, news_slant)\nWhile political concordance is not explicitly coded, we can build it from participants’ political leaning and the political slant of the news items.\nFrom the cleaning document, we know that participants’ party id (i.e. pro or contra the governing party) is coded by the variable ‘BJP’. The problem is: we don’t know what level (0,1) corresponds to which id. By replicating figure 5 (with the replication file accessible online) from the paper, we figured out that 0 = non BJP and 1 = BJP.\nWe also know which fake news items are pro-BJP (gomutra, attacks, pulwama, soldier, flag, note) and which fake news items are anti_BJP (cctv, evm, ganga, kidnap_dv, plastic, patel). Regarding true news, combining the cleaning document and the supplement (table D.1), we know that true1 (man ki baat) = pro BJP; and true2 (pulwama) = anti BJP.\n\n\nCode\npro_BJP &lt;- c(\"gomutra\", \"attacks\", \"pulwama\", \"soldier\", \"flag\", \"note\", \"true1\")\n\nlong_d &lt;- long_d %&gt;% \n  # make a binary variable indicating political slant of news\n  mutate(news_slant = ifelse(item %in% pro_BJP, \"pro_BJP\", \"anti_BJP\"),\n         # make a clearer party id variable\n         partisan_identity = recode_factor(BJP, `0` = \"non_BJP\", `1` = \"BJP\"),\n         # combine party id and political slant \n         concordance = case_when(news_slant == \"pro_BJP\" & partisan_identity == \"BJP\" ~ \"concordant\",\n                                 news_slant == \"anti_BJP\" & partisan_identity == \"non_BJP\" ~ \"concordant\", \n                                 TRUE ~ \"discordant\")\n  )\n\n# check \n# long_d %&gt;% select(political_slant, party_id, concordance)\n\n\n\n\nsubject_id\n\n\nCode\n# check id \nnrow(d) # n participants (one row per participant)\n\n\n[1] 1224\n\n\nCode\nn_distinct(long_d$Serial.No.) # likely the correct variable\n\n\n[1] 1224\n\n\nCode\nlong_d %&gt;% \n  group_by(Serial.No) %&gt;% \n  count() # second check, 14 observations\n\n\n# A tibble: 1,224 × 2\n# Groups:   Serial.No [1,224]\n   Serial.No     n\n       &lt;dbl&gt; &lt;int&gt;\n 1         1    14\n 2         2    14\n 3         4    14\n 4         9    14\n 5        10    14\n 6        12    14\n 7        13    14\n 8        14    14\n 9        16    14\n10        17    14\n# ℹ 1,214 more rows\n\n\n\n\nCode\nlong_d &lt;- long_d |&gt; \n  mutate(subject_id = Serial.No.)\n\n\n\n\nage\n\n\nCode\n# check age \ntable(long_d$age, useNA = \"always\")\n\n\n\n  18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33 \n2086 1330 1680 1330 1204  784  770 1148  812  364  616  210  854  112  448  154 \n  34   35   36   37   38   39   40   41   42   43   44   45   48   49   50   51 \n 196  462  154   70  294   42  546   56  182   28   42  322  196   14  196   28 \n  52   53   55   58   59   61   62   64   65   68   85 &lt;NA&gt; \n 112   42   84   28   42   14   14   14   28   14   14    0 \n\n\n\n\nIdentifiers (country, paper_id, experiment_id)\n\n\nCode\n# make final data\nbadrinathan_2021 &lt;- long_d %&gt;% \n  mutate(\n    experiment_id = 1,\n    country = \"India\",\n    paper_id = \"badrinathan_2021\") |&gt; \n  # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  # reduce to target variables\n  select(any_of(target_variables))\n\n\n\n\nWrite out data\n\n\nCode\nsave_data(badrinathan_2021)"
  },
  {
    "objectID": "data/papers/guess_2020/guess_2020.html",
    "href": "data/papers/guess_2020/guess_2020.html",
    "title": "A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.",
    "section": "",
    "text": "Guess, Andrew M., Michael Lerner, Benjamin Lyons, Jacob M. Montgomery, Brendan Nyhan, Jason Reifler, and Neelanjan Sircar. 2020. “A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.” Proceedings of the National Academy of Sciences 117 (27): 15536–45. https://doi.org/10.1073/pnas.1920498117."
  },
  {
    "objectID": "data/papers/guess_2020/guess_2020.html#reference",
    "href": "data/papers/guess_2020/guess_2020.html#reference",
    "title": "A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.",
    "section": "",
    "text": "Guess, Andrew M., Michael Lerner, Benjamin Lyons, Jacob M. Montgomery, Brendan Nyhan, Jason Reifler, and Neelanjan Sircar. 2020. “A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.” Proceedings of the National Academy of Sciences 117 (27): 15536–45. https://doi.org/10.1073/pnas.1920498117."
  },
  {
    "objectID": "data/papers/guess_2020/guess_2020.html#intervention",
    "href": "data/papers/guess_2020/guess_2020.html#intervention",
    "title": "A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = 'In both studies, participants were randomly assigned either to a control group, or a media literacy intervention group. In the intervention group, participants would read general tips on how detect misinformation (e.g.: \"Some stories are intentionally false. Think critically about the stories you read, and only share news that you know to be credible.\")',\n    intervention_selection = \"literacy\",\n    originally_identified_treatment_effect = TRUE\n      )\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\n\n\n\n\nIn both studies, participants were randomly assigned either to a control group, or a media literacy intervention group. In the intervention group, participants would read general tips on how detect misinformation (e.g.: \"Some stories are intentionally false. Think critically about the stories you read, and only share news that you know to be credible.\")\n\n\n\n\n\n\n\n\nNotes\nThis is a two-wave study. In the US, the same items have been used in Wave 1 and 2. Therefore, only Wave 1 is relevant. In India, however, two different sets of items have been used, making both waves relevant.\n\n“Unlike the US study (where the same headlines were used in both waves 1 and 2 to test for prior exposure effects), we used different sets of headlines in each wave.”\n\nThe format was different in the US studies, they used a face-book like format, but no lede (headline, picture and source). In India there was no picture in India:\n\nRespondents were presented with the headline in text format in the online survey, while enumerators read the headlines to respondents in the face-to-face survey.\n\nIt is ambiguous whether sources were shown or not, but not plausible at least in the face-to-face condition. We will, in doubt, code no source.\nThe authors identify a treatment effect on discernment:\n\n“Strikingly, our results indicate that exposure to variants of the Facebook media literacy intervention reduces people’s belief in false headlines. These effects are not only an artifact of greater skepticism toward all information—although the perceived accuracy of mainstream news headlines slightly decreased, exposure to the intervention widened the gap in perceived accuracy between mainstream and false news headlines overall.”"
  },
  {
    "objectID": "data/papers/guess_2020/guess_2020.html#data-cleaning",
    "href": "data/papers/guess_2020/guess_2020.html#data-cleaning",
    "title": "A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nStudy 1 (United States)\n\n\nCode\nd &lt;- read_dta(\"guess_2020-US.dta\")\nhead(d)\n\n\n# A tibble: 6 × 511\n     caseid weight headline_1_article_name          headline_2_article_name     \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+lbl&gt;                        &lt;dbl+lbl&gt;                   \n1 389941943  0.768 12 [and_now_its_the_tallest_png]  1 [donald_trump_caught_png]\n2 395525876  0.988  2 [franklin_graham_png]         13 [google_employees_png]   \n3 397441725  0.965 13 [google_employees_png]         1 [donald_trump_caught_png]\n4 403011277  0.803 13 [google_employees_png]         2 [franklin_graham_png]    \n5 419842674  1.44  16 [economy_adds_more_png]        6 [kavanaugh_accuser_png]  \n6 422239823  0.578  8 [lisa_page_png]               12 [and_now_its_the_tallest…\n# ℹ 507 more variables: headline_3_article_name &lt;dbl+lbl&gt;,\n#   headline_4_article_name &lt;dbl+lbl&gt;, headline_5_article_name &lt;dbl+lbl&gt;,\n#   headline_6_article_name &lt;dbl+lbl&gt;, headline_7_article_name &lt;dbl+lbl&gt;,\n#   headline_8_article_name &lt;dbl+lbl&gt;, instructions_treat &lt;dbl+lbl&gt;,\n#   consent &lt;dbl+lbl&gt;, inputstate &lt;dbl+lbl&gt;, ideo &lt;dbl+lbl&gt;, pid3 &lt;dbl+lbl&gt;,\n#   pid3_t &lt;chr&gt;, pid7 &lt;dbl+lbl&gt;, pol_interest &lt;dbl+lbl&gt;,\n#   trump_approve &lt;dbl+lbl&gt;, pol_therm_dem &lt;dbl+lbl&gt;, …\n\n\n\nveracity, accuracy_raw, news_id, news_slant\nThere is a ton of candidate variables. The best shot, since we don’t know abou the veracity of the variables yet, is to take the named ones.\n\n\nCode\n# accuracy ratings by headline\nheadlines &lt;- c(\n  \"accuracy_donald_trump_caught\",     # Pro-D hyperpartisan\n  \"accuracy_franklin_graham\",         # Pro-D hyperpartisan\n  \"accuracy_vp_mike_pence\",           # Pro-D false\n  \"accuracy_vice_president_pence\",    # Pro-D false\n  \"accuracy_soros_money_behind\",      # Pro-R hyperpartisan\n  \"accuracy_kavanaugh_accuser\",       # Pro-R hyperpartisan\n  \"accuracy_fbi_agent_who\",           # Pro-R false\n  \"accuracy_lisa_page\",               # Pro-R false\n  \"accuracy_a_series1\",               # Pro-D mainstream (low)\n  \"accuracy_a_border_patrol\",         # Pro-D mainstream (low)\n  \"accuracy_detention_of_migrant\",    # Pro-D mainstream (high)\n  \"accuracy_and_now1\",                # Pro-D mainstream (high)\n  \"accuracy_google_employees\",        # Pro-R mainstream (low)\n  \"accuracy_feds_said_alleged\",       # Pro-R mainstream (low)\n  \"accuracy_small_busisness_opt\",     # Pro-R mainstream (high)\n  \"accuracy_economy_adds_more\"        # Pro-R mainstream (high)\n)\n\n# check\ntable(d$accuracy_donald_trump_caught, useNA = \"always\")\n\n\n\n   1    2    3    4 &lt;NA&gt; \n1221  640  370  196 2480 \n\n\nThe fact that the same variables exist with the suffix “w2”, suggests that these are containing the ratings for Wave 1 that we are looking for.\nThe next step is to qualitatively match the rather cryptic variable names to the headlines given in the appendix:\n\nPro-Democrat Hyperpartisan News\nPro-D hyper 1: Donald Trump caught privately wishing he’d sided more thoroughly with white supremacists. (accuracy_donald_trump_caught)\nPro-D hyper 2: Franklin Graham: Attempted rape not a crime. Kavanaugh ‘respected’ his victim by not finishing. (accuracy_franklin_graham)\nPro-Democrat False News\nPro-D false 1: VP Mike Pence Busted Stealing Campaign Funds To Pay His Mortgage Like A Thief. (accuracy_vp_mike_pence)\nPro-D false 2: Vice President Pence now being investigated for campaign fraud, his ties to Russia and Manafort. (accuracy_vice_president_pence)\nPro-Republican Hyperpartisan News\nPro-R hyper 1: Soros Money Behind ‘Black Political Power’ Outfit Supporting Andrew Gillum in Florida. (accuracy_soros_money_behind)\nPro-R hyper 2: Kavanaugh Accuser Christine Blasey Exposed For Ties To Big Pharma Abortion Pill Maker. Effort To Derail Kavanaugh Is Plot To Protect Abortion Industry Profits. (accuracy_kavanaugh_accuser)\nPro-Republican False News\nPro-R false 1: Special Agent David Raynor was due to testify against Hillary Clinton when he died. (accuracy_fbi_agent_who)\nPro-R false 2: Lisa Page Squeals: DNC Server Was Not Hacked By Russia. (accuracy_lisa_page)\nMainstream News Congenial to Democrats (Low-Prominence Source)\nPro-D Mainstream 1: A Series Of Suspicious Money Transfers Followed The Trump Tower Meeting. (accuracy_a_series1)\nPro-D Mainstream 2: A Border Patrol Agent Has Been Called a ‘Serial Killer’ by Police After Murdering 4 Women. (accuracy_a_border_patrol)\nMainstream News Congenial to Democrats (High-Prominence Source)\nPro-D Mainstream 3: Detention of Migrant Children Has Skyrocketed to Highest Levels Ever. (accuracy_detention_of_migrant)\nPro-D Mainstream 4: ‘And now it’s the tallest’: Trump, in otherwise sombre 9/11 interview, couldn’t help touting one of his buildings. (accuracy_and_now1)\nMainstream News Congenial to Republicans (Low-Prominence Source)\nPro-R Mainstream 1: Google Workers Discussed Tweaking Search Function to Counter Travel Ban. (accuracy_google_employees)\nPro-R Mainstream 2: Feds said alleged Russian spy Maria Butina used sex for influence. Now, they’re walking that back. (accuracy_feds_said_alleged)\nMainstream News Congenial to Republicans (High-Prominence Source)\nPro-R Mainstream 3: Small business optimism surges to highest level ever, topping previous record under Reagan. (accuracy_small_busisness_opt)\nPro-R Mainstream 4: Economy adds more jobs than expected in August, and wage growth hits postrecession high. (accuracy_economy_adds_more)\n\nWe first code a lookup table for headline name, veracity, and political slant.\n\n\nCode\n# Create a lookup table\nheadline_info &lt;- tibble(\n  news_id = headlines,\n  news_slant = c(\n    rep(\"democrat\", 4),  # first 4 are Pro-D\n    rep(\"republican\", 4),# next 4 are Pro-R\n    rep(\"democrat\", 4),  # next 4 are Pro-D\n    rep(\"republican\", 4) # last 4 are Pro-R\n  ),\n  veracity = c(\n    \"hyperpartisan\", \"hyperpartisan\", \"false\", \"false\",     # Pro-D\n    \"hyperpartisan\", \"hyperpartisan\", \"false\", \"false\",     # Pro-R\n    \"true\", \"true\", \"true\", \"true\",                         # Pro-D mainstream\n    \"true\", \"true\", \"true\", \"true\"                          # Pro-R mainstream\n  )\n)\n\n\nWe then reshape the data and add veracity and news slant based on the lookup table.\n\n\nCode\n# Now pivot and join\nd_long &lt;- d |&gt; \n  pivot_longer(\n    cols = all_of(headlines),\n    names_to = \"news_id\",\n    values_to = \"accuracy_raw\"\n  ) |&gt; \n  left_join(headline_info, by = \"news_id\") |&gt; \n  # remove the \"accuracy_\" prefix\n  mutate(news_id = sub(\"^accuracy_\", \"\", news_id))\n\n# check\n# d_long |&gt; \n#   group_by(news_id, veracity, news_slant) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n# plausibility check\n# d_long |&gt; \n#   group_by(veracity) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n\nWe remove the hyperpartisan items.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  filter(veracity != \"hyperpartisan\")\n\n\n\n\nConditions (intervention_label, condition)\nFrom an e-mail exchange with the first author, we know that the treatment variable is tips, where ‘0’ corresponds to control and ‘1’ corresponds to the literacy intervention.\n\n\nCode\ntable(d_long$tips)\n\n\n\n    0     1 \n29436 29448 \n\n\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(condition = ifelse(tips == 0, \"control\", \"treatment\"), \n         intervention_label = \"literacy\"\n         )\n\n\n\n\nscale\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(scale = 4)\n\n\n\n\nnews_selection\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(news_selection = \"researchers\")\n\n\n\n\nage\nThere is already an age variable.\n\n\nCode\ntable(d_long$age)\n\n\n\n  18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33 \n 552  456  708  828  720  660  792  912  996 1068 1212 1140  816  924 1164 1128 \n  34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49 \n 936 1128 1068 1020 1152 1152 1140  816  864  828 1008  660  564  768  984 1020 \n  50   51   52   53   54   55   56   57   58   59   60   61   62   63   64   65 \n 840  984  888  936 1104 1224 1356 1104 1116 1248 1392 1368 1188 1356 1152 1200 \n  66   67   68   69   70   71   72   73   74   75   76   77   78   79   80   81 \n1080 1056 1128  792 1092  816  804  600  540  576  588  420  456  300  216  192 \n  82   83   84   85   87   88   89   90   93 \n 156   96   96   84   48   48   36   12   12 \n\n\n\n\nyear\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = year(ymd_hms(starttime)))\n\n# check\n# d_long |&gt; \n#   select(starttime, year)\n\n# check\ntable(d_long$year)\n\n\n\n 2018 \n58884 \n\n\n\n\nConcordance (concordance, partisan_identity)\nCheck the value labels\n\n\nCode\nval_labels(d$pid3)\n\n\n   Democrat  Republican Independent       Other    Not sure     skipped \n          1           2           3           4           5           8 \n  not asked \n          9 \n\n\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(partisan_identity = tolower(as_factor(pid3)),\n         # make everything that is not democrat or republican NA\n         partisan_identity = ifelse(partisan_identity %in% c(\"democrat\", \"republican\"), \n                                    partisan_identity, \n                                    NA),\n         # Make concordance variable\n         concordance = ifelse(partisan_identity == news_slant, \"concordant\", \"discordant\")\n  )\n\n# check\nd_long |&gt; \n  select(partisan_identity, pid3, news_slant, concordance)\n\n\n# A tibble: 58,884 × 4\n   partisan_identity pid3            news_slant concordance\n   &lt;chr&gt;             &lt;dbl+lbl&gt;       &lt;chr&gt;      &lt;chr&gt;      \n 1 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 2 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 3 &lt;NA&gt;              3 [Independent] republican &lt;NA&gt;       \n 4 &lt;NA&gt;              3 [Independent] republican &lt;NA&gt;       \n 5 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 6 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 7 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 8 &lt;NA&gt;              3 [Independent] democrat   &lt;NA&gt;       \n 9 &lt;NA&gt;              3 [Independent] republican &lt;NA&gt;       \n10 &lt;NA&gt;              3 [Independent] republican &lt;NA&gt;       \n# ℹ 58,874 more rows\n\n\n\n\nIdentifiers (subject_id, experiment_id, country) and control_format\nCheck candidate variable for subject identifier.\n\n\nCode\nn_distinct(d_long$caseid)\n\n\n[1] 4907\n\n\nThis corresponds to the number reported in the paper.\n\n\nCode\nd1 &lt;- d_long |&gt; \n  mutate(subject_id = caseid, \n         experiment_id = 1, \n         country = \"United States\",\n         control_format = \"picture, source\")\n\n\n\n\n\nStudy 2 (India, face-to-face)\n\n\nCode\nd &lt;- read_dta(\"guess_2020-India_facetoface.dta\")\nhead(d)\n\n\n# A tibble: 6 × 89\n  survey_date  caseid `_11_gaccuracy`  male low_caste college whatsapp\n  &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Apr 24, 2019   1001            3        1         1       0        0\n2 Apr 24, 2019   1002            5        1         1       0        0\n3 Apr 24, 2019   1003            3        1         1       0        0\n4 Apr 24, 2019   1004            4.10     1         1       0        0\n5 Apr 24, 2019   1005            3        0         1       0        0\n6 Apr 24, 2019   1006            5        1         1       0        0\n# ℹ 82 more variables: days_whatsapp &lt;dbl&gt;, hindu &lt;dbl&gt;, muslim &lt;dbl&gt;,\n#   tips &lt;dbl&gt;, placebo_assigned &lt;dbl&gt;, factcheck_assigned &lt;dbl&gt;,\n#   placebo_factcheck &lt;dbl&gt;, BSP_feelings &lt;dbl&gt;, BJP_feelings &lt;dbl&gt;,\n#   INC_feelings &lt;dbl&gt;, SP_feelings &lt;dbl&gt;, bjp_support &lt;dbl&gt;, bjp_oppose &lt;dbl&gt;,\n#   accuracy_modi_stone &lt;dbl&gt;, accuracy_gandhi_pune &lt;dbl&gt;,\n#   accuracy_india_jobs &lt;dbl&gt;, accuracy_congress_riots &lt;dbl&gt;,\n#   accuracy_modi_kumbh &lt;dbl&gt;, accuracy_congress_pakistan &lt;dbl&gt;, …\n\n\n\nveracity, accuracy_raw, news_id, news_slant\nFor the studies in India, we know that:\n\nFinally, 4 additional false headlines were included in the second wave based on fact checks conducted between the two waves. In total, respondents rated 12 headlines in wave 1 (6 false and 6 true) and 16 in wave 2 (10 false and 6 true).\n\nWe also know that (appendix):\n\nBoth Wave 1 and Wave 2 included both mainstream and false headlines that were either congenial to Bharatiya Janata Party (BJP) supporters or congenial to BJP opponents as well as headlines pertaining to nationalism issues (either India-Pakistan or HinduMuslim relations).\n\nWe have matched cryptic variable names and news headlines in an external .csv. In the documentation of the study, it is not exactly clear, but “FTF” and “MTurk” characterize likely four different items of the second wave, the former being used in the face-to-face, the latter being used in the online survey.\n\nFinally, 4 additional false headlines were included in the second wave based on fact checks conducted between the two waves.\n\n\n\nCode\nheadline_info &lt;- read_delim(\"india_headlines.csv\", delim = \";\")\n\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): news_id, headline, news_slant\ndbl (1): wave\nlgl (1): veracity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(headline_info)\n\n\n# A tibble: 6 × 5\n  news_id                    headline                  news_slant veracity  wave\n  &lt;chr&gt;                      &lt;chr&gt;                     &lt;chr&gt;      &lt;lgl&gt;    &lt;dbl&gt;\n1 accuracy_modi_stone        Modi lays foundation sto… Pro-BJP    TRUE         1\n2 accuracy_gandhi_pune       Rahul Gandhi greeted wit… Pro-BJP    TRUE         1\n3 accuracy_india_jobs        Govt tried to suppress d… Anti-BJP   TRUE         1\n4 accuracy_congress_riots    Study: More riots if Con… Anti-BJP   TRUE         1\n5 accuracy_modi_kumbh        Modi first head of state… Pro-BJP    FALSE        1\n6 accuracy_congress_pakistan Congress workers chant “… Pro-BJP    FALSE        1\n\n\nWe then reshape the data and add veracity and news slant based on the lookup table.\n\n\nCode\n# Now pivot and join\nd_long &lt;- d |&gt; \n  pivot_longer(\n    cols = any_of(headline_info$news_id),\n    names_to = \"news_id\",\n    values_to = \"accuracy_raw\"\n  ) |&gt; \n  left_join(headline_info, by = \"news_id\") |&gt; \n  mutate(\n    # remove the \"accuracy_\" prefix\n    news_id = sub(\"^accuracy\", \"\", news_id), \n    # give correct veracity values\n    veracity = ifelse(veracity == TRUE, \"true\", \"false\")\n    )\n\n# check\n# d_long |&gt; \n#   group_by(news_id, veracity, news_slant) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n# plausibility check\n# d_long |&gt; \n#   group_by(veracity) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nlong_term, time_elapsed\nThe news ratings of the second wave are more distant in time, in order to evaluate long-term effects of the intervention. We therefore want to separate them (and not consider them in our main analyses).\nAs for the elapsed time between intervention and the follow-up evaluation, we know that:\n\n“In the online survey, we collected survey data from a national convenience sample of Hindi-speaking Indians recruited via Mechanical Turk and the Internet Research Bureau’s Online Bureau survey panels (wave 1, April 17 to May 1, 2019, N = 3, 273; wave 2, May 13 to 19, 2019, N = 1, 369).”\n\nWe calculate the average time between these.\n\n\nCode\n# May 16 − April 24 = 22 days\naverage_time_elapsed &lt;- 22\n\n\nCheck the different news_ids.\n\n\nCode\nd_long |&gt; \n  distinct(news_id)\n\n\n# A tibble: 28 × 1\n   news_id           \n   &lt;chr&gt;             \n 1 _modi_stone       \n 2 _gandhi_pune      \n 3 _india_jobs       \n 4 _congress_riots   \n 5 _modi_kumbh       \n 6 _congress_pakistan\n 7 _modi_court       \n 8 _blair_gandhi     \n 9 _iaf_pakistan     \n10 _water_pakistan   \n# ℹ 18 more rows\n\n\nLabel those containing “w2” as long term effect measures.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(\n    # use news_id labels as identifiers\n    long_term = ifelse(str_detect(news_id, \"w2\"), TRUE, FALSE), \n    time_elapsed = average_time_elapsed\n    )\n\n# check\ntable(d_long$long_term, useNA = \"always\")\n\n\n\nFALSE  TRUE  &lt;NA&gt; \n44928 59904     0 \n\n\n\n\nConditions (intervention_label, condition)\nFrom an e-mail exchange with the first author, we know that the treatment variable is tips, where ‘0’ corresponds to control and ‘1’ corresponds to the literacy intervention.\n\n\nCode\ntable(d_long$tips)\n\n\n\n    0     1 \n53704 51128 \n\n\nCode\n# plausbility check\n# d_long |&gt; \n#   group_by(tips, veracity) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(condition = ifelse(tips == 0, \"control\", \"treatment\"), \n         intervention_label = \"literacy\"\n         )\n\n\n\n\nscale\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(scale = 4)\n\n\n\n\nnews_selection\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(news_selection = \"researchers\")\n\n\n\n\nage\nThere is only an agegroup variable, but it is unclear what the groups correspond to. We therefor code no age variable.\n\n\nCode\ntable(d$agegroup)\n\n\n\n  1   2   3   4 \n819 839 517 306 \n\n\n\n\nyear\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = year(mdy(survey_date)))\n\n# check\n# d_long |&gt;\n#   select(survey_date, year)\n\n# check\ntable(d_long$year)\n\n\n\n  2008   2014   2015   2016   2018   2019   2024 \n    28     28    112     56     28 104524     28 \n\n\nSince there likely are coding issues, we’ll just put “2019”.\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = 2019)\n\n\n\n\nConcordance (concordance, partisan_identity)\nIt seems the two relevant variables for partisan support are bjp_support and bjp_oppose, with 0 meaning FALSE and 1 TRUE.\n\n\nCode\nd_long |&gt; \n  group_by(bjp_support, bjp_oppose) |&gt; \n  summarize(n = n_distinct(caseid))\n\n\n`summarise()` has grouped output by 'bjp_support'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 3\n# Groups:   bjp_support [2]\n  bjp_support bjp_oppose     n\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1           0          0  1022\n2           0          1   997\n3           1          0  1725\n\n\nWe make a single variable out of these, and match it with the news_slant variable.\n\n\nCode\ntable(d_long$news_slant)\n\n\n\n   Anti-BJP         FTF Nationalist     Pro-BJP \n      29952       14976       29952       29952 \n\n\n\n\nCode\nd_long&lt;- d_long %&gt;% \n  # make a binary variable indicating political slant of news\n  mutate(# make a clearer party id variable (goes from the most specific to the most general)\n         partisan_identity = case_when(bjp_support == 0 & bjp_oppose == 0 ~ NA_character_,\n                                       bjp_support == 0 ~ \"non_BJP\", \n                                       bjp_support == 1 ~ \"BJP\"),\n         # combine party id and political slant \n         concordance = case_when(news_slant == \"Pro-BJP\" & partisan_identity == \"BJP\" ~ \"concordant\",\n                                 news_slant == \"Anti-BJP\" & partisan_identity == \"non_BJP\" ~ \"concordant\", \n                                 news_slant == \"Pro-BJP\" & partisan_identity == \"non_BJP\" ~ \"discordant\",\n                                 news_slant == \"Anti-BJP\" & partisan_identity == \"BJP\" ~ \"discordant\", \n                                 TRUE ~ NA_character_)\n  )\n\n# check\n# d_long |&gt; \n#   select(partisan_identity, news_slant, concordance)\n\n\n\n\nIdentifiers (subject_id, experiment_id, country) and control_format\nCheck candidate variable for subject identifier.\n\n\nCode\nn_distinct(d_long$caseid)\n\n\n[1] 3744\n\n\nThis corresponds to the number reported in the paper.\n\n\nCode\nd2 &lt;- d_long |&gt; \n  mutate(subject_id = caseid, \n         experiment_id = 2, \n         country = \"India\")\n\n\n\n\n\nStudy 3 (India, online)\n\n\nCode\nd &lt;- read_dta(\"guess_2020-India_online.dta\")\nhead(d)\n\n\n# A tibble: 6 × 96\n  StartDate           EndDate             ResponseId mid    tips  male low_caste\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 2019-04-30 04:25:24 2019-04-30 04:31:01 R_273QTwi… A101…     0     1         1\n2 2019-04-18 11:34:34 2019-04-18 11:43:15 R_2uIkP3i… A10C…     0     1         1\n3 2019-04-22 00:10:06 2019-04-22 01:23:20 R_241K9Cw… A10N…     1     0         0\n4 2019-04-21 21:57:06 2019-04-21 22:36:15 R_27I1VPP… A10S…     1     1         1\n5 2019-04-22 21:58:32 2019-04-22 22:10:55 R_3QMGscu… A10Z…     0     1         0\n6 2019-04-23 00:02:35 2019-04-23 00:33:28 R_3O6I9VC… A112…     1     1         0\n# ℹ 89 more variables: college &lt;dbl&gt;, hindu &lt;dbl&gt;, muslim &lt;dbl&gt;,\n#   whatsapp &lt;dbl&gt;, days_whatsapp &lt;dbl&gt;, birthyear &lt;dbl&gt;, BSP_feelings &lt;dbl&gt;,\n#   BJP_feelings &lt;dbl&gt;, INC_feelings &lt;dbl&gt;, SP_feelings &lt;dbl&gt;,\n#   bjp_support &lt;dbl&gt;, bjp_oppose &lt;dbl&gt;, pure_control &lt;dbl&gt;,\n#   placebo_assigned &lt;dbl&gt;, factcheck_assigned &lt;dbl&gt;,\n#   control_placebo_factcheck &lt;dbl&gt;, accuracy_modi_stone &lt;dbl&gt;,\n#   accuracy_gandhi_pune &lt;dbl&gt;, accuracy_india_jobs &lt;dbl&gt;, …\n\n\n\nveracity, accuracy_raw, news_id, news_slant\nWe proceed as previously in Study 2, since the headlines are the same.\n\n\nCode\nheadline_info &lt;- read_delim(\"india_headlines.csv\", delim = \";\")\n\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): news_id, headline, news_slant\ndbl (1): wave\nlgl (1): veracity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(headline_info)\n\n\n# A tibble: 6 × 5\n  news_id                    headline                  news_slant veracity  wave\n  &lt;chr&gt;                      &lt;chr&gt;                     &lt;chr&gt;      &lt;lgl&gt;    &lt;dbl&gt;\n1 accuracy_modi_stone        Modi lays foundation sto… Pro-BJP    TRUE         1\n2 accuracy_gandhi_pune       Rahul Gandhi greeted wit… Pro-BJP    TRUE         1\n3 accuracy_india_jobs        Govt tried to suppress d… Anti-BJP   TRUE         1\n4 accuracy_congress_riots    Study: More riots if Con… Anti-BJP   TRUE         1\n5 accuracy_modi_kumbh        Modi first head of state… Pro-BJP    FALSE        1\n6 accuracy_congress_pakistan Congress workers chant “… Pro-BJP    FALSE        1\n\n\nWe then reshape the data and add veracity and news slant based on the lookup table.\n\n\nCode\n# Now pivot and join\nd_long &lt;- d |&gt; \n  pivot_longer(\n    cols = any_of(headline_info$news_id),\n    names_to = \"news_id\",\n    values_to = \"accuracy_raw\"\n  ) |&gt; \n  left_join(headline_info, by = \"news_id\") |&gt; \n  mutate(\n    # remove the \"accuracy_\" prefix\n    news_id = sub(\"^accuracy\", \"\", news_id), \n    # give correct veracity values\n    veracity = ifelse(veracity == TRUE, \"true\", \"false\")\n    )\n\n# check\nd_long |&gt;\n  group_by(news_id, veracity, news_slant) |&gt;\n  summarize(mean(accuracy_raw, na.rm=TRUE))\n\n\n`summarise()` has grouped output by 'news_id', 'veracity'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 28 × 4\n# Groups:   news_id, veracity [28]\n   news_id            veracity news_slant  `mean(accuracy_raw, na.rm = TRUE)`\n   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;                                    &lt;dbl&gt;\n 1 _blair_gandhi      false    Anti-BJP                                  2.09\n 2 _congress_pakistan false    Pro-BJP                                   2.21\n 3 _congress_riots    true     Anti-BJP                                  2.50\n 4 _gandhi_pune       true     Pro-BJP                                   2.67\n 5 _iaf_pakistan      true     Nationalist                               2.91\n 6 _india_jobs        true     Anti-BJP                                  2.61\n 7 _kashmir_hindu     false    Nationalist                               2.56\n 8 _modi_court        false    Anti-BJP                                  2.14\n 9 _modi_kumbh        false    Pro-BJP                                   2.62\n10 _modi_stone        true     Pro-BJP                                   2.81\n# ℹ 18 more rows\n\n\nCode\n# plausibility check\n# d_long |&gt; \n#   group_by(veracity) |&gt; \n#   summarize(mean(accuracy_raw, na.rm=TRUE))\n\n\n\n\nlong_term, time_elapsed\nThe news ratings of the second wave are more distant in time, in order to evaluate long-term effects of the intervention. We therefore want to separate them (and not consider them in our main analyses).\nAs for the elapsed time between intervention and the follow-up evaluation, we know that:\n\n“The India face-to-face survey was conducted by the polling firm Morsel in Barabanki, Bahraich, Domariyaganj, and Shrawasti, four parliamentary constituencies in the state of Uttar Pradesh where Hindi is the dominant language (wave 1, April 13 to May 2, 2019, N = 3, 744; wave 2, May 7 to 19, 2019, N = 2,695).”\n\nWe calculate the average time between these.\n\n\nCode\n# May 13, 2019 − April 22, 2019 = 21 days\naverage_time_elapsed &lt;- 21\n\n\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(\n    # use news_id labels as identifiers\n    long_term = ifelse(str_detect(news_id, \"w2\"), TRUE, FALSE), \n    time_elapsed = average_time_elapsed\n    )\n\n\n\n\nConditions (intervention_label, condition)\nFrom an e-mail exchange with the first author, we know that the treatment variable is tips, where ‘0’ corresponds to control and ‘1’ corresponds to the literacy intervention.\n\n\nCode\ntable(d_long$tips)\n\n\n\n    0     1 \n46312 45332 \n\n\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(condition = ifelse(tips == 0, \"control\", \"treatment\"), \n         intervention_label = \"literacy\"\n         )\n\n\n\n\nscale\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(scale = 4)\n\n\n\n\nnews_selection\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(news_selection = \"researchers\")\n\n\n\n\nage\nThere is only an agegroup variable, but it is unclear what the groups correspond to. We therefor code no age variable.\n\n\nCode\ntable(d$agegroup)\n\n\n\n   1    2    3    4 \n1564 1157  203  349 \n\n\n\n\nyear\n\n\nCode\nd_long &lt;- d_long |&gt; \n  mutate(year = year(ymd_hms(StartDate)))\n\n# check\n# d_long |&gt;\n#   select(StartDate, year)\n\n\n\n\nConcordance (concordance, partisan_identity)\nIt seems the two relevant variables for partisan support are bjp_support and bjp_oppose, with 0 meaning FALSE and 1 TRUE.\n\n\nCode\nd_long |&gt; \n  group_by(bjp_support, bjp_oppose) |&gt; \n  summarize(n = n_distinct(caseid))\n\n\n`summarise()` has grouped output by 'bjp_support'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 3\n# Groups:   bjp_support [2]\n  bjp_support bjp_oppose     n\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1           0          0  1003\n2           0          1   887\n3           1          0  1383\n\n\nWe make a single variable out of these, and match it with the news_slant variable.\n\n\nCode\ntable(d_long$news_slant)\n\n\n\n   Anti-BJP       MTurk Nationalist     Pro-BJP \n      26184       13092       26184       26184 \n\n\n\n\nCode\nd_long&lt;- d_long %&gt;% \n  # make a binary variable indicating political slant of news\n  mutate(# make a clearer party id variable (goes from the most specific to the most general)\n         partisan_identity = case_when(bjp_support == 0 & bjp_oppose == 0 ~ NA_character_,\n                                       bjp_support == 0 ~ \"non_BJP\", \n                                       bjp_support == 1 ~ \"BJP\"),\n         # combine party id and political slant \n         concordance = case_when(news_slant == \"Pro-BJP\" & partisan_identity == \"BJP\" ~ \"concordant\",\n                                 news_slant == \"Anti-BJP\" & partisan_identity == \"non_BJP\" ~ \"concordant\", \n                                 news_slant == \"Pro-BJP\" & partisan_identity == \"non_BJP\" ~ \"discordant\",\n                                 news_slant == \"Anti-BJP\" & partisan_identity == \"BJP\" ~ \"discordant\", \n                                 TRUE ~ NA_character_)\n  )\n\n# check\n# d_long |&gt;\n#   select(partisan_identity, news_slant, concordance)\n\n\n\n\nIdentifiers (subject_id, experiment_id, country) and control_format\nCheck candidate variable for subject identifier.\n\n\nCode\nn_distinct(d_long$caseid)\n\n\n[1] 3273\n\n\nThis corresponds to the number reported in the paper.\n\n\nCode\nd3 &lt;- d_long |&gt; \n  mutate(subject_id = caseid, \n         experiment_id = 3, \n         country = \"India\")\n\n\n\n\n\nCombine and add identifiers (paper_id)\nWe combine both studies.\n\n\nCode\n## Combine + add remaining variables\nguess_2020 &lt;- bind_rows(d1, d2, d3) |&gt; \n  mutate(paper_id = \"guess_2020\") |&gt; \n  # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n# check\nguess_2020 |&gt;\n  group_by(paper_id, experiment_id) |&gt;\n  summarize(n_observations = n())\n\n\n`summarise()` has grouped output by 'paper_id'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 3\n# Groups:   paper_id [1]\n  paper_id   experiment_id n_observations\n  &lt;chr&gt;              &lt;dbl&gt;          &lt;int&gt;\n1 guess_2020             1          58884\n2 guess_2020             2         104832\n3 guess_2020             3          91644\n\n\nSince in both Indian studies the same news have been used (with the same labels), we can just keep the labels in news_id.\n\nnews_selection"
  },
  {
    "objectID": "data/papers/guess_2020/guess_2020.html#write-out-data",
    "href": "data/papers/guess_2020/guess_2020.html#write-out-data",
    "title": "A Digital Media Literacy Intervention Increases Discernment Between Mainstream and False News in the United States and India.",
    "section": "Write out data",
    "text": "Write out data\n\n\nCode\nsave_data(guess_2020)"
  },
  {
    "objectID": "data/papers/clayton_2020/clayton_2020.html",
    "href": "data/papers/clayton_2020/clayton_2020.html",
    "title": "Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.",
    "section": "",
    "text": "Clayton, Katherine, Spencer Blair, Jonathan A. Busam, Samuel Forstner, John Glance, Guy Green, Anna Kawata, et al. 2020. “Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.” Political Behavior 42 (4): 1073–95. https://doi.org/10.1007/s11109-019-09533-0."
  },
  {
    "objectID": "data/papers/clayton_2020/clayton_2020.html#reference",
    "href": "data/papers/clayton_2020/clayton_2020.html#reference",
    "title": "Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.",
    "section": "",
    "text": "Clayton, Katherine, Spencer Blair, Jonathan A. Busam, Samuel Forstner, John Glance, Guy Green, Anna Kawata, et al. 2020. “Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.” Political Behavior 42 (4): 1073–95. https://doi.org/10.1007/s11109-019-09533-0."
  },
  {
    "objectID": "data/papers/clayton_2020/clayton_2020.html#intervention",
    "href": "data/papers/clayton_2020/clayton_2020.html#intervention",
    "title": "Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.",
    "section": "Intervention",
    "text": "Intervention\n\n\nCode\nintervention_info &lt;- tibble(\n    intervention_description = '\"The experiment used a 2× 3 between-subjects design that also includes a pure control group. Participants were randomly assigned with equal probability to a pure control group or to one of six experimental conditions (see Table 1). We manipulated whether participants were exposed to a general warning about misleading articles or not (middle column of Table 1). We also independently randomized noncontrols into one of three headline conditions: a condition in which no fact-checking tags were presented (first two rows of Table 1), a specific warning condition that included tags labeling articles as “Disputed” (second two rows of Table 1), and a specific warning condition in which they were instead labeled as “Rated false” (last two rows of Table 1).\"',\n    intervention_selection = \"false_no_warning\",\n    intervention_selection_description = \"We believe the warning tag intervention to be more interesting than the general warning intervention. We therefor use only conditions where there is NO general warning. We also discard the pure control group in which participants didn't read any news items. There are two types of warning tag interventions: One that labels false articles as 'disputed' and another that labels them as 'rated false'. We will merge these two conditions as they seem sufficiently similar to be part of a warning tag category.\",\n    control_selection = \"control_no_warning\",\n    control_selection_description = \"We decide to measure the effect of warning tag interventions. We use the control condition where there is NO general warning.\",\n    control_format = \"picture, lede\",\n    # the author measured detection of misinformation, not discernment  \n    originally_identified_treatment_effect = NA)\n\n# display\nshow_conditions(intervention_info)\n\n\n\n\n\nintervention_description\nintervention_selection_description\ncontrol_selection_description\n\n\n\n\n\"The experiment used a 2× 3 between-subjects design that also includes a pure control group. Participants were randomly assigned with equal probability to a pure control group or to one of six experimental conditions (see Table 1). We manipulated whether participants were exposed to a general warning about misleading articles or not (middle column of Table 1). We also independently randomized noncontrols into one of three headline conditions: a condition in which no fact-checking tags were presented (first two rows of Table 1), a specific warning condition that included tags labeling articles as “Disputed” (second two rows of Table 1), and a specific warning condition in which they were instead labeled as “Rated false” (last two rows of Table 1).\"\nWe believe the warning tag intervention to be more interesting than the general warning intervention. We therefor use only conditions where there is NO general warning. We also discard the pure control group in which participants didn't read any news items. There are two types of warning tag interventions: One that labels false articles as 'disputed' and another that labels them as 'rated false'. We will merge these two conditions as they seem sufficiently similar to be part of a warning tag category.\nWe decide to measure the effect of warning tag interventions. We use the control condition where there is NO general warning.\n\n\n\n\n\n\n\n\nNotes\nThe study tested the effect of a literacy intervention. For an overview of the labels we’ve assigned, see the intervention_label column in Table Table 1.\n\n\n\n\nTable 1: Table: Participant Counts by Tag and General Warning\n\n\n\n\n\n\nTag\nGeneral warning\nN\nintervention_label\n\n\n\n\nNone\nNo\n469\nNA\n\n\nNone\nYes\n424\nNA\n\n\n“Disputed”\nNo\n413\ndisputed_no_warning\n\n\n“Disputed”\nYes\n429\ndisputed_warning\n\n\n“Rated false”\nNo\n429\nfalse_no_warning\n\n\n“Rated false”\nYes\n397\nfalse_warning\n\n\nPure control\nNA\n433\nNA\n\n\n\n\n\n\n\n\n\n“In the pure control group, respondents were exposed to no images, no articles, no general warning, no tags, and no headlines, and proceeded directly to the questions measuring the outcome variable (discussed in the next section).” “In the general warning condition, participants were shown a message warning them about misleading articles and providing advice for identifying false information (see Online Appendix A for exact wording and design).”"
  },
  {
    "objectID": "data/papers/clayton_2020/clayton_2020.html#data-cleaning",
    "href": "data/papers/clayton_2020/clayton_2020.html#data-cleaning",
    "title": "Real Solutions for Fake News? Measuring the Effectiveness of General Warnings and Fact-Check Tags in Reducing Belief in False Stories on Social Media.",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nRead data.\n\n\nCode\n# import data \nload(\"clayton_2020.RData\")\n\n# by default, the data frame is called \"table\" \n# rename\n\ndata &lt;- table\n\n\n\nConditions (intervention_label, control_label, condition)\nWe first need to indentiy the different experimental conditions. This is a bit tricky because the documentation is bad.\n\n\nCode\n# cross-reading paper and stata code, these seem to be the relevant variables for condition\ndata %&gt;% \n  select(cond, nocorr_condition, disputed_condition, false_condition, \n                   flag_cond, purecontrol, warning, nowarning)\n\n\n# A tibble: 2,994 × 8\n    cond nocorr_condition disputed_condition false_condition flag_cond \n   &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt; &lt;hvn_lbll&gt;\n 1     4                0                  1               0  2        \n 2     2                1                  0               0  1        \n 3     5                0                  1               0  2        \n 4     1                0                  0               0 NA        \n 5     4                0                  1               0  2        \n 6     3                1                  0               0  1        \n 7     2                1                  0               0  1        \n 8     2                1                  0               0  1        \n 9     1                0                  0               0 NA        \n10     4                0                  1               0  2        \n# ℹ 2,984 more rows\n# ℹ 3 more variables: purecontrol &lt;dbl&gt;, warning &lt;dbl&gt;, nowarning &lt;dbl&gt;\n\n\nCode\n# cross-check sample sizes with what is reported in the paper to be sure these are the conditions\ndata %&gt;% \n  group_by(cond, nocorr_condition, disputed_condition, false_condition, \n                   flag_cond, purecontrol, warning, nowarning) %&gt;% \n  summarize(n_per_condition = n()) \n\n\n`summarise()` has grouped output by 'cond', 'nocorr_condition',\n'disputed_condition', 'false_condition', 'flag_cond', 'purecontrol', 'warning'.\nYou can override using the `.groups` argument.\n\n\n# A tibble: 7 × 9\n# Groups:   cond, nocorr_condition, disputed_condition, false_condition,\n#   flag_cond, purecontrol, warning [7]\n   cond nocorr_condition disputed_condition false_condition flag_cond \n  &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt; &lt;hvn_lbll&gt;\n1     1                0                  0               0 NA        \n2     2                1                  0               0  1        \n3     3                1                  0               0  1        \n4     4                0                  1               0  2        \n5     5                0                  1               0  2        \n6     6                0                  0               1  3        \n7     7                0                  0               1  3        \n# ℹ 4 more variables: purecontrol &lt;dbl&gt;, warning &lt;dbl&gt;, nowarning &lt;dbl&gt;,\n#   n_per_condition &lt;int&gt;\n\n\nBased on this information, we build a condition variable with more meaningful value labels.\n\n\nCode\n# make new easy-to-read condition variable\ndata &lt;- data |&gt;\n  mutate(\n    intervention_label = case_when(\n      cond == 4 ~ \"disputed_no_warning\",\n      cond == 5 ~ \"disputed_warning\", \n      cond == 6 ~ \"false_no_warning\", \n      cond == 7 ~ \"false_warning\",\n      TRUE ~ NA_character_\n    ),\n    control_label = case_when(\n      cond == 1 ~ \"pure_control\", \n      cond == 2 ~ \"control_no_warning\", \n      cond == 3 ~ \"control_warning\",\n      TRUE ~ NA_character_\n    ),\n    condition = if_else(cond %in% c(1,2,3), \"control\", \"treatment\")\n  )\n\n\n# check for correct sample size\ndata %&gt;% group_by(condition) %&gt;% summarise(n = n())\n\n\n# A tibble: 2 × 2\n  condition     n\n  &lt;chr&gt;     &lt;int&gt;\n1 control    1326\n2 treatment  1668\n\n\n\n\nveracity, accuracy_raw\nAs a next step, we need to identify which variables code instances of news ratings\n\n\nCode\n# trying to identify news ratings  \ndata %&gt;% select(belief_old_fake_news, belief_real_news, real_civil_war_belief,\n                 real_syria_belief, real_gorsuch_belief, draft_belief, bee_belief, \n                 chaf_belief, protester_belief, marines_belief, fbiagent_belief) \n\n\n# A tibble: 2,994 × 11\n   belief_old_fake_news belief_real_news real_civil_war_belief real_syria_belief\n                  &lt;dbl&gt;            &lt;dbl&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n 1                 1.75             2.60                     3                 3\n 2                 2.75             3.60                     3                 4\n 3                 1.25             4                        3                 3\n 4                 2                3.60                     2                 3\n 5                 1.75             3.20                     3                 4\n 6                 1                4                        3                 4\n 7                 2                4                        3                 4\n 8                 2.25             3.20                     2                 4\n 9                 2                3.80                     2                 2\n10                 2.25             2.80                     1                 4\n# ℹ 2,984 more rows\n# ℹ 7 more variables: real_gorsuch_belief &lt;dbl&gt;, draft_belief &lt;dbl&gt;,\n#   bee_belief &lt;dbl&gt;, chaf_belief &lt;dbl&gt;, protester_belief &lt;dbl&gt;,\n#   marines_belief &lt;dbl&gt;, fbiagent_belief &lt;dbl&gt;\n\n\nIt seems that all true news are preceded by ‘real’. Next, we bring the data into long format to build a veracity variable.\n\n\nCode\n# bring data to long format\nlong_data &lt;- data %&gt;% \n  # make an id variable\n  mutate(id = 1:nrow(.)) %&gt;% \n  pivot_longer(c(real_civil_war_belief, real_syria_belief, real_gorsuch_belief, \n                 draft_belief, bee_belief, chaf_belief, protester_belief, \n                 marines_belief, fbiagent_belief), \n               names_to = \"item\",\n               values_to = \"accuracy_raw\") %&gt;% \n  # make an binary 'veracity' variable identifying true and fake\n  mutate(veracity = ifelse(grepl('real', item), 'true', 'false'))\n\n# check that veracity corresponds to correct items\n# long_data %&gt;% \n#   group_by(item, veracity) %&gt;% \n#   summarize(n = n())\n\n\n\n\nscale\n\n\nCode\ntable(long_data$accuracy_raw, useNA = \"always\")\n\n\n\n   1    2    3    4 &lt;NA&gt; \n8890 6446 5302 6159  149 \n\n\n\n\nCode\nlong_data &lt;- long_data|&gt;\n  mutate(scale = 4)\n\n\n\n\nnews_idand news_selection\nIn the previous section, we have already created a news identifier item. Here we just rename this identifier.\n\n\nCode\nlong_data &lt;- long_data |&gt; \n  mutate(news_id = item, \n         news_selection = \"researchers\")\n\n\n\n\nage, age_range\nAge is only provided in bins. We make a cleaner version of the variable.\n\n\nCode\n# Extract labels from the variable's attributes\nlabels &lt;- attr(long_data$agegroup, \"labels\")\n\n# Use the numeric values as levels and their names as labels\nlong_data &lt;- long_data %&gt;%\n  mutate(age_range = factor(agegroup, \n                      levels = labels, \n                      labels = names(labels)))\n\n\nBased on this bin variable, we take the age category mid-point as a proxy for age.\n\n\nCode\n# Define midpoints for each age group\nage_midpoints &lt;- c(\n  \"Under 18\" = 17,             \n  \"18 - 24\" = (18 + 24) / 2,\n  \"25 - 34\" = (25 + 34) / 2,\n  \"35 - 44\" = (35 + 44) / 2,\n  \"45 - 54\" = (45 + 54) / 2,\n  \"55 - 64\" = (55 + 64) / 2,\n  \"65 - 74\" = (65 + 74) / 2,\n  \"75 - 84\" = (75 + 84) / 2,\n  \"85 or older\" = 85          \n)\n\n# Map numeric codes to labels, then to midpoints\nage_midpoints &lt;- setNames(as.numeric(age_midpoints), names(labels))\n\n# Replace agegroup with midpoints\nlong_data &lt;- long_data %&gt;%\n  mutate(age = ifelse(!is.na(age_range), age_midpoints[age_range], NA))\n\n# check\n# long_data %&gt;% \n#   select(age, age_range)\n\n\n\n\nyear\nV8 is the StartDate variable\n\n\nCode\nhead(long_data$V8)\n\n\n[1] \"2017-05-08 07:31:27 UTC\" \"2017-05-08 07:31:27 UTC\"\n[3] \"2017-05-08 07:31:27 UTC\" \"2017-05-08 07:31:27 UTC\"\n[5] \"2017-05-08 07:31:27 UTC\" \"2017-05-08 07:31:27 UTC\"\n\n\n\n\nCode\nlong_data &lt;- long_data |&gt; \n  mutate(year = year(V8)\n         )\n\n# check\nlong_data |&gt; \n  select(V8, year)\n\n\n# A tibble: 26,946 × 2\n   V8                   year\n   &lt;dttm&gt;              &lt;dbl&gt;\n 1 2017-05-08 07:31:27  2017\n 2 2017-05-08 07:31:27  2017\n 3 2017-05-08 07:31:27  2017\n 4 2017-05-08 07:31:27  2017\n 5 2017-05-08 07:31:27  2017\n 6 2017-05-08 07:31:27  2017\n 7 2017-05-08 07:31:27  2017\n 8 2017-05-08 07:31:27  2017\n 9 2017-05-08 07:31:27  2017\n10 2017-05-08 07:51:57  2017\n# ℹ 26,936 more rows\n\n\n\n\nIdentifiers (subject_id, experiment_id, paper_id, country)\n\n\nCode\n# make final data\nclayton_2020 &lt;- long_data |&gt;  \n  mutate(\n    subject_id = id,\n    experiment_id = 1,\n    country = \"United States\",\n    paper_id = \"clayton_2020\") |&gt; \n    # add_intervention_info \n  bind_cols(intervention_info) |&gt; \n  select(any_of(target_variables))\n\n\n# check conditions\n# clayton_2020 |&gt;\n#   group_by(condition) |&gt;\n#   reframe(unique(intervention_label))\n\n\n\n\nWrite out data\n\n\nCode\nsave_data(clayton_2020)"
  },
  {
    "objectID": "presentations/madrid.html#many-individual-level-interventions-have-been-proposed-to-reduce-belief-in-misinformation",
    "href": "presentations/madrid.html#many-individual-level-interventions-have-been-proposed-to-reduce-belief-in-misinformation",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Many individual-level interventions have been proposed to reduce belief in misinformation",
    "text": "Many individual-level interventions have been proposed to reduce belief in misinformation\n\nBad news game, literacy tips, labels"
  },
  {
    "objectID": "presentations/madrid.html#countries",
    "href": "presentations/madrid.html#countries",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Countries",
    "text": "Countries"
  },
  {
    "objectID": "presentations/madrid.html#skepticism-bias",
    "href": "presentations/madrid.html#skepticism-bias",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Skepticism bias",
    "text": "Skepticism bias"
  },
  {
    "objectID": "presentations/madrid.html#interim-conclusion",
    "href": "presentations/madrid.html#interim-conclusion",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Interim conclusion",
    "text": "Interim conclusion\n\n\n\nPeople discern rather well between true and false news\n\n\n\n\n\nIf they err, they tend to be more skeptical of true news than gullible towards false news"
  },
  {
    "objectID": "presentations/madrid.html#but",
    "href": "presentations/madrid.html#but",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "But…",
    "text": "But…\n\n\n\nThe effectiveness of these interventions has been evaluated in terms of discernment only\n\n\n\n\n\nSome misinformation interventions have been shown to foster general skepticism."
  },
  {
    "objectID": "presentations/madrid.html#steps",
    "href": "presentations/madrid.html#steps",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Steps",
    "text": "Steps\n\nIdentify relevant studies\nCollect raw data\nClean and bring into common format\nAnalyze using Signal Detection Theory framework"
  },
  {
    "objectID": "presentations/madrid.html#political-concordance-1",
    "href": "presentations/madrid.html#political-concordance-1",
    "title": "How effective are interventions designed to help people detect misinformation?",
    "section": "Political concordance",
    "text": "Political concordance\n\n\n\nAlignment between personal political stance and the political slant of the news\n\n\n\n\n\nFor example, pro-republican news rated by republicans are coded as concordant"
  }
]