{
  "hash": "4b0e769f6a6ce54fbca1aee4f5d483eb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: How effective are interventions designed to help people detect misinformation?\nsubtitle: Preliminary Analyses\ntitle-block-banner: true\nexecute:\n  message: false\n  warning: false\n  \nabstract: |\n  \n  \n  In recent years, many studies have proposed individual-level interventions to reduce people's susceptibility for believing in misinformation. To evaluate the success of their interventions, these studies have used a simple discernment measure. The problem with this measure is that it does not allow between two different kinds of effects: First, the effect on sensitivity, which is the ability of discriminating between true and false news that researchers typically look for. Second, the effect on response bias, which is the extent to which participants become generally more or less skeptical in their accuracy ratings for all news (whether true or false). To distinguish between these two outcomes, we re-assess the findings of the misinformation intervention literature in a Signal Detection Theory (SDT) framework. We run an Individual Participant Data (IPD) meta-analysis based on a sample of studies identified by four recent systematic literature reviews following the PRISMA guidelines. We use a two-stage approach: First, we extract individual participant data and run a Signal Detection Theory analysis separately for each experiment. Second, we run a meta-analysis on the experiment-level outcomes.\n  \nbibliography: ../references.bib\n---\n\n\n\n::: {.callout-caution}\n## This manuscript is not yet at the stage of a working paper\n\nIt currently shows **highly preliminary** results based on very few studies. These preliminary analyses have been performed for the project to be presented at the 15th Annual Conference of the European Political Science Association (EPSA), at Universidad Carlos III de Madrid, from June 26-28, 2025. \n\nAll analyses performed on the data are strictly in line with our preregistration, [registered on the OSF on December 2, 2024](https://osf.io/gkjuz). No additional analyses have been performed.\n:::\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load plot theme\nsource(\"../R/plot_theme.R\") \n\n# load other functions\nsource(\"../R/custom_functions.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\ndata <- readRDS(\"../data/data.rds\")\n```\n:::\n\n\n\n# Introduction\n\nIn recent years, many studies have tested interventions designed to help people detect online misinformation. However, the results are often not directly comparable, because researchers have used different modes of evaluating the effectiveness of these interventions [@guayHowThinkWhether2023]. Moreover, the most popular outcome measure--a discernment score based on Likert-scale mean differences between true and false news--has recently been shown to be biased [@highamMeanRatingDifference2024a].\n\nThe aim of our paper is to re-analyze the effectiveness of individual-level interventions designed to reduce people's susceptibility for believing in misinformation. Following a recent literature [@highamMeanRatingDifference2024a; @gawronskiSignaldetectionFrameworkMisinformation2024; @modirrousta-galianGamifiedInoculationInterventions2023; @bataillerSignalDetectionApproach2019] we will use a Signal Detection Theory (SDT) framework. This allows us to evaluate two different effects of interventions: First, the effect on sensitivity, which is the ability of discriminating between true and false news. Second, the effect on response bias, which is the extent to which participants shift their general response criterion, i.e. the extent to which they become generally more/less skeptical in their accuracy ratings for all news (regardless of whether true or false).\n\nWe formulate two main research questions: \n\nRQ1: How do interventions against misinformation affect people's ability to discriminate between true and false news (sensitivity, or \"d'\", in a SDT framework)?\n\nRQ2: How do interventions against misinformation affect people's skepticism towards news in general (i.e. response bias, or \"c\", in a SDT framework)?\n\nWe also test some moderator effects, such as the type of interventions, the concordance of the news with people's political identity, and age.\n\nTo answer our research questions we run an [Individual Participant Data meta-analysis (IPD)](https://training.cochrane.org/handbook/current/chapter-26), using a two-stage approach: First, we extract individual participant data from relevant studies and run a Signal Detection Theory analysis separately for each experiment. Second, we run a meta-analysis on the outcomes of the experiments.\n\n# Results\n\n@tbl-sdt-vocabulary shows how instances of news ratings map onto SDT terminology. Our analysis measures the effects of misinformation interventions on two outcomes of Signal Detection Theory (SDT): $d'$ (\"d prime\", sensitivity), and $c$ (response bias). \n\n\n\n::: {#tbl-sdt-vocabulary .cell tbl-cap='Accuracy ratings in Signal Detection Theory terms'}\n\n```{.r .cell-code}\n# Data\ntable_data <- tibble(\n  Stimulus = c(\"True news (target)\",\"False news (distractor)\"),\n  Accurate = c(\"Hit\", \"False alarm\"),\n  `Not Accurate` = c(\"Miss\", \"Correct rejection\")\n)\n\n# Set Stimulus as row names\n# rownames(table_data) <- table_data$Stimulus\n# table_data$Stimulus <- NULL\n\n# Create table using kable\nkable(table_data, \n      booktabs = TRUE) %>%\n  kable_paper(full_width = FALSE) %>%\n  add_header_above(c(\" \", \"Response\" = 2))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n<tr>\n<th style=\"empty-cells: hide;\" colspan=\"1\"></th>\n<th style=\"padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #00000020; padding-bottom: 5px; \">Response</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Stimulus </th>\n   <th style=\"text-align:left;\"> Accurate </th>\n   <th style=\"text-align:left;\"> Not Accurate </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> True news (target) </td>\n   <td style=\"text-align:left;\"> Hit </td>\n   <td style=\"text-align:left;\"> Miss </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> False news (distractor) </td>\n   <td style=\"text-align:left;\"> False alarm </td>\n   <td style=\"text-align:left;\"> Correct rejection </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Data\ntable_data <- tibble(\n  Stimulus = c(\"True news (target)\", \"False news (distractor)\"),\n  Accurate = c(\"Hit\", \"False alarm\"),\n  `Not Accurate` = c(\"Miss\", \"Correct rejection\"),\n  `SDT Metric` = c(\"Hit rate (HR) = Hits / (Hits + Misses)\", \n             \"False alarm rate (FAR) = False Alarms / (False Alarms + Correct Rejections)\")\n)\n\n# Create table using kable\nkable(table_data, booktabs = TRUE, escape = FALSE) %>%\n  kable_paper(full_width = FALSE) %>%\n  add_header_above(c(\" \", \"Participant response\" = 2, \" \"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n<tr>\n<th style=\"empty-cells: hide;\" colspan=\"1\"></th>\n<th style=\"padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #00000020; padding-bottom: 5px; \">Participant response</div></th>\n<th style=\"empty-cells: hide;\" colspan=\"1\"></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Stimulus </th>\n   <th style=\"text-align:left;\"> Accurate </th>\n   <th style=\"text-align:left;\"> Not Accurate </th>\n   <th style=\"text-align:left;\"> SDT Metric </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> True news (target) </td>\n   <td style=\"text-align:left;\"> Hit </td>\n   <td style=\"text-align:left;\"> Miss </td>\n   <td style=\"text-align:left;\"> Hit rate (HR) = Hits / (Hits + Misses) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> False news (distractor) </td>\n   <td style=\"text-align:left;\"> False alarm </td>\n   <td style=\"text-align:left;\"> Correct rejection </td>\n   <td style=\"text-align:left;\"> False alarm rate (FAR) = False Alarms / (False Alarms + Correct Rejections) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  filter(condition == \"control\") |> \n  group_by(veracity) |> \n  summarise(\n    not_accurate = sum(accuracy == 0, na.rm=TRUE),\n    accurate = sum(accuracy == 1, na.rm=TRUE)) |> \n  mutate(sdt_outcome = accurate/ (accurate + not_accurate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  veracity not_accurate accurate sdt_outcome\n  <chr>           <int>    <int>       <dbl>\n1 false           60536    23387       0.279\n2 true            27138    56458       0.675\n```\n\n\n:::\n\n```{.r .cell-code}\ndata |> \n  filter(condition == \"treatment\") |> \n  group_by(veracity) |> \n  summarise(\n    not_accurate = sum(accuracy == 0, na.rm=TRUE),\n    accurate = sum(accuracy == 1, na.rm=TRUE)) |> \n  mutate(sdt_outcome = accurate/ (accurate + not_accurate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  veracity not_accurate accurate sdt_outcome\n  <chr>           <int>    <int>       <dbl>\n1 false           67329    21805       0.245\n2 true            24915    59820       0.706\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running this model takes some time. We therefor stored the results in a data frame that we can reload. \nfilename <- \"../data/models/models_by_experiment.csv\" \n\n# run a loop with the sdt model separatel for each experiment\nrun_loop(data, filename)\n\n# read saved model results\nmodel_results <- read_csv(filename)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# make plot\nggplot(model_results, aes(x = estimate, fill = SDT_term)) +\n  geom_density(alpha = 0.5, adjust = 1.5) +\n  # colors \n  scale_fill_viridis_d(option = \"inferno\", begin = 0.1, end = 0.9) +\n  # labels and scales\n  labs(x = \"z-Score\", y = \"Density\") +\n  guides(fill = FALSE, color = FALSE) +\n  plot_theme +\n  theme(strip.text = element_text(size = 14)) +\n  facet_wrap(~SDT_term)\n```\n\n::: {.cell-output-display}\n![Distributions of Signal Detection Theory outcomes across experiments. Note that these distributions are purely descriptive - effect sizes are not weighted by sample size of the respective experiment, as they are in the meta-analysis.](manuscript_files/figure-html/fig-distributions-1.png){#fig-distributions width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# model for delta dprime\ndelta_dprime <- calculate_meta_model(data = model_results |> \n                                       filter(SDT_term == \"delta d'\"), \n                                     yi = SDT_estimate, \n                                     vi = sampling_variance, \n                                     robust = TRUE) |> \n  tidy(conf.int=TRUE) |> \n    mutate(term = ifelse(term == \"overall\", \"delta d'\", NA))\n\n# model for delta c\ndelta_c <- calculate_meta_model(data = model_results |>\n                                  filter(SDT_term == \"delta c\"), \n                                yi = SDT_estimate, \n                                vi = sampling_variance, \n                                robust = TRUE) |> \n  tidy(conf.int=TRUE) |> \n    mutate(term = ifelse(term == \"overall\", \"delta c\", NA))\n\nmeta_estimates <- bind_rows(delta_dprime, delta_c)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## make plot data\nforest_data <- model_results |> \n  filter(SDT_term %in% c(\"delta c\", \"delta d'\")) |> \n  # Calculate weights (e.g., inverse of standard error)\n  mutate(weight = 1 / sqrt(sampling_variance)) |> \n  group_by(SDT_term) |> \n  arrange(desc(SDT_estimate)) |> \n  mutate(position = 6+row_number()) |> \n  ungroup()\n\n## model outcome\nforest_meta <- meta_estimates |> \n  mutate_if(is.numeric, round, digits = 2) |> \n  mutate(\n    # rename term to be coherent with forest data\n    SDT_term = term,\n    # make label for plot\n    label = paste0(estimate, \" [\", conf.low, \", \", conf.high, \"]\"))\n\n## Plot using ggplot\nggplot(forest_data, aes(x = SDT_estimate, y = position, xmin = SDT_conf.low, xmax = SDT_conf.high)) +\n  geom_pointrange(size = 0.1) +\n  geom_pointrange(data = forest_meta, \n                  aes(x = estimate, y = 0, xmin = conf.low, xmax = conf.high), \n                  shape = 5,\n                  inherit.aes = FALSE) + \n  geom_text(data = forest_meta, \n            aes(x = estimate , y = 1, \n                label = label), \n            vjust = 0, hjust = \"center\", size = 3, inherit.aes = FALSE) + \n  scale_color_viridis_d(option = \"plasma\", name = \"Article\", \n                        begin = 0.5, end = 0.9) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Cohen's D\", y = \"Study\") +\n  plot_theme + \n  theme(legend.position = \"left\",\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank()) + \n  facet_wrap(~SDT_term, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Forest plots for discernment and skepticism bias. The figure displays all effect sizes for both outcomes. Effects are weighed by their sample size. Effect sizes are calculated as z-scores. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the paper level.](manuscript_files/figure-html/fig-forest-1.png){#fig-forest width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to create SDT plot with baseline comparison\ncreate_sdt_plot <- function(hit, miss, fa, cr, baseline_dprime = NULL, \n                            baseline_crit = NULL, show_response_regions = FALSE) {\n  # Calculate SDT parameters\n  hr <- hit / (hit + miss)\n  fr <- fa / (fa + cr)\n  discernment <- hr - fr\n  zhr <- qnorm(hr)\n  zfr <- qnorm(fr)\n  crit <- -(zhr + zfr) / 2 \n  dprime <- zhr - zfr \n  maxl <- dnorm(0)\n  ry <- 0.01\n  \n  # Generate distributions\n  x_vals <- seq(-4, 4, length.out = 1000)\n  densities_long <- tibble(\n    x = x_vals,\n    `false` = dnorm(x_vals, mean = -dprime/2, sd = 1),\n    `true` = dnorm(x_vals, mean = dprime/2, sd = 1)\n  ) |>\n    pivot_longer(cols = c(`false`, `true`), names_to = \"news\", values_to = \"density\") \n  \n  # Base plot\n  p <- ggplot(densities_long, aes(x = x, y = density, fill = news)) +\n    geom_area(alpha = 0.33, position = \"identity\") +\n    scale_fill_viridis_d() +\n    \n    # Current criterion and labels\n    geom_vline(xintercept = crit, linewidth = 0.3, linetype = \"dashed\") +\n    annotate(\"text\", label = paste0(\"c = \", round(crit, 2)), x = crit + 0.1, y = maxl/2, vjust = -0.5) +\n\n    # Current d-prime\n    annotate(\"segment\", x = -dprime/2, xend = dprime/2, y = maxl, yend = maxl,\n             arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"both\")) +\n    annotate(\"text\", label = paste0(\"d' = \", round(dprime, 2)), x = 0, y = maxl, vjust = -0.5) +\n\n    # Discernment, HR, FR\n    annotate(\"text\", \n             label = paste0(\"discernment = \", round(discernment, 2), \"\\n\",\n                            \"HR = \", round(hr, 2), \"\\n\",\n                            \"FAR = \", round(fr, 2)),\n             x = 3, y = maxl/3, vjust = -0.5, size = 3)\n\n  # Response region annotations\n  if (show_response_regions) {\n    p <- p +\n      annotate(\"segment\", x = crit + 0.2, xend = crit + 1.6, y = ry, yend = ry,\n               linewidth = 0.25, arrow = arrow(length = unit(4, \"pt\"), type = \"closed\")) +\n      annotate(\"text\", label = '\"Accurate\"', x = crit + 1, y = ry, vjust = -0.5) +\n      annotate(\"segment\", x = crit - 0.2, xend = crit - 1.6, y = ry, yend = ry,\n               linewidth = 0.4, arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"last\")) +\n      annotate(\"text\", label = '\"Not accurate\"', x = crit - 1, y = ry, vjust = -0.5)\n  }\n\n  # Styling\n  p <- p +\n    scale_y_continuous(\"Density\", expand = expansion(c(0, 0.15))) +\n    labs(x = \"z-score\") +\n    coord_cartesian(ylim = c(0, maxl)) +\n    theme_minimal() +\n    theme(\n      axis.ticks.y = element_blank(),\n      axis.text.y = element_blank(),\n      legend.position = \"none\",\n      plot.title = element_blank()\n    )\n\n  # Add baseline overlays\n  if (!is.null(baseline_dprime)) {\n    p <- p + \n      annotate(\"segment\", x = -baseline_dprime/2, xend = baseline_dprime/2, \n               y = maxl * 0.9, yend = maxl * 0.9,\n               color = \"gray50\", linetype = \"dashed\", linewidth = 0.3,\n               arrow = arrow(length = unit(4, \"pt\"), type = \"closed\", ends = \"both\"))\n  }\n\n  if (!is.null(baseline_crit)) {\n    p <- p + \n      geom_vline(xintercept = baseline_crit, color = \"gray50\", \n                 linewidth = 0.1, linetype = \"dotted\") +\n      annotate(\"segment\", x = baseline_crit, xend = crit, \n               y = maxl * 0.5, yend = maxl * 0.5, \n               linetype = \"dashed\", color = \"gray50\",\n               arrow = arrow(length = unit(5, \"pt\"), type = \"closed\"))\n  }\n\n  return(p)\n}\n\n\n# Generate plots\nplots <- list(\n  p1 = create_sdt_plot(60, 40, 40, 60, show_response_regions = TRUE),\n  p2 = create_sdt_plot(70, 30, 30, 70, \n                       baseline_dprime = qnorm(0.6) - qnorm(0.4)),\n  p3 = create_sdt_plot(50, 50, 10, 90,\n                       baseline_dprime = qnorm(0.6) - qnorm(0.4),\n                       baseline_crit = -(qnorm(0.6) + qnorm(0.4)) / 2)\n)\n\n# Combine and label\nfinal_plot <- plots$p1 / (plots$p2 + plots$p3) + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"top\")\n\nfinal_plot\n```\n\n::: {.cell-output-display}\n![](manuscript_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "manuscript_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}